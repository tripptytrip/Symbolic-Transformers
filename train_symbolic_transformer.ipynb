{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGtE5_xgTFHG"
      },
      "source": [
        "# üß† Symbolic Transformer Training\n",
        "\n",
        "Train a tiny transformer to predict next symbols in First-Order Logic formulas.\n",
        "\n",
        "**What this does:**\n",
        "- Generates synthetic FOL training data\n",
        "- Trains a small transformer model (566K - 19.6M parameters)\n",
        "- Learns syntax rules like: `‚àÄ` ‚Üí must be followed by `VAR`\n",
        "\n",
        "**Quick Start:** Run cells 1-4 in order. Training takes ~30s-90s/epoch on GPU.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF64tUdlTFHI"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup Environment\n",
        "Clone the repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzVNpxbRTFHI",
        "outputId": "e8826d55-8e5e-46ca-832f-ab0e3cded2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checking GPU availability...\n",
            "NVIDIA A100-SXM4-80GB, 81920 MiB\n",
            "\n",
            "üì¶ Cloning Symbolic-Transformers repository...\n",
            "/content\n",
            "‚úì Repository cloned\n",
            "/content/Symbolic-Transformers\n",
            "\n",
            "üìö Installing dependencies...\n",
            "‚úì Dependencies installed\n",
            "\n",
            "üî§ Verifying vocabulary...\n",
            "‚úì Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "‚úì Vocabulary loaded: 662 tokens\n",
            "\n",
            "==================================================\n",
            "‚úÖ Setup complete! Proceed to Step 2.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Setup Environment { display-mode: \"form\" }\n",
        "#@markdown Run this cell first to set up the environment.\n",
        "\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "print(\"üîç Checking GPU availability...\")\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo \"‚ö†Ô∏è No GPU detected - training will be slower\"\n",
        "\n",
        "# Clone repository\n",
        "print(\"\\nüì¶ Cloning Symbolic-Transformers repository...\")\n",
        "%cd /content\n",
        "if not os.path.exists('Symbolic-Transformers'):\n",
        "    !git clone -q https://github.com/tripptytrip/Symbolic-Transformers.git\n",
        "    print(\"‚úì Repository cloned\")\n",
        "else:\n",
        "    !cd Symbolic-Transformers && git stash && git pull -q\n",
        "    print(\"‚úì Repository updated\")\n",
        "\n",
        "%cd /content/Symbolic-Transformers\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\nüìö Installing dependencies...\")\n",
        "!pip -q install numpy scipy pandas tqdm rich tensorboard\n",
        "print(\"‚úì Dependencies installed\")\n",
        "\n",
        "# Verify vocabulary\n",
        "print(\"\\nüî§ Verifying vocabulary...\")\n",
        "!python -c \"from utils.vocabulary import Vocabulary; v = Vocabulary('unified_vocabulary.json'); print(f'‚úì Vocabulary loaded: {v.vocab_size} tokens')\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Setup complete! Proceed to Step 2.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPKqzjMATFHI"
      },
      "source": [
        "## 2Ô∏è‚É£ Configure Training\n",
        "Adjust the settings below, then run the cell to apply them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7xB8AeHTFHJ",
        "outputId": "16a6ca53-2fdd-4a2d-b51a-bda45fdb526e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "üìã TRAINING CONFIGURATION\n",
            "==================================================\n",
            "\n",
            "üèóÔ∏è  Model:     tiny (566K parameters, ~2.2MB)\n",
            "üìä Dataset:   50000 train / 5000 val formulas\n",
            "üß¨ Generator: Advanced (functions, fixed signatures, Horn clauses)\n",
            "‚öôÔ∏è  Training:  500 epochs, batch size 32\n",
            "üîÑ Resume:    No (fresh start)\n",
            "\n",
            "‚è±Ô∏è  Estimated training time: ~1562 minutes on A100\n",
            "   (187s per epoch, 23437 batches)\n",
            "\n",
            "==================================================\n",
            "‚úÖ Configuration saved! Proceed to Step 3.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Training Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### üèóÔ∏è Model Size\n",
        "model_size = \"tiny\" #@param [\"tiny\", \"small\", \"base\"]\n",
        "#@markdown - **tiny**: 566K params (~2.2MB) - Fast training, good for experiments\n",
        "#@markdown - **small**: 3.5M params (~14MB) - Better accuracy, moderate training time\n",
        "#@markdown - **base**: 19.6M params (~78MB) - Best accuracy, longest training\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üìä Dataset Size\n",
        "num_train_formulas = 50000 #@param {type:\"slider\", min:1000, max:50000, step:1000}\n",
        "#@markdown Number of unique FOL formulas to generate for training.\n",
        "#@markdown - 1000-3000: Quick experiments\n",
        "#@markdown - 5000-10000: Standard training\n",
        "#@markdown - 20000-50000: Large-scale training (recommended for small/base models)\n",
        "\n",
        "num_val_formulas = 5000 #@param {type:\"slider\", min:100, max:5000, step:100}\n",
        "#@markdown Number of formulas for validation (typically 10-20% of training).\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üß¨ Data Generator\n",
        "use_advanced_generator = True #@param {type:\"boolean\"}\n",
        "#@markdown **Advanced generator** adds:\n",
        "#@markdown - Function symbols: `P(f(x), g(y))` instead of just `P(x, y)`\n",
        "#@markdown - Fixed signatures: `PRED_5` is *always* arity 2 (model learns consistency)\n",
        "#@markdown - Horn clauses: `(A ‚àß B ‚àß C) ‚Üí D` (common logic programming pattern)\n",
        "#@markdown - Vacuous quantification: `‚àÄx P(y)` (tests scope understanding)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ‚öôÔ∏è Training Parameters\n",
        "num_epochs = 500 #@param {type:\"slider\", min:10, max:500, step:10}\n",
        "#@markdown Number of training epochs.\n",
        "#@markdown - 10-30: Quick experiments\n",
        "#@markdown - 50-100: Standard training\n",
        "#@markdown - 100-200: Train to convergence (watch for overfitting!)\n",
        "\n",
        "batch_size = 32 #@param [32, 64, 128, 256] {type:\"raw\"}\n",
        "#@markdown Larger batches = faster training but more memory.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üíæ Resume from Checkpoint\n",
        "resume_training = False #@param {type:\"boolean\"}\n",
        "#@markdown Resume from the last saved checkpoint.\n",
        "\n",
        "# Store configuration\n",
        "config = {\n",
        "    'model_size': model_size,\n",
        "    'num_train_formulas': num_train_formulas,\n",
        "    'num_val_formulas': num_val_formulas,\n",
        "    'num_test_formulas': max(100, num_val_formulas // 2),\n",
        "    'num_epochs': num_epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'resume': resume_training,\n",
        "    'use_advanced_generator': use_advanced_generator\n",
        "}\n",
        "\n",
        "# Display configuration summary\n",
        "print(\"=\"*50)\n",
        "print(\"üìã TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model_params = {'tiny': '566K', 'small': '3.5M', 'base': '19.6M'}\n",
        "model_size_mb = {'tiny': '~2.2MB', 'small': '~14MB', 'base': '~78MB'}\n",
        "\n",
        "print(f\"\\nüèóÔ∏è  Model:     {model_size} ({model_params[model_size]} parameters, {model_size_mb[model_size]})\")\n",
        "print(f\"üìä Dataset:   {num_train_formulas} train / {num_val_formulas} val formulas\")\n",
        "print(f\"üß¨ Generator: {'Advanced (functions, fixed signatures, Horn clauses)' if use_advanced_generator else 'Basic'}\")\n",
        "print(f\"‚öôÔ∏è  Training:  {num_epochs} epochs, batch size {batch_size}\")\n",
        "print(f\"üîÑ Resume:    {'Yes' if resume_training else 'No (fresh start)'}\")\n",
        "\n",
        "# Estimate training time\n",
        "samples_per_formula = 15 if use_advanced_generator else 14\n",
        "total_samples = num_train_formulas * samples_per_formula\n",
        "batches_per_epoch = total_samples // batch_size\n",
        "time_per_batch = {'tiny': 0.008, 'small': 0.020, 'base': 0.040}  # seconds on A100\n",
        "est_epoch_time = batches_per_epoch * time_per_batch[model_size]\n",
        "est_total_time = est_epoch_time * num_epochs / 60\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Estimated training time: ~{est_total_time:.0f} minutes on A100\")\n",
        "print(f\"   ({est_epoch_time:.0f}s per epoch, {batches_per_epoch} batches)\")\n",
        "\n",
        "# Recommendations\n",
        "if model_size in ['small', 'base'] and num_train_formulas < 10000:\n",
        "    print(f\"\\nüí° TIP: {model_size} model benefits from more data.\")\n",
        "    print(f\"   Consider increasing to 20000+ formulas.\")\n",
        "\n",
        "if num_epochs > 100 and not use_advanced_generator:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: High epochs ({num_epochs}) with basic generator.\")\n",
        "    print(f\"   Risk of overfitting! Consider:\")\n",
        "    print(f\"   - Enabling advanced generator for richer data\")\n",
        "    print(f\"   - Or reducing epochs to 50-100\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Configuration saved! Proceed to Step 3.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VJA9pkTFHJ"
      },
      "source": [
        "## 3Ô∏è‚É£ Generate Training Data\n",
        "Create synthetic First-Order Logic formulas for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a25zb42qTFHJ",
        "outputId": "288d31ae-3057-44a0-8659-3b6ddacc3e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Generating training data...\n",
            "   Train: 50000 formulas\n",
            "   Val:   5000 formulas\n",
            "   Test:  2500 formulas\n",
            "   Generator: Advanced\n",
            "\n",
            "============================================================\n",
            "ADVANCED FOL DATASET GENERATION\n",
            "============================================================\n",
            "‚úì Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "\n",
            "‚ÑπÔ∏è Fixed Signatures (consistent across all formulas):\n",
            "   Predicates: {0: 2, 1: 1, 2: 1, 3: 1, 4: 2, 5: 2, 6: 2, 7: 1, 8: 2, 9: 1}\n",
            "   Functions:  {0: 1, 1: 1, 2: 1, 3: 1}\n",
            "\n",
            "Generating train set (50000 formulas)...\n",
            "  Complexity 1: 10000 formulas\n",
            "  Complexity 2: 20000 formulas\n",
            "  Complexity 3: 15000 formulas\n",
            "  Complexity 4: 5000 formulas\n",
            "\n",
            "Generating val set (5000 formulas)...\n",
            "  Complexity 1: 1000 formulas\n",
            "  Complexity 2: 2000 formulas\n",
            "  Complexity 3: 1500 formulas\n",
            "  Complexity 4: 500 formulas\n",
            "\n",
            "Generating test set (2500 formulas)...\n",
            "  Complexity 1: 500 formulas\n",
            "  Complexity 2: 1000 formulas\n",
            "  Complexity 3: 750 formulas\n",
            "  Complexity 4: 250 formulas\n",
            "‚úì Saved 1682302 samples to datasets/fol_next_symbol/train.json\n",
            "‚úì Saved 166031 samples to datasets/fol_next_symbol/val.json\n",
            "‚úì Saved 81694 samples to datasets/fol_next_symbol/test.json\n",
            "\n",
            "============================================================\n",
            "‚úì Advanced dataset generation complete!\n",
            "‚úì Output directory: datasets/fol_next_symbol\n",
            "‚úì Train samples: 1682302\n",
            "‚úì Val samples: 166031\n",
            "‚úì Test samples: 81694\n",
            "============================================================\n",
            "\n",
            "üìÅ Dataset files:\n",
            "-rw-r--r-- 1 root root  522 Dec 21 09:56 datasets/fol_next_symbol/metadata.json\n",
            "-rw-r--r-- 1 root root  12M Dec 21 09:56 datasets/fol_next_symbol/test.json\n",
            "-rw-r--r-- 1 root root 238M Dec 21 09:56 datasets/fol_next_symbol/train.json\n",
            "-rw-r--r-- 1 root root  24M Dec 21 09:56 datasets/fol_next_symbol/val.json\n",
            "\n",
            "==================================================\n",
            "‚úÖ Data generated! Proceed to Step 4.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 3. Generate Training Data { display-mode: \"form\" }\n",
        "#@markdown This generates synthetic FOL formulas.\n",
        "#@markdown\n",
        "#@markdown **Basic generator examples:**\n",
        "#@markdown - `‚àÄx‚ÇÅ (P‚ÇÖ(x‚ÇÅ) ‚Üí Q‚ÇÇ(x‚ÇÅ))`\n",
        "#@markdown - `‚àÉx‚ÇÄ ‚àÉx‚ÇÅ (R‚ÇÉ(x‚ÇÄ, x‚ÇÅ) ‚àß P‚ÇÅ(x‚ÇÄ))`\n",
        "#@markdown\n",
        "#@markdown **Advanced generator adds:**\n",
        "#@markdown - `‚àÄx P(f(x), g(x, y))` (function symbols)\n",
        "#@markdown - `(A ‚àß B ‚àß C) ‚Üí D` (Horn clauses)\n",
        "#@markdown - Fixed predicate arities across all formulas\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"üîÑ Generating training data...\")\n",
        "print(f\"   Train: {config['num_train_formulas']} formulas\")\n",
        "print(f\"   Val:   {config['num_val_formulas']} formulas\")\n",
        "print(f\"   Test:  {config['num_test_formulas']} formulas\")\n",
        "print(f\"   Generator: {'Advanced' if config['use_advanced_generator'] else 'Basic'}\")\n",
        "print()\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/Symbolic-Transformers')\n",
        "\n",
        "if config['use_advanced_generator']:\n",
        "    # Use advanced generator with functions, fixed signatures, Horn clauses\n",
        "    from data.advanced_generator import generate_advanced_training_data\n",
        "\n",
        "    generate_advanced_training_data(\n",
        "        vocab_path=\"unified_vocabulary.json\",\n",
        "        output_dir=\"datasets/fol_next_symbol\",\n",
        "        n_train=config['num_train_formulas'],\n",
        "        n_val=config['num_val_formulas'],\n",
        "        n_test=config['num_test_formulas'],\n",
        "    )\n",
        "else:\n",
        "    # Use basic generator\n",
        "    from data.dataset_generator import generate_training_data\n",
        "\n",
        "    generate_training_data(\n",
        "        vocab_path=\"unified_vocabulary.json\",\n",
        "        output_dir=\"datasets/fol_next_symbol\",\n",
        "        n_train=config['num_train_formulas'],\n",
        "        n_val=config['num_val_formulas'],\n",
        "        n_test=config['num_test_formulas'],\n",
        "    )\n",
        "\n",
        "# Show dataset stats\n",
        "print(\"\\nüìÅ Dataset files:\")\n",
        "!ls -lh datasets/fol_next_symbol/*.json\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Data generated! Proceed to Step 4.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2H8LpcTFHJ"
      },
      "source": [
        "## 4Ô∏è‚É£ Train the Model\n",
        "Start training! Watch the loss decrease over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBTsqEasTFHJ",
        "outputId": "0c64fe28-795b-4529-ddb8-f65b4148f397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "üöÄ STARTING TRAINING\n",
            "============================================================\n",
            "Model: tiny | Epochs: 500\n",
            "============================================================\n",
            "\n",
            "‚úì Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "\n",
            "‚úì Loaded vocabulary: 662 tokens\n",
            "\n",
            "Loading datasets...\n",
            "‚úì Loaded 1682302 samples from datasets/fol_next_symbol/train.json\n",
            "‚úì Loaded 166031 samples from datasets/fol_next_symbol/val.json\n",
            "‚úì Train batches: 26286\n",
            "‚úì Val batches: 2595\n",
            "\n",
            "Creating model...\n",
            "‚úì Created tiny model with 566,934 parameters\n",
            "‚úì Using device: cuda\n",
            "‚úì GPU: NVIDIA A100-SXM4-80GB\n",
            "‚úì VRAM: 85.2 GB\n",
            "\n",
            "============================================================\n",
            "TRAINING START\n",
            "============================================================\n",
            "Model: tiny\n",
            "Vocab size: 662\n",
            "Batch size: 64\n",
            "Epochs: 500\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "Epoch 1/500\n",
            "  Batch 100/26286 | Loss: 6.3634 | LR: 5.00e-06\n",
            "  Batch 200/26286 | Loss: 6.2427 | LR: 1.00e-05\n",
            "  Batch 300/26286 | Loss: 6.0659 | LR: 1.50e-05\n",
            "  Batch 400/26286 | Loss: 5.8739 | LR: 2.00e-05\n",
            "  Batch 500/26286 | Loss: 5.6750 | LR: 2.50e-05\n",
            "  Batch 600/26286 | Loss: 5.4612 | LR: 3.00e-05\n",
            "  Batch 700/26286 | Loss: 5.2400 | LR: 3.50e-05\n",
            "  Batch 800/26286 | Loss: 5.0163 | LR: 4.00e-05\n",
            "  Batch 900/26286 | Loss: 4.8055 | LR: 4.50e-05\n",
            "  Batch 1000/26286 | Loss: 4.6194 | LR: 5.00e-05\n",
            "  Batch 1100/26286 | Loss: 4.4597 | LR: 5.50e-05\n",
            "  Batch 1200/26286 | Loss: 4.3214 | LR: 6.00e-05\n",
            "  Batch 1300/26286 | Loss: 4.2020 | LR: 6.50e-05\n",
            "  Batch 1400/26286 | Loss: 4.0970 | LR: 7.00e-05\n",
            "  Batch 1500/26286 | Loss: 4.0034 | LR: 7.50e-05\n",
            "  Batch 1600/26286 | Loss: 3.9188 | LR: 8.00e-05\n",
            "  Batch 1700/26286 | Loss: 3.8427 | LR: 8.50e-05\n",
            "  Batch 1800/26286 | Loss: 3.7707 | LR: 9.00e-05\n",
            "  Batch 1900/26286 | Loss: 3.7024 | LR: 9.50e-05\n",
            "  Batch 2000/26286 | Loss: 3.6393 | LR: 1.00e-04\n",
            "  Batch 2100/26286 | Loss: 3.5803 | LR: 1.00e-04\n",
            "  Batch 2200/26286 | Loss: 3.5246 | LR: 1.00e-04\n",
            "  Batch 2300/26286 | Loss: 3.4718 | LR: 1.00e-04\n",
            "  Batch 2400/26286 | Loss: 3.4212 | LR: 1.00e-04\n",
            "  Batch 2500/26286 | Loss: 3.3723 | LR: 1.00e-04\n",
            "  Batch 2600/26286 | Loss: 3.3249 | LR: 1.00e-04\n",
            "  Batch 2700/26286 | Loss: 3.2805 | LR: 1.00e-04\n",
            "  Batch 2800/26286 | Loss: 3.2369 | LR: 1.00e-04\n",
            "  Batch 2900/26286 | Loss: 3.1960 | LR: 1.00e-04\n",
            "  Batch 3000/26286 | Loss: 3.1570 | LR: 1.00e-04\n",
            "  Batch 3100/26286 | Loss: 3.1193 | LR: 1.00e-04\n",
            "  Batch 3200/26286 | Loss: 3.0842 | LR: 1.00e-04\n",
            "  Batch 3300/26286 | Loss: 3.0497 | LR: 1.00e-04\n",
            "  Batch 3400/26286 | Loss: 3.0160 | LR: 1.00e-04\n",
            "  Batch 3500/26286 | Loss: 2.9850 | LR: 1.00e-04\n",
            "  Batch 3600/26286 | Loss: 2.9543 | LR: 1.00e-04\n",
            "  Batch 3700/26286 | Loss: 2.9240 | LR: 1.00e-04\n",
            "  Batch 3800/26286 | Loss: 2.8949 | LR: 1.00e-04\n",
            "  Batch 3900/26286 | Loss: 2.8670 | LR: 1.00e-04\n",
            "  Batch 4000/26286 | Loss: 2.8393 | LR: 1.00e-04\n",
            "  Batch 4100/26286 | Loss: 2.8115 | LR: 1.00e-04\n",
            "  Batch 4200/26286 | Loss: 2.7853 | LR: 1.00e-04\n",
            "  Batch 4300/26286 | Loss: 2.7601 | LR: 1.00e-04\n",
            "  Batch 4400/26286 | Loss: 2.7351 | LR: 1.00e-04\n",
            "  Batch 4500/26286 | Loss: 2.7107 | LR: 1.00e-04\n",
            "  Batch 4600/26286 | Loss: 2.6869 | LR: 1.00e-04\n",
            "  Batch 4700/26286 | Loss: 2.6645 | LR: 1.00e-04\n",
            "  Batch 4800/26286 | Loss: 2.6421 | LR: 1.00e-04\n",
            "  Batch 4900/26286 | Loss: 2.6207 | LR: 1.00e-04\n",
            "  Batch 5000/26286 | Loss: 2.5999 | LR: 1.00e-04\n",
            "  Batch 5100/26286 | Loss: 2.5796 | LR: 1.00e-04\n",
            "  Batch 5200/26286 | Loss: 2.5601 | LR: 1.00e-04\n",
            "  Batch 5300/26286 | Loss: 2.5411 | LR: 1.00e-04\n",
            "  Batch 5400/26286 | Loss: 2.5225 | LR: 1.00e-04\n",
            "  Batch 5500/26286 | Loss: 2.5046 | LR: 1.00e-04\n",
            "  Batch 5600/26286 | Loss: 2.4871 | LR: 1.00e-04\n",
            "  Batch 5700/26286 | Loss: 2.4701 | LR: 1.00e-04\n",
            "  Batch 5800/26286 | Loss: 2.4529 | LR: 1.00e-04\n",
            "  Batch 5900/26286 | Loss: 2.4366 | LR: 1.00e-04\n",
            "  Batch 6000/26286 | Loss: 2.4211 | LR: 1.00e-04\n",
            "  Batch 6100/26286 | Loss: 2.4060 | LR: 1.00e-04\n",
            "  Batch 6200/26286 | Loss: 2.3911 | LR: 1.00e-04\n",
            "  Batch 6300/26286 | Loss: 2.3767 | LR: 1.00e-04\n",
            "  Batch 6400/26286 | Loss: 2.3627 | LR: 1.00e-04\n",
            "  Batch 6500/26286 | Loss: 2.3487 | LR: 1.00e-04\n",
            "  Batch 6600/26286 | Loss: 2.3350 | LR: 1.00e-04\n",
            "  Batch 6700/26286 | Loss: 2.3219 | LR: 1.00e-04\n",
            "  Batch 6800/26286 | Loss: 2.3089 | LR: 1.00e-04\n",
            "  Batch 6900/26286 | Loss: 2.2964 | LR: 1.00e-04\n",
            "  Batch 7000/26286 | Loss: 2.2846 | LR: 1.00e-04\n",
            "  Batch 7100/26286 | Loss: 2.2726 | LR: 1.00e-04\n",
            "  Batch 7200/26286 | Loss: 2.2609 | LR: 1.00e-04\n",
            "  Batch 7300/26286 | Loss: 2.2499 | LR: 1.00e-04\n",
            "  Batch 7400/26286 | Loss: 2.2389 | LR: 1.00e-04\n",
            "  Batch 7500/26286 | Loss: 2.2278 | LR: 1.00e-04\n",
            "  Batch 7600/26286 | Loss: 2.2171 | LR: 1.00e-04\n",
            "  Batch 7700/26286 | Loss: 2.2068 | LR: 1.00e-04\n",
            "  Batch 7800/26286 | Loss: 2.1967 | LR: 1.00e-04\n",
            "  Batch 7900/26286 | Loss: 2.1868 | LR: 1.00e-04\n",
            "  Batch 8000/26286 | Loss: 2.1770 | LR: 1.00e-04\n",
            "  Batch 8100/26286 | Loss: 2.1674 | LR: 1.00e-04\n",
            "  Batch 8200/26286 | Loss: 2.1577 | LR: 1.00e-04\n",
            "  Batch 8300/26286 | Loss: 2.1483 | LR: 1.00e-04\n",
            "  Batch 8400/26286 | Loss: 2.1391 | LR: 1.00e-04\n",
            "  Batch 8500/26286 | Loss: 2.1303 | LR: 1.00e-04\n",
            "  Batch 8600/26286 | Loss: 2.1214 | LR: 1.00e-04\n",
            "  Batch 8700/26286 | Loss: 2.1128 | LR: 1.00e-04\n",
            "  Batch 8800/26286 | Loss: 2.1046 | LR: 1.00e-04\n",
            "  Batch 8900/26286 | Loss: 2.0965 | LR: 1.00e-04\n",
            "  Batch 9000/26286 | Loss: 2.0885 | LR: 1.00e-04\n",
            "  Batch 9100/26286 | Loss: 2.0805 | LR: 1.00e-04\n",
            "  Batch 9200/26286 | Loss: 2.0725 | LR: 1.00e-04\n",
            "  Batch 9300/26286 | Loss: 2.0646 | LR: 1.00e-04\n",
            "  Batch 9400/26286 | Loss: 2.0569 | LR: 1.00e-04\n",
            "  Batch 9500/26286 | Loss: 2.0495 | LR: 1.00e-04\n",
            "  Batch 9600/26286 | Loss: 2.0420 | LR: 1.00e-04\n",
            "  Batch 9700/26286 | Loss: 2.0348 | LR: 1.00e-04\n",
            "  Batch 9800/26286 | Loss: 2.0278 | LR: 1.00e-04\n",
            "  Batch 9900/26286 | Loss: 2.0209 | LR: 1.00e-04\n",
            "  Batch 10000/26286 | Loss: 2.0142 | LR: 1.00e-04\n",
            "  Batch 10100/26286 | Loss: 2.0075 | LR: 1.00e-04\n",
            "  Batch 10200/26286 | Loss: 2.0012 | LR: 1.00e-04\n",
            "  Batch 10300/26286 | Loss: 1.9947 | LR: 1.00e-04\n",
            "  Batch 10400/26286 | Loss: 1.9880 | LR: 1.00e-04\n",
            "  Batch 10500/26286 | Loss: 1.9818 | LR: 1.00e-04\n",
            "  Batch 10600/26286 | Loss: 1.9758 | LR: 1.00e-04\n",
            "  Batch 10700/26286 | Loss: 1.9698 | LR: 1.00e-04\n",
            "  Batch 10800/26286 | Loss: 1.9638 | LR: 1.00e-04\n",
            "  Batch 10900/26286 | Loss: 1.9578 | LR: 1.00e-04\n",
            "  Batch 11000/26286 | Loss: 1.9519 | LR: 1.00e-04\n",
            "  Batch 11100/26286 | Loss: 1.9462 | LR: 1.00e-04\n",
            "  Batch 11200/26286 | Loss: 1.9403 | LR: 1.00e-04\n",
            "  Batch 11300/26286 | Loss: 1.9347 | LR: 1.00e-04\n",
            "  Batch 11400/26286 | Loss: 1.9292 | LR: 1.00e-04\n",
            "  Batch 11500/26286 | Loss: 1.9238 | LR: 1.00e-04\n",
            "  Batch 11600/26286 | Loss: 1.9187 | LR: 1.00e-04\n",
            "  Batch 11700/26286 | Loss: 1.9134 | LR: 1.00e-04\n",
            "  Batch 11800/26286 | Loss: 1.9081 | LR: 1.00e-04\n",
            "  Batch 11900/26286 | Loss: 1.9029 | LR: 1.00e-04\n",
            "  Batch 12000/26286 | Loss: 1.8979 | LR: 1.00e-04\n",
            "  Batch 12100/26286 | Loss: 1.8929 | LR: 1.00e-04\n",
            "  Batch 12200/26286 | Loss: 1.8880 | LR: 1.00e-04\n",
            "  Batch 12300/26286 | Loss: 1.8830 | LR: 1.00e-04\n",
            "  Batch 12400/26286 | Loss: 1.8782 | LR: 1.00e-04\n",
            "  Batch 12500/26286 | Loss: 1.8735 | LR: 1.00e-04\n",
            "  Batch 12600/26286 | Loss: 1.8689 | LR: 1.00e-04\n",
            "  Batch 12700/26286 | Loss: 1.8643 | LR: 1.00e-04\n",
            "  Batch 12800/26286 | Loss: 1.8598 | LR: 1.00e-04\n",
            "  Batch 12900/26286 | Loss: 1.8554 | LR: 1.00e-04\n",
            "  Batch 13000/26286 | Loss: 1.8508 | LR: 1.00e-04\n",
            "  Batch 13100/26286 | Loss: 1.8464 | LR: 1.00e-04\n",
            "  Batch 13200/26286 | Loss: 1.8422 | LR: 1.00e-04\n",
            "  Batch 13300/26286 | Loss: 1.8376 | LR: 1.00e-04\n",
            "  Batch 13400/26286 | Loss: 1.8334 | LR: 1.00e-04\n",
            "  Batch 13500/26286 | Loss: 1.8292 | LR: 1.00e-04\n",
            "  Batch 13600/26286 | Loss: 1.8251 | LR: 1.00e-04\n",
            "  Batch 13700/26286 | Loss: 1.8210 | LR: 1.00e-04\n",
            "  Batch 13800/26286 | Loss: 1.8169 | LR: 1.00e-04\n",
            "  Batch 13900/26286 | Loss: 1.8127 | LR: 1.00e-04\n",
            "  Batch 14000/26286 | Loss: 1.8089 | LR: 1.00e-04\n",
            "  Batch 14100/26286 | Loss: 1.8051 | LR: 1.00e-04\n",
            "  Batch 14200/26286 | Loss: 1.8012 | LR: 1.00e-04\n",
            "  Batch 14300/26286 | Loss: 1.7973 | LR: 1.00e-04\n",
            "  Batch 14400/26286 | Loss: 1.7936 | LR: 1.00e-04\n",
            "  Batch 14500/26286 | Loss: 1.7897 | LR: 1.00e-04\n",
            "  Batch 14600/26286 | Loss: 1.7860 | LR: 1.00e-04\n",
            "  Batch 14700/26286 | Loss: 1.7824 | LR: 1.00e-04\n",
            "  Batch 14800/26286 | Loss: 1.7789 | LR: 1.00e-04\n",
            "  Batch 14900/26286 | Loss: 1.7753 | LR: 1.00e-04\n",
            "  Batch 15000/26286 | Loss: 1.7717 | LR: 1.00e-04\n",
            "  Batch 15100/26286 | Loss: 1.7683 | LR: 1.00e-04\n",
            "  Batch 15200/26286 | Loss: 1.7648 | LR: 1.00e-04\n",
            "  Batch 15300/26286 | Loss: 1.7615 | LR: 1.00e-04\n",
            "  Batch 15400/26286 | Loss: 1.7582 | LR: 1.00e-04\n",
            "  Batch 15500/26286 | Loss: 1.7549 | LR: 1.00e-04\n",
            "  Batch 15600/26286 | Loss: 1.7516 | LR: 1.00e-04\n",
            "  Batch 15700/26286 | Loss: 1.7484 | LR: 1.00e-04\n",
            "  Batch 15800/26286 | Loss: 1.7451 | LR: 1.00e-04\n",
            "  Batch 15900/26286 | Loss: 1.7418 | LR: 1.00e-04\n",
            "  Batch 16000/26286 | Loss: 1.7388 | LR: 1.00e-04\n",
            "  Batch 16100/26286 | Loss: 1.7356 | LR: 1.00e-04\n",
            "  Batch 16200/26286 | Loss: 1.7326 | LR: 1.00e-04\n",
            "  Batch 16300/26286 | Loss: 1.7295 | LR: 1.00e-04\n",
            "  Batch 16400/26286 | Loss: 1.7263 | LR: 1.00e-04\n",
            "  Batch 16500/26286 | Loss: 1.7232 | LR: 1.00e-04\n",
            "  Batch 16600/26286 | Loss: 1.7203 | LR: 1.00e-04\n",
            "  Batch 16700/26286 | Loss: 1.7174 | LR: 1.00e-04\n",
            "  Batch 16800/26286 | Loss: 1.7145 | LR: 1.00e-04\n",
            "  Batch 16900/26286 | Loss: 1.7115 | LR: 1.00e-04\n",
            "  Batch 17000/26286 | Loss: 1.7084 | LR: 1.00e-04\n",
            "  Batch 17100/26286 | Loss: 1.7054 | LR: 1.00e-04\n",
            "  Batch 17200/26286 | Loss: 1.7027 | LR: 1.00e-04\n",
            "  Batch 17300/26286 | Loss: 1.6999 | LR: 1.00e-04\n",
            "  Batch 17400/26286 | Loss: 1.6971 | LR: 1.00e-04\n",
            "  Batch 17500/26286 | Loss: 1.6943 | LR: 1.00e-04\n",
            "  Batch 17600/26286 | Loss: 1.6915 | LR: 1.00e-04\n",
            "  Batch 17700/26286 | Loss: 1.6887 | LR: 1.00e-04\n",
            "  Batch 17800/26286 | Loss: 1.6860 | LR: 1.00e-04\n",
            "  Batch 17900/26286 | Loss: 1.6833 | LR: 1.00e-04\n",
            "  Batch 18000/26286 | Loss: 1.6804 | LR: 1.00e-04\n",
            "  Batch 18100/26286 | Loss: 1.6777 | LR: 1.00e-04\n",
            "  Batch 18200/26286 | Loss: 1.6752 | LR: 1.00e-04\n",
            "  Batch 18300/26286 | Loss: 1.6725 | LR: 1.00e-04\n",
            "  Batch 18400/26286 | Loss: 1.6698 | LR: 1.00e-04\n",
            "  Batch 18500/26286 | Loss: 1.6673 | LR: 1.00e-04\n",
            "  Batch 18600/26286 | Loss: 1.6648 | LR: 1.00e-04\n",
            "  Batch 18700/26286 | Loss: 1.6624 | LR: 1.00e-04\n",
            "  Batch 18800/26286 | Loss: 1.6600 | LR: 1.00e-04\n",
            "  Batch 18900/26286 | Loss: 1.6574 | LR: 1.00e-04\n",
            "  Batch 19000/26286 | Loss: 1.6552 | LR: 1.00e-04\n",
            "  Batch 19100/26286 | Loss: 1.6527 | LR: 1.00e-04\n",
            "  Batch 19200/26286 | Loss: 1.6503 | LR: 1.00e-04\n",
            "  Batch 19300/26286 | Loss: 1.6479 | LR: 1.00e-04\n",
            "  Batch 19400/26286 | Loss: 1.6457 | LR: 1.00e-04\n",
            "  Batch 19500/26286 | Loss: 1.6433 | LR: 1.00e-04\n",
            "  Batch 19600/26286 | Loss: 1.6410 | LR: 1.00e-04\n",
            "  Batch 19700/26286 | Loss: 1.6386 | LR: 1.00e-04\n",
            "  Batch 19800/26286 | Loss: 1.6362 | LR: 1.00e-04\n",
            "  Batch 19900/26286 | Loss: 1.6339 | LR: 1.00e-04\n",
            "  Batch 20000/26286 | Loss: 1.6317 | LR: 1.00e-04\n",
            "  Batch 20100/26286 | Loss: 1.6294 | LR: 1.00e-04\n",
            "  Batch 20200/26286 | Loss: 1.6272 | LR: 1.00e-04\n",
            "  Batch 20300/26286 | Loss: 1.6250 | LR: 1.00e-04\n",
            "  Batch 20400/26286 | Loss: 1.6228 | LR: 1.00e-04\n",
            "  Batch 20500/26286 | Loss: 1.6207 | LR: 1.00e-04\n",
            "  Batch 20600/26286 | Loss: 1.6185 | LR: 1.00e-04\n",
            "  Batch 20700/26286 | Loss: 1.6163 | LR: 1.00e-04\n",
            "  Batch 20800/26286 | Loss: 1.6143 | LR: 1.00e-04\n",
            "  Batch 20900/26286 | Loss: 1.6122 | LR: 1.00e-04\n",
            "  Batch 21000/26286 | Loss: 1.6100 | LR: 1.00e-04\n",
            "  Batch 21100/26286 | Loss: 1.6080 | LR: 1.00e-04\n",
            "  Batch 21200/26286 | Loss: 1.6060 | LR: 1.00e-04\n",
            "  Batch 21300/26286 | Loss: 1.6040 | LR: 1.00e-04\n",
            "  Batch 21400/26286 | Loss: 1.6020 | LR: 1.00e-04\n",
            "  Batch 21500/26286 | Loss: 1.6000 | LR: 1.00e-04\n",
            "  Batch 21600/26286 | Loss: 1.5980 | LR: 1.00e-04\n",
            "  Batch 21700/26286 | Loss: 1.5961 | LR: 1.00e-04\n",
            "  Batch 21800/26286 | Loss: 1.5941 | LR: 1.00e-04\n",
            "  Batch 21900/26286 | Loss: 1.5923 | LR: 1.00e-04\n",
            "  Batch 22000/26286 | Loss: 1.5903 | LR: 1.00e-04\n",
            "  Batch 22100/26286 | Loss: 1.5885 | LR: 1.00e-04\n",
            "  Batch 22200/26286 | Loss: 1.5867 | LR: 1.00e-04\n",
            "  Batch 22300/26286 | Loss: 1.5848 | LR: 1.00e-04\n",
            "  Batch 22400/26286 | Loss: 1.5829 | LR: 1.00e-04\n",
            "  Batch 22500/26286 | Loss: 1.5811 | LR: 1.00e-04\n",
            "  Batch 22600/26286 | Loss: 1.5793 | LR: 1.00e-04\n",
            "  Batch 22700/26286 | Loss: 1.5775 | LR: 1.00e-04\n",
            "  Batch 22800/26286 | Loss: 1.5756 | LR: 1.00e-04\n",
            "  Batch 22900/26286 | Loss: 1.5739 | LR: 1.00e-04\n",
            "  Batch 23000/26286 | Loss: 1.5721 | LR: 1.00e-04\n",
            "  Batch 23100/26286 | Loss: 1.5704 | LR: 1.00e-04\n",
            "  Batch 23200/26286 | Loss: 1.5687 | LR: 1.00e-04\n",
            "  Batch 23300/26286 | Loss: 1.5670 | LR: 1.00e-04\n",
            "  Batch 23400/26286 | Loss: 1.5653 | LR: 1.00e-04\n",
            "  Batch 23500/26286 | Loss: 1.5636 | LR: 1.00e-04\n",
            "  Batch 23600/26286 | Loss: 1.5619 | LR: 1.00e-04\n",
            "  Batch 23700/26286 | Loss: 1.5602 | LR: 1.00e-04\n",
            "  Batch 23800/26286 | Loss: 1.5586 | LR: 1.00e-04\n",
            "  Batch 23900/26286 | Loss: 1.5571 | LR: 1.00e-04\n",
            "  Batch 24000/26286 | Loss: 1.5553 | LR: 1.00e-04\n",
            "  Batch 24100/26286 | Loss: 1.5536 | LR: 1.00e-04\n",
            "  Batch 24200/26286 | Loss: 1.5520 | LR: 1.00e-04\n",
            "  Batch 24300/26286 | Loss: 1.5505 | LR: 1.00e-04\n",
            "  Batch 24400/26286 | Loss: 1.5488 | LR: 1.00e-04\n",
            "  Batch 24500/26286 | Loss: 1.5472 | LR: 1.00e-04\n",
            "  Batch 24600/26286 | Loss: 1.5457 | LR: 1.00e-04\n",
            "  Batch 24700/26286 | Loss: 1.5441 | LR: 1.00e-04\n",
            "  Batch 24800/26286 | Loss: 1.5426 | LR: 1.00e-04\n",
            "  Batch 24900/26286 | Loss: 1.5409 | LR: 1.00e-04\n",
            "  Batch 25000/26286 | Loss: 1.5395 | LR: 1.00e-04\n",
            "  Batch 25100/26286 | Loss: 1.5379 | LR: 1.00e-04\n",
            "  Batch 25200/26286 | Loss: 1.5365 | LR: 1.00e-04\n",
            "  Batch 25300/26286 | Loss: 1.5350 | LR: 1.00e-04\n",
            "  Batch 25400/26286 | Loss: 1.5334 | LR: 1.00e-04\n",
            "  Batch 25500/26286 | Loss: 1.5319 | LR: 1.00e-04\n",
            "  Batch 25600/26286 | Loss: 1.5305 | LR: 1.00e-04\n",
            "  Batch 25700/26286 | Loss: 1.5291 | LR: 1.00e-04\n",
            "  Batch 25800/26286 | Loss: 1.5276 | LR: 1.00e-04\n",
            "  Batch 25900/26286 | Loss: 1.5262 | LR: 1.00e-04\n",
            "  Batch 26000/26286 | Loss: 1.5247 | LR: 1.00e-04\n",
            "  Batch 26100/26286 | Loss: 1.5233 | LR: 1.00e-04\n",
            "  Batch 26200/26286 | Loss: 1.5218 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.5206\n",
            "  Val Loss:   1.0599 [BEST]\n",
            "  Time:       304.8s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_1.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 2/500\n",
            "  Batch 100/26286 | Loss: 1.1324 | LR: 1.00e-04\n",
            "  Batch 200/26286 | Loss: 1.1367 | LR: 1.00e-04\n",
            "  Batch 300/26286 | Loss: 1.1410 | LR: 1.00e-04\n",
            "  Batch 400/26286 | Loss: 1.1396 | LR: 1.00e-04\n",
            "  Batch 500/26286 | Loss: 1.1390 | LR: 1.00e-04\n",
            "  Batch 600/26286 | Loss: 1.1388 | LR: 1.00e-04\n",
            "  Batch 700/26286 | Loss: 1.1392 | LR: 1.00e-04\n",
            "  Batch 800/26286 | Loss: 1.1393 | LR: 1.00e-04\n",
            "  Batch 900/26286 | Loss: 1.1407 | LR: 1.00e-04\n",
            "  Batch 1000/26286 | Loss: 1.1412 | LR: 1.00e-04\n",
            "  Batch 1100/26286 | Loss: 1.1431 | LR: 1.00e-04\n",
            "  Batch 1200/26286 | Loss: 1.1439 | LR: 1.00e-04\n",
            "  Batch 1300/26286 | Loss: 1.1430 | LR: 1.00e-04\n",
            "  Batch 1400/26286 | Loss: 1.1416 | LR: 1.00e-04\n",
            "  Batch 1500/26286 | Loss: 1.1416 | LR: 1.00e-04\n",
            "  Batch 1600/26286 | Loss: 1.1417 | LR: 1.00e-04\n",
            "  Batch 1700/26286 | Loss: 1.1433 | LR: 1.00e-04\n",
            "  Batch 1800/26286 | Loss: 1.1421 | LR: 1.00e-04\n",
            "  Batch 1900/26286 | Loss: 1.1420 | LR: 1.00e-04\n",
            "  Batch 2000/26286 | Loss: 1.1405 | LR: 1.00e-04\n",
            "  Batch 2100/26286 | Loss: 1.1395 | LR: 1.00e-04\n",
            "  Batch 2200/26286 | Loss: 1.1392 | LR: 1.00e-04\n",
            "  Batch 2300/26286 | Loss: 1.1401 | LR: 1.00e-04\n",
            "  Batch 2400/26286 | Loss: 1.1390 | LR: 1.00e-04\n",
            "  Batch 2500/26286 | Loss: 1.1392 | LR: 1.00e-04\n",
            "  Batch 2600/26286 | Loss: 1.1389 | LR: 1.00e-04\n",
            "  Batch 2700/26286 | Loss: 1.1393 | LR: 1.00e-04\n",
            "  Batch 2800/26286 | Loss: 1.1395 | LR: 1.00e-04\n",
            "  Batch 2900/26286 | Loss: 1.1397 | LR: 1.00e-04\n",
            "  Batch 3000/26286 | Loss: 1.1389 | LR: 1.00e-04\n",
            "  Batch 3100/26286 | Loss: 1.1386 | LR: 1.00e-04\n",
            "  Batch 3200/26286 | Loss: 1.1383 | LR: 1.00e-04\n",
            "  Batch 3300/26286 | Loss: 1.1382 | LR: 1.00e-04\n",
            "  Batch 3400/26286 | Loss: 1.1376 | LR: 1.00e-04\n",
            "  Batch 3500/26286 | Loss: 1.1376 | LR: 1.00e-04\n",
            "  Batch 3600/26286 | Loss: 1.1376 | LR: 1.00e-04\n",
            "  Batch 3700/26286 | Loss: 1.1367 | LR: 1.00e-04\n",
            "  Batch 3800/26286 | Loss: 1.1364 | LR: 1.00e-04\n",
            "  Batch 3900/26286 | Loss: 1.1360 | LR: 1.00e-04\n",
            "  Batch 4000/26286 | Loss: 1.1358 | LR: 1.00e-04\n",
            "  Batch 4100/26286 | Loss: 1.1355 | LR: 1.00e-04\n",
            "  Batch 4200/26286 | Loss: 1.1352 | LR: 1.00e-04\n",
            "  Batch 4300/26286 | Loss: 1.1344 | LR: 1.00e-04\n",
            "  Batch 4400/26286 | Loss: 1.1344 | LR: 1.00e-04\n",
            "  Batch 4500/26286 | Loss: 1.1344 | LR: 1.00e-04\n",
            "  Batch 4600/26286 | Loss: 1.1338 | LR: 1.00e-04\n",
            "  Batch 4700/26286 | Loss: 1.1338 | LR: 1.00e-04\n",
            "  Batch 4800/26286 | Loss: 1.1335 | LR: 1.00e-04\n",
            "  Batch 4900/26286 | Loss: 1.1333 | LR: 1.00e-04\n",
            "  Batch 5000/26286 | Loss: 1.1333 | LR: 1.00e-04\n",
            "  Batch 5100/26286 | Loss: 1.1334 | LR: 1.00e-04\n",
            "  Batch 5200/26286 | Loss: 1.1331 | LR: 1.00e-04\n",
            "  Batch 5300/26286 | Loss: 1.1329 | LR: 1.00e-04\n",
            "  Batch 5400/26286 | Loss: 1.1329 | LR: 1.00e-04\n",
            "  Batch 5500/26286 | Loss: 1.1325 | LR: 1.00e-04\n",
            "  Batch 5600/26286 | Loss: 1.1327 | LR: 1.00e-04\n",
            "  Batch 5700/26286 | Loss: 1.1325 | LR: 1.00e-04\n",
            "  Batch 5800/26286 | Loss: 1.1326 | LR: 1.00e-04\n",
            "  Batch 5900/26286 | Loss: 1.1325 | LR: 1.00e-04\n",
            "  Batch 6000/26286 | Loss: 1.1323 | LR: 1.00e-04\n",
            "  Batch 6100/26286 | Loss: 1.1320 | LR: 1.00e-04\n",
            "  Batch 6200/26286 | Loss: 1.1321 | LR: 1.00e-04\n",
            "  Batch 6300/26286 | Loss: 1.1318 | LR: 1.00e-04\n",
            "  Batch 6400/26286 | Loss: 1.1319 | LR: 1.00e-04\n",
            "  Batch 6500/26286 | Loss: 1.1318 | LR: 1.00e-04\n",
            "  Batch 6600/26286 | Loss: 1.1314 | LR: 1.00e-04\n",
            "  Batch 6700/26286 | Loss: 1.1312 | LR: 1.00e-04\n",
            "  Batch 6800/26286 | Loss: 1.1311 | LR: 1.00e-04\n",
            "  Batch 6900/26286 | Loss: 1.1307 | LR: 1.00e-04\n",
            "  Batch 7000/26286 | Loss: 1.1306 | LR: 1.00e-04\n",
            "  Batch 7100/26286 | Loss: 1.1304 | LR: 1.00e-04\n",
            "  Batch 7200/26286 | Loss: 1.1302 | LR: 1.00e-04\n",
            "  Batch 7300/26286 | Loss: 1.1299 | LR: 1.00e-04\n",
            "  Batch 7400/26286 | Loss: 1.1296 | LR: 1.00e-04\n",
            "  Batch 7500/26286 | Loss: 1.1294 | LR: 1.00e-04\n",
            "  Batch 7600/26286 | Loss: 1.1291 | LR: 1.00e-04\n",
            "  Batch 7700/26286 | Loss: 1.1289 | LR: 1.00e-04\n",
            "  Batch 7800/26286 | Loss: 1.1285 | LR: 1.00e-04\n",
            "  Batch 7900/26286 | Loss: 1.1284 | LR: 1.00e-04\n",
            "  Batch 8000/26286 | Loss: 1.1282 | LR: 1.00e-04\n",
            "  Batch 8100/26286 | Loss: 1.1282 | LR: 1.00e-04\n",
            "  Batch 8200/26286 | Loss: 1.1279 | LR: 1.00e-04\n",
            "  Batch 8300/26286 | Loss: 1.1275 | LR: 1.00e-04\n",
            "  Batch 8400/26286 | Loss: 1.1274 | LR: 1.00e-04\n",
            "  Batch 8500/26286 | Loss: 1.1274 | LR: 1.00e-04\n",
            "  Batch 8600/26286 | Loss: 1.1274 | LR: 1.00e-04\n",
            "  Batch 8700/26286 | Loss: 1.1271 | LR: 1.00e-04\n",
            "  Batch 8800/26286 | Loss: 1.1269 | LR: 1.00e-04\n",
            "  Batch 8900/26286 | Loss: 1.1266 | LR: 1.00e-04\n",
            "  Batch 9000/26286 | Loss: 1.1265 | LR: 1.00e-04\n",
            "  Batch 9100/26286 | Loss: 1.1262 | LR: 1.00e-04\n",
            "  Batch 9200/26286 | Loss: 1.1260 | LR: 1.00e-04\n",
            "  Batch 9300/26286 | Loss: 1.1257 | LR: 1.00e-04\n",
            "  Batch 9400/26286 | Loss: 1.1253 | LR: 1.00e-04\n",
            "  Batch 9500/26286 | Loss: 1.1252 | LR: 1.00e-04\n",
            "  Batch 9600/26286 | Loss: 1.1250 | LR: 1.00e-04\n",
            "  Batch 9700/26286 | Loss: 1.1248 | LR: 1.00e-04\n",
            "  Batch 9800/26286 | Loss: 1.1248 | LR: 1.00e-04\n",
            "  Batch 9900/26286 | Loss: 1.1248 | LR: 1.00e-04\n",
            "  Batch 10000/26286 | Loss: 1.1246 | LR: 1.00e-04\n",
            "  Batch 10100/26286 | Loss: 1.1243 | LR: 1.00e-04\n",
            "  Batch 10200/26286 | Loss: 1.1242 | LR: 1.00e-04\n",
            "  Batch 10300/26286 | Loss: 1.1239 | LR: 1.00e-04\n",
            "  Batch 10400/26286 | Loss: 1.1237 | LR: 1.00e-04\n",
            "  Batch 10500/26286 | Loss: 1.1236 | LR: 1.00e-04\n",
            "  Batch 10600/26286 | Loss: 1.1234 | LR: 1.00e-04\n",
            "  Batch 10700/26286 | Loss: 1.1231 | LR: 1.00e-04\n",
            "  Batch 10800/26286 | Loss: 1.1229 | LR: 1.00e-04\n",
            "  Batch 10900/26286 | Loss: 1.1228 | LR: 1.00e-04\n",
            "  Batch 11000/26286 | Loss: 1.1226 | LR: 1.00e-04\n",
            "  Batch 11100/26286 | Loss: 1.1225 | LR: 1.00e-04\n",
            "  Batch 11200/26286 | Loss: 1.1222 | LR: 1.00e-04\n",
            "  Batch 11300/26286 | Loss: 1.1221 | LR: 1.00e-04\n",
            "  Batch 11400/26286 | Loss: 1.1219 | LR: 1.00e-04\n",
            "  Batch 11500/26286 | Loss: 1.1216 | LR: 1.00e-04\n",
            "  Batch 11600/26286 | Loss: 1.1214 | LR: 1.00e-04\n",
            "  Batch 11700/26286 | Loss: 1.1213 | LR: 1.00e-04\n",
            "  Batch 11800/26286 | Loss: 1.1212 | LR: 1.00e-04\n",
            "  Batch 11900/26286 | Loss: 1.1210 | LR: 1.00e-04\n",
            "  Batch 12000/26286 | Loss: 1.1206 | LR: 1.00e-04\n",
            "  Batch 12100/26286 | Loss: 1.1205 | LR: 1.00e-04\n",
            "  Batch 12200/26286 | Loss: 1.1202 | LR: 1.00e-04\n",
            "  Batch 12300/26286 | Loss: 1.1202 | LR: 1.00e-04\n",
            "  Batch 12400/26286 | Loss: 1.1200 | LR: 1.00e-04\n",
            "  Batch 12500/26286 | Loss: 1.1198 | LR: 1.00e-04\n",
            "  Batch 12600/26286 | Loss: 1.1198 | LR: 1.00e-04\n",
            "  Batch 12700/26286 | Loss: 1.1197 | LR: 1.00e-04\n",
            "  Batch 12800/26286 | Loss: 1.1195 | LR: 1.00e-04\n",
            "  Batch 12900/26286 | Loss: 1.1194 | LR: 1.00e-04\n",
            "  Batch 13000/26286 | Loss: 1.1192 | LR: 1.00e-04\n",
            "  Batch 13100/26286 | Loss: 1.1190 | LR: 1.00e-04\n",
            "  Batch 13200/26286 | Loss: 1.1187 | LR: 1.00e-04\n",
            "  Batch 13300/26286 | Loss: 1.1187 | LR: 1.00e-04\n",
            "  Batch 13400/26286 | Loss: 1.1186 | LR: 1.00e-04\n",
            "  Batch 13500/26286 | Loss: 1.1185 | LR: 1.00e-04\n",
            "  Batch 13600/26286 | Loss: 1.1183 | LR: 1.00e-04\n",
            "  Batch 13700/26286 | Loss: 1.1180 | LR: 1.00e-04\n",
            "  Batch 13800/26286 | Loss: 1.1178 | LR: 1.00e-04\n",
            "  Batch 13900/26286 | Loss: 1.1176 | LR: 1.00e-04\n",
            "  Batch 14000/26286 | Loss: 1.1174 | LR: 1.00e-04\n",
            "  Batch 14100/26286 | Loss: 1.1171 | LR: 1.00e-04\n",
            "  Batch 14200/26286 | Loss: 1.1170 | LR: 1.00e-04\n",
            "  Batch 14300/26286 | Loss: 1.1167 | LR: 1.00e-04\n",
            "  Batch 14400/26286 | Loss: 1.1164 | LR: 1.00e-04\n",
            "  Batch 14500/26286 | Loss: 1.1162 | LR: 1.00e-04\n",
            "  Batch 14600/26286 | Loss: 1.1158 | LR: 1.00e-04\n",
            "  Batch 14700/26286 | Loss: 1.1157 | LR: 1.00e-04\n",
            "  Batch 14800/26286 | Loss: 1.1154 | LR: 1.00e-04\n",
            "  Batch 14900/26286 | Loss: 1.1152 | LR: 1.00e-04\n",
            "  Batch 15000/26286 | Loss: 1.1151 | LR: 1.00e-04\n",
            "  Batch 15100/26286 | Loss: 1.1149 | LR: 1.00e-04\n",
            "  Batch 15200/26286 | Loss: 1.1147 | LR: 1.00e-04\n",
            "  Batch 15300/26286 | Loss: 1.1146 | LR: 1.00e-04\n",
            "  Batch 15400/26286 | Loss: 1.1143 | LR: 1.00e-04\n",
            "  Batch 15500/26286 | Loss: 1.1141 | LR: 1.00e-04\n",
            "  Batch 15600/26286 | Loss: 1.1139 | LR: 1.00e-04\n",
            "  Batch 15700/26286 | Loss: 1.1136 | LR: 1.00e-04\n",
            "  Batch 15800/26286 | Loss: 1.1136 | LR: 1.00e-04\n",
            "  Batch 15900/26286 | Loss: 1.1134 | LR: 1.00e-04\n",
            "  Batch 16000/26286 | Loss: 1.1131 | LR: 1.00e-04\n",
            "  Batch 16100/26286 | Loss: 1.1129 | LR: 1.00e-04\n",
            "  Batch 16200/26286 | Loss: 1.1128 | LR: 1.00e-04\n",
            "  Batch 16300/26286 | Loss: 1.1125 | LR: 1.00e-04\n",
            "  Batch 16400/26286 | Loss: 1.1124 | LR: 1.00e-04\n",
            "  Batch 16500/26286 | Loss: 1.1122 | LR: 1.00e-04\n",
            "  Batch 16600/26286 | Loss: 1.1120 | LR: 1.00e-04\n",
            "  Batch 16700/26286 | Loss: 1.1118 | LR: 1.00e-04\n",
            "  Batch 16800/26286 | Loss: 1.1117 | LR: 1.00e-04\n",
            "  Batch 16900/26286 | Loss: 1.1114 | LR: 1.00e-04\n",
            "  Batch 17000/26286 | Loss: 1.1112 | LR: 1.00e-04\n",
            "  Batch 17100/26286 | Loss: 1.1111 | LR: 1.00e-04\n",
            "  Batch 17200/26286 | Loss: 1.1109 | LR: 1.00e-04\n",
            "  Batch 17300/26286 | Loss: 1.1108 | LR: 1.00e-04\n",
            "  Batch 17400/26286 | Loss: 1.1107 | LR: 1.00e-04\n",
            "  Batch 17500/26286 | Loss: 1.1106 | LR: 1.00e-04\n",
            "  Batch 17600/26286 | Loss: 1.1104 | LR: 1.00e-04\n",
            "  Batch 17700/26286 | Loss: 1.1102 | LR: 1.00e-04\n",
            "  Batch 17800/26286 | Loss: 1.1101 | LR: 1.00e-04\n",
            "  Batch 17900/26286 | Loss: 1.1098 | LR: 1.00e-04\n",
            "  Batch 18000/26286 | Loss: 1.1096 | LR: 1.00e-04\n",
            "  Batch 18100/26286 | Loss: 1.1094 | LR: 1.00e-04\n",
            "  Batch 18200/26286 | Loss: 1.1093 | LR: 1.00e-04\n",
            "  Batch 18300/26286 | Loss: 1.1092 | LR: 1.00e-04\n",
            "  Batch 18400/26286 | Loss: 1.1092 | LR: 1.00e-04\n",
            "  Batch 18500/26286 | Loss: 1.1090 | LR: 1.00e-04\n",
            "  Batch 18600/26286 | Loss: 1.1088 | LR: 1.00e-04\n",
            "  Batch 18700/26286 | Loss: 1.1087 | LR: 1.00e-04\n",
            "  Batch 18800/26286 | Loss: 1.1086 | LR: 1.00e-04\n",
            "  Batch 18900/26286 | Loss: 1.1085 | LR: 1.00e-04\n",
            "  Batch 19000/26286 | Loss: 1.1084 | LR: 1.00e-04\n",
            "  Batch 19100/26286 | Loss: 1.1084 | LR: 1.00e-04\n",
            "  Batch 19200/26286 | Loss: 1.1083 | LR: 1.00e-04\n",
            "  Batch 19300/26286 | Loss: 1.1081 | LR: 1.00e-04\n",
            "  Batch 19400/26286 | Loss: 1.1078 | LR: 1.00e-04\n",
            "  Batch 19500/26286 | Loss: 1.1078 | LR: 1.00e-04\n",
            "  Batch 19600/26286 | Loss: 1.1076 | LR: 1.00e-04\n",
            "  Batch 19700/26286 | Loss: 1.1074 | LR: 1.00e-04\n",
            "  Batch 19800/26286 | Loss: 1.1073 | LR: 1.00e-04\n",
            "  Batch 19900/26286 | Loss: 1.1072 | LR: 1.00e-04\n",
            "  Batch 20000/26286 | Loss: 1.1070 | LR: 1.00e-04\n",
            "  Batch 20100/26286 | Loss: 1.1068 | LR: 1.00e-04\n",
            "  Batch 20200/26286 | Loss: 1.1065 | LR: 1.00e-04\n",
            "  Batch 20300/26286 | Loss: 1.1064 | LR: 1.00e-04\n",
            "  Batch 20400/26286 | Loss: 1.1063 | LR: 1.00e-04\n",
            "  Batch 20500/26286 | Loss: 1.1062 | LR: 1.00e-04\n",
            "  Batch 20600/26286 | Loss: 1.1060 | LR: 1.00e-04\n",
            "  Batch 20700/26286 | Loss: 1.1058 | LR: 1.00e-04\n",
            "  Batch 20800/26286 | Loss: 1.1056 | LR: 1.00e-04\n",
            "  Batch 20900/26286 | Loss: 1.1054 | LR: 1.00e-04\n",
            "  Batch 21000/26286 | Loss: 1.1053 | LR: 1.00e-04\n",
            "  Batch 21100/26286 | Loss: 1.1051 | LR: 1.00e-04\n",
            "  Batch 21200/26286 | Loss: 1.1050 | LR: 1.00e-04\n",
            "  Batch 21300/26286 | Loss: 1.1048 | LR: 1.00e-04\n",
            "  Batch 21400/26286 | Loss: 1.1046 | LR: 1.00e-04\n",
            "  Batch 21500/26286 | Loss: 1.1045 | LR: 1.00e-04\n",
            "  Batch 21600/26286 | Loss: 1.1043 | LR: 1.00e-04\n",
            "  Batch 21700/26286 | Loss: 1.1041 | LR: 1.00e-04\n",
            "  Batch 21800/26286 | Loss: 1.1039 | LR: 1.00e-04\n",
            "  Batch 21900/26286 | Loss: 1.1036 | LR: 1.00e-04\n",
            "  Batch 22000/26286 | Loss: 1.1034 | LR: 1.00e-04\n",
            "  Batch 22100/26286 | Loss: 1.1032 | LR: 1.00e-04\n",
            "  Batch 22200/26286 | Loss: 1.1030 | LR: 1.00e-04\n",
            "  Batch 22300/26286 | Loss: 1.1028 | LR: 1.00e-04\n",
            "  Batch 22400/26286 | Loss: 1.1026 | LR: 1.00e-04\n",
            "  Batch 22500/26286 | Loss: 1.1025 | LR: 1.00e-04\n",
            "  Batch 22600/26286 | Loss: 1.1022 | LR: 1.00e-04\n",
            "  Batch 22700/26286 | Loss: 1.1021 | LR: 1.00e-04\n",
            "  Batch 22800/26286 | Loss: 1.1019 | LR: 1.00e-04\n",
            "  Batch 22900/26286 | Loss: 1.1017 | LR: 1.00e-04\n",
            "  Batch 23000/26286 | Loss: 1.1015 | LR: 1.00e-04\n",
            "  Batch 23100/26286 | Loss: 1.1013 | LR: 1.00e-04\n",
            "  Batch 23200/26286 | Loss: 1.1011 | LR: 1.00e-04\n",
            "  Batch 23300/26286 | Loss: 1.1010 | LR: 1.00e-04\n",
            "  Batch 23400/26286 | Loss: 1.1008 | LR: 1.00e-04\n",
            "  Batch 23500/26286 | Loss: 1.1007 | LR: 1.00e-04\n",
            "  Batch 23600/26286 | Loss: 1.1006 | LR: 1.00e-04\n",
            "  Batch 23700/26286 | Loss: 1.1005 | LR: 1.00e-04\n",
            "  Batch 23800/26286 | Loss: 1.1003 | LR: 1.00e-04\n",
            "  Batch 23900/26286 | Loss: 1.1001 | LR: 1.00e-04\n",
            "  Batch 24000/26286 | Loss: 1.1000 | LR: 1.00e-04\n",
            "  Batch 24100/26286 | Loss: 1.0998 | LR: 1.00e-04\n",
            "  Batch 24200/26286 | Loss: 1.0997 | LR: 1.00e-04\n",
            "  Batch 24300/26286 | Loss: 1.0995 | LR: 1.00e-04\n",
            "  Batch 24400/26286 | Loss: 1.0992 | LR: 1.00e-04\n",
            "  Batch 24500/26286 | Loss: 1.0991 | LR: 1.00e-04\n",
            "  Batch 24600/26286 | Loss: 1.0990 | LR: 1.00e-04\n",
            "  Batch 24700/26286 | Loss: 1.0989 | LR: 1.00e-04\n",
            "  Batch 24800/26286 | Loss: 1.0986 | LR: 1.00e-04\n",
            "  Batch 24900/26286 | Loss: 1.0985 | LR: 1.00e-04\n",
            "  Batch 25000/26286 | Loss: 1.0983 | LR: 1.00e-04\n",
            "  Batch 25100/26286 | Loss: 1.0982 | LR: 1.00e-04\n",
            "  Batch 25200/26286 | Loss: 1.0981 | LR: 1.00e-04\n",
            "  Batch 25300/26286 | Loss: 1.0980 | LR: 1.00e-04\n",
            "  Batch 25400/26286 | Loss: 1.0979 | LR: 1.00e-04\n",
            "  Batch 25500/26286 | Loss: 1.0977 | LR: 1.00e-04\n",
            "  Batch 25600/26286 | Loss: 1.0977 | LR: 1.00e-04\n",
            "  Batch 25700/26286 | Loss: 1.0975 | LR: 1.00e-04\n",
            "  Batch 25800/26286 | Loss: 1.0973 | LR: 1.00e-04\n",
            "  Batch 25900/26286 | Loss: 1.0972 | LR: 1.00e-04\n",
            "  Batch 26000/26286 | Loss: 1.0970 | LR: 1.00e-04\n",
            "  Batch 26100/26286 | Loss: 1.0969 | LR: 1.00e-04\n",
            "  Batch 26200/26286 | Loss: 1.0968 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.0966\n",
            "  Val Loss:   1.0011 [BEST]\n",
            "  Time:       316.1s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_2.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 3/500\n",
            "  Batch 100/26286 | Loss: 1.0778 | LR: 1.00e-04\n",
            "  Batch 200/26286 | Loss: 1.0720 | LR: 1.00e-04\n",
            "  Batch 300/26286 | Loss: 1.0645 | LR: 1.00e-04\n",
            "  Batch 400/26286 | Loss: 1.0671 | LR: 1.00e-04\n",
            "  Batch 500/26286 | Loss: 1.0665 | LR: 1.00e-04\n",
            "  Batch 600/26286 | Loss: 1.0657 | LR: 1.00e-04\n",
            "  Batch 700/26286 | Loss: 1.0654 | LR: 1.00e-04\n",
            "  Batch 800/26286 | Loss: 1.0657 | LR: 1.00e-04\n",
            "  Batch 900/26286 | Loss: 1.0672 | LR: 1.00e-04\n",
            "  Batch 1000/26286 | Loss: 1.0671 | LR: 1.00e-04\n",
            "  Batch 1100/26286 | Loss: 1.0649 | LR: 1.00e-04\n",
            "  Batch 1200/26286 | Loss: 1.0636 | LR: 1.00e-04\n",
            "  Batch 1300/26286 | Loss: 1.0629 | LR: 1.00e-04\n",
            "  Batch 1400/26286 | Loss: 1.0627 | LR: 1.00e-04\n",
            "  Batch 1500/26286 | Loss: 1.0626 | LR: 1.00e-04\n",
            "  Batch 1600/26286 | Loss: 1.0616 | LR: 1.00e-04\n",
            "  Batch 1700/26286 | Loss: 1.0605 | LR: 1.00e-04\n",
            "  Batch 1800/26286 | Loss: 1.0607 | LR: 1.00e-04\n",
            "  Batch 1900/26286 | Loss: 1.0614 | LR: 1.00e-04\n",
            "  Batch 2000/26286 | Loss: 1.0621 | LR: 1.00e-04\n",
            "  Batch 2100/26286 | Loss: 1.0620 | LR: 1.00e-04\n",
            "  Batch 2200/26286 | Loss: 1.0617 | LR: 1.00e-04\n",
            "  Batch 2300/26286 | Loss: 1.0611 | LR: 1.00e-04\n",
            "  Batch 2400/26286 | Loss: 1.0613 | LR: 1.00e-04\n",
            "  Batch 2500/26286 | Loss: 1.0610 | LR: 1.00e-04\n",
            "  Batch 2600/26286 | Loss: 1.0600 | LR: 1.00e-04\n",
            "  Batch 2700/26286 | Loss: 1.0586 | LR: 1.00e-04\n",
            "  Batch 2800/26286 | Loss: 1.0586 | LR: 1.00e-04\n",
            "  Batch 2900/26286 | Loss: 1.0582 | LR: 1.00e-04\n",
            "  Batch 3000/26286 | Loss: 1.0581 | LR: 1.00e-04\n",
            "  Batch 3100/26286 | Loss: 1.0580 | LR: 1.00e-04\n",
            "  Batch 3200/26286 | Loss: 1.0584 | LR: 1.00e-04\n",
            "  Batch 3300/26286 | Loss: 1.0578 | LR: 1.00e-04\n",
            "  Batch 3400/26286 | Loss: 1.0581 | LR: 1.00e-04\n",
            "  Batch 3500/26286 | Loss: 1.0580 | LR: 1.00e-04\n",
            "  Batch 3600/26286 | Loss: 1.0579 | LR: 1.00e-04\n",
            "  Batch 3700/26286 | Loss: 1.0580 | LR: 1.00e-04\n",
            "  Batch 3800/26286 | Loss: 1.0581 | LR: 1.00e-04\n",
            "  Batch 3900/26286 | Loss: 1.0575 | LR: 1.00e-04\n",
            "  Batch 4000/26286 | Loss: 1.0572 | LR: 1.00e-04\n",
            "  Batch 4100/26286 | Loss: 1.0570 | LR: 1.00e-04\n",
            "  Batch 4200/26286 | Loss: 1.0564 | LR: 1.00e-04\n",
            "  Batch 4300/26286 | Loss: 1.0562 | LR: 1.00e-04\n",
            "  Batch 4400/26286 | Loss: 1.0566 | LR: 1.00e-04\n"
          ]
        }
      ],
      "source": [
        "#@title 4. Train Model { display-mode: \"form\" }\n",
        "#@markdown Training will begin with the configuration from Step 2.\n",
        "#@markdown\n",
        "#@markdown **What to watch for:**\n",
        "#@markdown - Loss should decrease over epochs\n",
        "#@markdown - Val loss < 1.0 is good progress\n",
        "#@markdown - Val loss < 0.85 is excellent\n",
        "#@markdown - `[BEST]` indicates a new best checkpoint was saved\n",
        "#@markdown - **Stop if val loss starts rising** (overfitting)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {config['model_size']} | Epochs: {config['num_epochs']}\")\n",
        "if config['resume']:\n",
        "    print(\"Resuming from last checkpoint...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Build training command\n",
        "# Note: batch-size defaults to 64 in train.py\n",
        "cmd = f\"python training/train.py --model-size {config['model_size']} --num-epochs {config['num_epochs']}\"\n",
        "if config['resume']:\n",
        "    cmd += \" --resume\"\n",
        "\n",
        "# Run training\n",
        "!{cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüìÅ Saved checkpoints:\")\n",
        "!ls -lh checkpoints/*.pt 2>/dev/null | tail -5\n",
        "print(\"\\nBest model saved to: checkpoints/best_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK5pG240TFHK"
      },
      "source": [
        "## 5Ô∏è‚É£ Evaluate Model (Optional)\n",
        "Run evaluation on the test set to see accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqNCfzUXTFHK",
        "outputId": "32845b67-2671-4973-8c5c-c984a27dbcc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Evaluating model on test set...\n",
            "\n",
            "\n",
            "============================================================\n",
            "EVALUATING TRAINED MODEL\n",
            "============================================================\n",
            "\n",
            "Loading vocabulary...\n",
            "‚úì Vocabulary size: 662\n",
            "\n",
            "Loading model...\n",
            "‚úì Created tiny model with 566,934 parameters\n",
            "‚úì Loaded model from epoch 33\n",
            "‚úì Best val loss: 0.9493\n",
            "\n",
            "Loading test data...\n",
            "‚úì Test samples: 82146\n",
            "\n",
            "============================================================\n",
            "ACCURACY METRICS\n",
            "============================================================\n",
            "\n",
            "Computing top-1 accuracy...\n",
            "‚úì Top-1 Accuracy: 26.41%\n",
            "  (26.4% chance of exact next symbol)\n",
            "\n",
            "Computing top-5 accuracy...\n",
            "object address  : 0x780987d631c0\n",
            "object refcount : 3\n",
            "object type     : 0xa2a4e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "#@title 5. Evaluate Model { display-mode: \"form\" }\n",
        "#@markdown Run evaluation on the test set.\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"üìä Evaluating model on test set...\\n\")\n",
        "\n",
        "!python evaluate_model.py \\\n",
        "    --checkpoint checkpoints/best_model.pt \\\n",
        "    --test-data datasets/fol_next_symbol/test.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7uXvIWTFHK"
      },
      "source": [
        "## 6Ô∏è‚É£ Download Trained Model (Optional)\n",
        "Download the trained model checkpoint to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uImTbmmlTFHK"
      },
      "outputs": [],
      "source": [
        "#@title 6. Download Model { display-mode: \"form\" }\n",
        "#@markdown Download the best model checkpoint.\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "checkpoint_path = '/content/Symbolic-Transformers/checkpoints/best_model.pt'\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"üì¶ Preparing download...\")\n",
        "    file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
        "    print(f\"   File: best_model.pt ({file_size:.1f} MB)\")\n",
        "    print(f\"   Model: {config['model_size']}\")\n",
        "    print()\n",
        "    files.download(checkpoint_path)\n",
        "else:\n",
        "    print(\"‚ùå No checkpoint found. Run training first (Step 4).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2xHc6xzTFHK"
      },
      "source": [
        "## 7Ô∏è‚É£ Interactive Demo (Optional)\n",
        "Try the model interactively - type tokens and see predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ocH3f-STFHK"
      },
      "outputs": [],
      "source": [
        "#@title 7. Quick Demo { display-mode: \"form\" }\n",
        "#@markdown See the model's predictions for a sample input.\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "sys.path.insert(0, '/content/Symbolic-Transformers')\n",
        "\n",
        "from utils.vocabulary import Vocabulary\n",
        "from models.transformer import SymbolicTransformer, get_model_config\n",
        "\n",
        "# Load model\n",
        "print(\"üîÑ Loading model...\")\n",
        "vocab = Vocabulary('unified_vocabulary.json')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "checkpoint = torch.load('checkpoints/best_model.pt', map_location=device, weights_only=False)\n",
        "model_config = get_model_config(checkpoint['config']['model_size'], vocab.vocab_size)\n",
        "model = SymbolicTransformer(**model_config).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úì Loaded {checkpoint['config']['model_size']} model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Val loss: {checkpoint['best_val_loss']:.4f}\\n\")\n",
        "\n",
        "# Demo predictions\n",
        "def predict_next(tokens, top_k=5):\n",
        "    \"\"\"Predict next token given a sequence.\"\"\"\n",
        "    token_ids = [vocab.encode_label(t) if t in vocab.label_to_id else int(t) for t in tokens]\n",
        "    x = torch.tensor([token_ids], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = torch.softmax(logits[0, -1], dim=-1)\n",
        "        top_probs, top_ids = torch.topk(probs, top_k)\n",
        "\n",
        "    print(f\"Input: {' '.join(tokens)}\")\n",
        "    print(f\"\\nTop {top_k} predictions:\")\n",
        "    for i, (prob, tid) in enumerate(zip(top_probs, top_ids)):\n",
        "        label = vocab.decode_id(tid.item())\n",
        "        bar = '‚ñà' * int(prob * 30)\n",
        "        print(f\"  {i+1}. {label:12s} {prob*100:5.1f}% {bar}\")\n",
        "    print()\n",
        "\n",
        "# Show example predictions\n",
        "print(\"=\"*50)\n",
        "print(\"üìä EXAMPLE PREDICTIONS\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# After FORALL, should predict VAR with high confidence\n",
        "predict_next(['FORALL'])\n",
        "\n",
        "# After FORALL VAR, should predict a numeral\n",
        "predict_next(['FORALL', 'VAR'])\n",
        "\n",
        "# After a complete variable binding\n",
        "predict_next(['FORALL', 'VAR', '1'])\n",
        "\n",
        "# After predicate (should predict LPAREN)\n",
        "predict_next(['PRED', '3'])\n",
        "\n",
        "print(\"\\nüí° The model learned FOL syntax rules!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N6cg1OvTFHL"
      },
      "source": [
        "---\n",
        "\n",
        "## üìñ Quick Reference\n",
        "\n",
        "### Model Sizes\n",
        "| Size | Parameters | File Size | Time/Epoch (A100) | Capacity |\n",
        "|------|-----------|-----------|-------------------|----------|\n",
        "| tiny | 566K | ~2.2MB | ~30s | Good for <10K formulas |\n",
        "| small | 3.5M | ~14MB | ~90s | Good for 10-50K formulas |\n",
        "| base | 19.6M | ~78MB | ~180s | Good for 50K+ formulas |\n",
        "\n",
        "### Data Generator Comparison\n",
        "| Feature | Basic | Advanced |\n",
        "|---------|-------|----------|\n",
        "| Predicates `P(x, y)` | ‚úì | ‚úì |\n",
        "| Functions `f(x)` | ‚úó | ‚úì |\n",
        "| Nested terms `P(f(g(x)))` | ‚úó | ‚úì |\n",
        "| Fixed arities | ‚úó | ‚úì |\n",
        "| Horn clauses | ‚úó | ‚úì |\n",
        "| Vacuous quantification | ‚úó | ‚úì |\n",
        "\n",
        "### Recommended Configurations\n",
        "\n",
        "**Quick Test (5-10 min):**\n",
        "- Model: tiny\n",
        "- Data: 1000 formulas (basic)\n",
        "- Epochs: 20\n",
        "\n",
        "**Standard Training (30-60 min):**\n",
        "- Model: small\n",
        "- Data: 10000 formulas (advanced)\n",
        "- Epochs: 50-100\n",
        "\n",
        "**Best Results (2+ hours):**\n",
        "- Model: base\n",
        "- Data: 30000+ formulas (advanced)\n",
        "- Epochs: 100-200 (watch val loss!)\n",
        "\n",
        "### Interpreting Results\n",
        "- **Val Loss > 1.5**: Model is still learning basic patterns\n",
        "- **Val Loss 1.0-1.5**: Good progress, learning syntax rules\n",
        "- **Val Loss 0.85-1.0**: Excellent, model understands FOL structure\n",
        "- **Val Loss < 0.85**: Very good, approaching optimal\n",
        "\n",
        "### ‚ö†Ô∏è Overfitting Warning Signs\n",
        "- Train loss keeps dropping but val loss stops improving\n",
        "- Val loss starts **increasing** while train loss decreases\n",
        "- Gap between train and val loss > 0.1\n",
        "\n",
        "**If overfitting:**\n",
        "1. Stop training and use the checkpoint with lowest val loss\n",
        "2. Generate more training data\n",
        "3. Enable advanced generator for richer patterns\n",
        "\n",
        "### Files Created\n",
        "- `checkpoints/best_model.pt` - Best performing model\n",
        "- `checkpoints/checkpoint_epoch_N.pt` - Periodic checkpoints\n",
        "- `datasets/fol_next_symbol/` - Training data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}