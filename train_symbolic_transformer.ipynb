{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripptytrip/Symbolic-Transformers/blob/main/train_symbolic_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGtE5_xgTFHG"
      },
      "source": [
        "# üß† Symbolic Transformer Training\n",
        "\n",
        "Train a tiny transformer to predict next symbols in First-Order Logic formulas.\n",
        "\n",
        "**What this does:**\n",
        "- Generates synthetic FOL training data\n",
        "- Trains a small transformer model (566K - 19.6M parameters)\n",
        "- Learns syntax rules like: `‚àÄ` ‚Üí must be followed by `VAR`\n",
        "\n",
        "**Quick Start:** Run cells 1-4 in order. Training takes ~30s-90s/epoch on GPU.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF64tUdlTFHI"
      },
      "source": [
        "## 1Ô∏è‚É£ Setup Environment\n",
        "Clone the repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mzVNpxbRTFHI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce75619-4d8f-4fd4-8800-298d85fa59e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checking GPU availability...\n",
            "NVIDIA A100-SXM4-80GB, 81920 MiB\n",
            "\n",
            "üì¶ Cloning Symbolic-Transformers repository...\n",
            "/content\n",
            "‚úì Repository cloned\n",
            "/content/Symbolic-Transformers\n",
            "\n",
            "üìö Installing dependencies...\n",
            "‚úì Dependencies installed\n",
            "\n",
            "üî§ Verifying vocabulary...\n",
            "‚úì Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "‚úì Vocabulary loaded: 662 tokens\n",
            "\n",
            "==================================================\n",
            "‚úÖ Setup complete! Proceed to Step 2.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Setup Environment { display-mode: \"form\" }\n",
        "#@markdown Run this cell first to set up the environment.\n",
        "\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "print(\"üîç Checking GPU availability...\")\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || echo \"‚ö†Ô∏è No GPU detected - training will be slower\"\n",
        "\n",
        "# Clone repository\n",
        "print(\"\\nüì¶ Cloning Symbolic-Transformers repository...\")\n",
        "%cd /content\n",
        "if not os.path.exists('Symbolic-Transformers'):\n",
        "    !git clone -q https://github.com/tripptytrip/Symbolic-Transformers.git\n",
        "    print(\"‚úì Repository cloned\")\n",
        "else:\n",
        "    !cd Symbolic-Transformers && git stash && git pull -q\n",
        "    print(\"‚úì Repository updated\")\n",
        "\n",
        "%cd /content/Symbolic-Transformers\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\nüìö Installing dependencies...\")\n",
        "!pip -q install numpy scipy pandas tqdm rich tensorboard\n",
        "print(\"‚úì Dependencies installed\")\n",
        "\n",
        "# Verify vocabulary\n",
        "print(\"\\nüî§ Verifying vocabulary...\")\n",
        "!python -c \"from utils.vocabulary import Vocabulary; v = Vocabulary('unified_vocabulary.json'); print(f'‚úì Vocabulary loaded: {v.vocab_size} tokens')\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Setup complete! Proceed to Step 2.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPKqzjMATFHI"
      },
      "source": [
        "## 2Ô∏è‚É£ Configure Training\n",
        "Adjust the settings below, then run the cell to apply them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B7xB8AeHTFHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1063c3b-a08b-4542-df8c-b5cc5bb8d845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "üìã TRAINING CONFIGURATION\n",
            "==================================================\n",
            "\n",
            "üèóÔ∏è  Model:     tiny (566K parameters, ~2.2MB)\n",
            "üìä Dataset:   50000 train / 5000 val formulas\n",
            "üß¨ Generator: Advanced (functions, fixed signatures, Horn clauses)\n",
            "‚öôÔ∏è  Training:  100 epochs, batch size 256\n",
            "üîÑ Resume:    No (fresh start)\n",
            "\n",
            "‚è±Ô∏è  Estimated training time: ~39 minutes on A100\n",
            "   (23s per epoch, 2929 batches)\n",
            "\n",
            "==================================================\n",
            "‚úÖ Configuration saved! Proceed to Step 3.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Training Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### üèóÔ∏è Model Size\n",
        "model_size = \"tiny\" #@param [\"tiny\", \"small\", \"base\"]\n",
        "#@markdown - **tiny**: 566K params (~2.2MB) - Fast training, good for experiments\n",
        "#@markdown - **small**: 3.5M params (~14MB) - Better accuracy, moderate training time\n",
        "#@markdown - **base**: 19.6M params (~78MB) - Best accuracy, longest training\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üìä Dataset Size\n",
        "num_train_formulas = 50000 #@param {type:\"slider\", min:1000, max:50000, step:1000}\n",
        "#@markdown Number of unique FOL formulas to generate for training.\n",
        "#@markdown - 1000-3000: Quick experiments\n",
        "#@markdown - 5000-10000: Standard training\n",
        "#@markdown - 20000-50000: Large-scale training (recommended for small/base models)\n",
        "\n",
        "num_val_formulas = 5000 #@param {type:\"slider\", min:100, max:5000, step:100}\n",
        "#@markdown Number of formulas for validation (typically 10-20% of training).\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üß¨ Data Generator\n",
        "use_advanced_generator = True #@param {type:\"boolean\"}\n",
        "#@markdown **Advanced generator** adds:\n",
        "#@markdown - Function symbols: `P(f(x), g(y))` instead of just `P(x, y)`\n",
        "#@markdown - Fixed signatures: `PRED_5` is *always* arity 2 (model learns consistency)\n",
        "#@markdown - Horn clauses: `(A ‚àß B ‚àß C) ‚Üí D` (common logic programming pattern)\n",
        "#@markdown - Vacuous quantification: `‚àÄx P(y)` (tests scope understanding)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ‚öôÔ∏è Training Parameters\n",
        "num_epochs = 100 #@param {type:\"slider\", min:10, max:500, step:10}\n",
        "#@markdown Number of training epochs.\n",
        "#@markdown - 10-30: Quick experiments\n",
        "#@markdown - 50-100: Standard training\n",
        "#@markdown - 100-200: Train to convergence (watch for overfitting!)\n",
        "\n",
        "batch_size = 256 #@param [32, 64, 128, 256] {type:\"raw\"}\n",
        "#@markdown Larger batches = faster training but more memory.\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üíæ Resume from Checkpoint\n",
        "resume_training = False #@param {type:\"boolean\"}\n",
        "#@markdown Resume from the last saved checkpoint.\n",
        "\n",
        "# Store configuration\n",
        "config = {\n",
        "    'model_size': model_size,\n",
        "    'num_train_formulas': num_train_formulas,\n",
        "    'num_val_formulas': num_val_formulas,\n",
        "    'num_test_formulas': max(100, num_val_formulas // 2),\n",
        "    'num_epochs': num_epochs,\n",
        "    'batch_size': batch_size,\n",
        "    'resume': resume_training,\n",
        "    'use_advanced_generator': use_advanced_generator\n",
        "}\n",
        "\n",
        "# Display configuration summary\n",
        "print(\"=\"*50)\n",
        "print(\"üìã TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model_params = {'tiny': '566K', 'small': '3.5M', 'base': '19.6M'}\n",
        "model_size_mb = {'tiny': '~2.2MB', 'small': '~14MB', 'base': '~78MB'}\n",
        "\n",
        "print(f\"\\nüèóÔ∏è  Model:     {model_size} ({model_params[model_size]} parameters, {model_size_mb[model_size]})\")\n",
        "print(f\"üìä Dataset:   {num_train_formulas} train / {num_val_formulas} val formulas\")\n",
        "print(f\"üß¨ Generator: {'Advanced (functions, fixed signatures, Horn clauses)' if use_advanced_generator else 'Basic'}\")\n",
        "print(f\"‚öôÔ∏è  Training:  {num_epochs} epochs, batch size {batch_size}\")\n",
        "print(f\"üîÑ Resume:    {'Yes' if resume_training else 'No (fresh start)'}\")\n",
        "\n",
        "# Estimate training time\n",
        "samples_per_formula = 15 if use_advanced_generator else 14\n",
        "total_samples = num_train_formulas * samples_per_formula\n",
        "batches_per_epoch = total_samples // batch_size\n",
        "time_per_batch = {'tiny': 0.008, 'small': 0.020, 'base': 0.040}  # seconds on A100\n",
        "est_epoch_time = batches_per_epoch * time_per_batch[model_size]\n",
        "est_total_time = est_epoch_time * num_epochs / 60\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Estimated training time: ~{est_total_time:.0f} minutes on A100\")\n",
        "print(f\"   ({est_epoch_time:.0f}s per epoch, {batches_per_epoch} batches)\")\n",
        "\n",
        "# Recommendations\n",
        "if model_size in ['small', 'base'] and num_train_formulas < 10000:\n",
        "    print(f\"\\nüí° TIP: {model_size} model benefits from more data.\")\n",
        "    print(f\"   Consider increasing to 20000+ formulas.\")\n",
        "\n",
        "if num_epochs > 100 and not use_advanced_generator:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: High epochs ({num_epochs}) with basic generator.\")\n",
        "    print(f\"   Risk of overfitting! Consider:\")\n",
        "    print(f\"   - Enabling advanced generator for richer data\")\n",
        "    print(f\"   - Or reducing epochs to 50-100\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Configuration saved! Proceed to Step 3.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VJA9pkTFHJ"
      },
      "source": [
        "## 3Ô∏è‚É£ Generate Training Data\n",
        "Create synthetic First-Order Logic formulas for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "a25zb42qTFHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410bebe9-0686-499d-c95d-1ca5700684bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Generating training data...\n",
            "   Train: 50000 formulas\n",
            "   Val:   5000 formulas\n",
            "   Test:  2500 formulas\n",
            "   Generator: Advanced\n",
            "\n",
            "============================================================\n",
            "ADVANCED FOL DATASET GENERATION\n",
            "============================================================\n",
            "‚úì Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "\n",
            "üìã Fixed Signatures (consistent across all formulas):\n",
            "   Predicates: {0: 2, 1: 1, 2: 1, 3: 1, 4: 2, 5: 2, 6: 2, 7: 1, 8: 2, 9: 1}\n",
            "   Functions:  {0: 1, 1: 1, 2: 1, 3: 1}\n",
            "\n",
            "Generating train set (50000 formulas)...\n",
            "  Complexity 1: 10000 formulas\n",
            "  Complexity 2: 20000 formulas\n",
            "  Complexity 3: 15000 formulas\n",
            "  Complexity 4: 5000 formulas\n",
            "\n",
            "Generating val set (5000 formulas)...\n",
            "  Complexity 1: 1000 formulas\n",
            "  Complexity 2: 2000 formulas\n",
            "  Complexity 3: 1500 formulas\n",
            "  Complexity 4: 500 formulas\n",
            "\n",
            "Generating test set (2500 formulas)...\n",
            "  Complexity 1: 500 formulas\n",
            "  Complexity 2: 1000 formulas\n",
            "  Complexity 3: 750 formulas\n",
            "  Complexity 4: 250 formulas\n",
            "‚úì Saved 1668374 samples to datasets/fol_next_symbol/train.json\n",
            "‚úì Saved 166032 samples to datasets/fol_next_symbol/val.json\n",
            "‚úì Saved 82146 samples to datasets/fol_next_symbol/test.json\n",
            "\n",
            "============================================================\n",
            "‚úì Advanced dataset generation complete!\n",
            "‚úì Output directory: datasets/fol_next_symbol\n",
            "‚úì Train samples: 1668374\n",
            "‚úì Val samples: 166032\n",
            "‚úì Test samples: 82146\n",
            "============================================================\n",
            "\n",
            "üìÅ Dataset files:\n",
            "-rw-r--r-- 1 root root  522 Dec 20 15:21 datasets/fol_next_symbol/metadata.json\n",
            "-rw-r--r-- 1 root root  12M Dec 20 15:21 datasets/fol_next_symbol/test.json\n",
            "-rw-r--r-- 1 root root 232M Dec 20 15:21 datasets/fol_next_symbol/train.json\n",
            "-rw-r--r-- 1 root root  24M Dec 20 15:21 datasets/fol_next_symbol/val.json\n",
            "\n",
            "==================================================\n",
            "‚úÖ Data generated! Proceed to Step 4.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 3. Generate Training Data { display-mode: \"form\" }\n",
        "#@markdown This generates synthetic FOL formulas.\n",
        "#@markdown\n",
        "#@markdown **Basic generator examples:**\n",
        "#@markdown - `‚àÄx‚ÇÅ (P‚ÇÖ(x‚ÇÅ) ‚Üí Q‚ÇÇ(x‚ÇÅ))`\n",
        "#@markdown - `‚àÉx‚ÇÄ ‚àÉx‚ÇÅ (R‚ÇÉ(x‚ÇÄ, x‚ÇÅ) ‚àß P‚ÇÅ(x‚ÇÄ))`\n",
        "#@markdown\n",
        "#@markdown **Advanced generator adds:**\n",
        "#@markdown - `‚àÄx P(f(x), g(x, y))` (function symbols)\n",
        "#@markdown - `(A ‚àß B ‚àß C) ‚Üí D` (Horn clauses)\n",
        "#@markdown - Fixed predicate arities across all formulas\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"üîÑ Generating training data...\")\n",
        "print(f\"   Train: {config['num_train_formulas']} formulas\")\n",
        "print(f\"   Val:   {config['num_val_formulas']} formulas\")\n",
        "print(f\"   Test:  {config['num_test_formulas']} formulas\")\n",
        "print(f\"   Generator: {'Advanced' if config['use_advanced_generator'] else 'Basic'}\")\n",
        "print()\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '/content/Symbolic-Transformers')\n",
        "\n",
        "if config['use_advanced_generator']:\n",
        "    # Use advanced generator with functions, fixed signatures, Horn clauses\n",
        "    from data.advanced_generator import generate_advanced_training_data\n",
        "\n",
        "    generate_advanced_training_data(\n",
        "        vocab_path=\"unified_vocabulary.json\",\n",
        "        output_dir=\"datasets/fol_next_symbol\",\n",
        "        n_train=config['num_train_formulas'],\n",
        "        n_val=config['num_val_formulas'],\n",
        "        n_test=config['num_test_formulas'],\n",
        "    )\n",
        "else:\n",
        "    # Use basic generator\n",
        "    from data.dataset_generator import generate_training_data\n",
        "\n",
        "    generate_training_data(\n",
        "        vocab_path=\"unified_vocabulary.json\",\n",
        "        output_dir=\"datasets/fol_next_symbol\",\n",
        "        n_train=config['num_train_formulas'],\n",
        "        n_val=config['num_val_formulas'],\n",
        "        n_test=config['num_test_formulas'],\n",
        "    )\n",
        "\n",
        "# Show dataset stats\n",
        "print(\"\\nüìÅ Dataset files:\")\n",
        "!ls -lh datasets/fol_next_symbol/*.json\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Data generated! Proceed to Step 4.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2H8LpcTFHJ"
      },
      "source": [
        "## 4Ô∏è‚É£ Train the Model\n",
        "Start training! Watch the loss decrease over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBTsqEasTFHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67644b0-ab3e-48da-b3b6-1f1d699002c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  Batch 6600/26069 | Loss: 0.9882 | LR: 9.51e-05\n",
            "  Batch 6700/26069 | Loss: 0.9879 | LR: 9.51e-05\n",
            "  Batch 6800/26069 | Loss: 0.9879 | LR: 9.51e-05\n",
            "  Batch 6900/26069 | Loss: 0.9880 | LR: 9.51e-05\n",
            "  Batch 7000/26069 | Loss: 0.9880 | LR: 9.51e-05\n",
            "  Batch 7100/26069 | Loss: 0.9879 | LR: 9.51e-05\n",
            "  Batch 7200/26069 | Loss: 0.9879 | LR: 9.51e-05\n",
            "  Batch 7300/26069 | Loss: 0.9882 | LR: 9.51e-05\n",
            "  Batch 7400/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 7500/26069 | Loss: 0.9883 | LR: 9.51e-05\n",
            "  Batch 7600/26069 | Loss: 0.9882 | LR: 9.51e-05\n",
            "  Batch 7700/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 7800/26069 | Loss: 0.9885 | LR: 9.51e-05\n",
            "  Batch 7900/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8000/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8100/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8200/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8300/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8400/26069 | Loss: 0.9885 | LR: 9.51e-05\n",
            "  Batch 8500/26069 | Loss: 0.9887 | LR: 9.51e-05\n",
            "  Batch 8600/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8700/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8800/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 8900/26069 | Loss: 0.9884 | LR: 9.51e-05\n",
            "  Batch 9000/26069 | Loss: 0.9883 | LR: 9.51e-05\n",
            "  Batch 9100/26069 | Loss: 0.9882 | LR: 9.51e-05\n",
            "  Batch 9200/26069 | Loss: 0.9881 | LR: 9.51e-05\n",
            "  Batch 9300/26069 | Loss: 0.9881 | LR: 9.51e-05\n",
            "  Batch 9400/26069 | Loss: 0.9881 | LR: 9.50e-05\n",
            "  Batch 9500/26069 | Loss: 0.9882 | LR: 9.50e-05\n",
            "  Batch 9600/26069 | Loss: 0.9880 | LR: 9.50e-05\n",
            "  Batch 9700/26069 | Loss: 0.9879 | LR: 9.50e-05\n",
            "  Batch 9800/26069 | Loss: 0.9879 | LR: 9.50e-05\n",
            "  Batch 9900/26069 | Loss: 0.9879 | LR: 9.50e-05\n",
            "  Batch 10000/26069 | Loss: 0.9878 | LR: 9.50e-05\n",
            "  Batch 10100/26069 | Loss: 0.9877 | LR: 9.50e-05\n",
            "  Batch 10200/26069 | Loss: 0.9876 | LR: 9.50e-05\n",
            "  Batch 10300/26069 | Loss: 0.9877 | LR: 9.50e-05\n",
            "  Batch 10400/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 10500/26069 | Loss: 0.9874 | LR: 9.50e-05\n",
            "  Batch 10600/26069 | Loss: 0.9876 | LR: 9.50e-05\n",
            "  Batch 10700/26069 | Loss: 0.9876 | LR: 9.50e-05\n",
            "  Batch 10800/26069 | Loss: 0.9876 | LR: 9.50e-05\n",
            "  Batch 10900/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 11000/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 11100/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 11200/26069 | Loss: 0.9876 | LR: 9.50e-05\n",
            "  Batch 11300/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 11400/26069 | Loss: 0.9878 | LR: 9.50e-05\n",
            "  Batch 11500/26069 | Loss: 0.9877 | LR: 9.50e-05\n",
            "  Batch 11600/26069 | Loss: 0.9879 | LR: 9.50e-05\n",
            "  Batch 11700/26069 | Loss: 0.9878 | LR: 9.50e-05\n",
            "  Batch 11800/26069 | Loss: 0.9880 | LR: 9.50e-05\n",
            "  Batch 11900/26069 | Loss: 0.9880 | LR: 9.50e-05\n",
            "  Batch 12000/26069 | Loss: 0.9881 | LR: 9.50e-05\n",
            "  Batch 12100/26069 | Loss: 0.9880 | LR: 9.50e-05\n",
            "  Batch 12200/26069 | Loss: 0.9879 | LR: 9.50e-05\n",
            "  Batch 12300/26069 | Loss: 0.9878 | LR: 9.50e-05\n",
            "  Batch 12400/26069 | Loss: 0.9878 | LR: 9.50e-05\n",
            "  Batch 12500/26069 | Loss: 0.9879 | LR: 9.50e-05\n",
            "  Batch 12600/26069 | Loss: 0.9877 | LR: 9.50e-05\n",
            "  Batch 12700/26069 | Loss: 0.9876 | LR: 9.50e-05\n",
            "  Batch 12800/26069 | Loss: 0.9876 | LR: 9.50e-05\n",
            "  Batch 12900/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 13000/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 13100/26069 | Loss: 0.9875 | LR: 9.50e-05\n",
            "  Batch 13200/26069 | Loss: 0.9877 | LR: 9.49e-05\n",
            "  Batch 13300/26069 | Loss: 0.9876 | LR: 9.49e-05\n",
            "  Batch 13400/26069 | Loss: 0.9876 | LR: 9.49e-05\n",
            "  Batch 13500/26069 | Loss: 0.9876 | LR: 9.49e-05\n",
            "  Batch 13600/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 13700/26069 | Loss: 0.9876 | LR: 9.49e-05\n",
            "  Batch 13800/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 13900/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 14000/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 14100/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 14200/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 14300/26069 | Loss: 0.9876 | LR: 9.49e-05\n",
            "  Batch 14400/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 14500/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 14600/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 14700/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 14800/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 14900/26069 | Loss: 0.9872 | LR: 9.49e-05\n",
            "  Batch 15000/26069 | Loss: 0.9872 | LR: 9.49e-05\n",
            "  Batch 15100/26069 | Loss: 0.9871 | LR: 9.49e-05\n",
            "  Batch 15200/26069 | Loss: 0.9871 | LR: 9.49e-05\n",
            "  Batch 15300/26069 | Loss: 0.9871 | LR: 9.49e-05\n",
            "  Batch 15400/26069 | Loss: 0.9872 | LR: 9.49e-05\n",
            "  Batch 15500/26069 | Loss: 0.9872 | LR: 9.49e-05\n",
            "  Batch 15600/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 15700/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 15800/26069 | Loss: 0.9874 | LR: 9.49e-05\n",
            "  Batch 15900/26069 | Loss: 0.9874 | LR: 9.49e-05\n",
            "  Batch 16000/26069 | Loss: 0.9874 | LR: 9.49e-05\n",
            "  Batch 16100/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 16200/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 16300/26069 | Loss: 0.9874 | LR: 9.49e-05\n",
            "  Batch 16400/26069 | Loss: 0.9873 | LR: 9.49e-05\n",
            "  Batch 16500/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 16600/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 16700/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 16800/26069 | Loss: 0.9875 | LR: 9.49e-05\n",
            "  Batch 16900/26069 | Loss: 0.9876 | LR: 9.49e-05\n",
            "  Batch 17000/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 17100/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 17200/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 17300/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 17400/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 17500/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 17600/26069 | Loss: 0.9874 | LR: 9.48e-05\n",
            "  Batch 17700/26069 | Loss: 0.9874 | LR: 9.48e-05\n",
            "  Batch 17800/26069 | Loss: 0.9874 | LR: 9.48e-05\n",
            "  Batch 17900/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 18000/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 18100/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 18200/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 18300/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 18400/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 18500/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 18600/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 18700/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 18800/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 18900/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 19000/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 19100/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 19200/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 19300/26069 | Loss: 0.9874 | LR: 9.48e-05\n",
            "  Batch 19400/26069 | Loss: 0.9875 | LR: 9.48e-05\n",
            "  Batch 19500/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 19600/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 19700/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 19800/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 19900/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 20000/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 20100/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 20200/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 20300/26069 | Loss: 0.9878 | LR: 9.48e-05\n",
            "  Batch 20400/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 20500/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 20600/26069 | Loss: 0.9877 | LR: 9.48e-05\n",
            "  Batch 20700/26069 | Loss: 0.9876 | LR: 9.48e-05\n",
            "  Batch 20800/26069 | Loss: 0.9875 | LR: 9.47e-05\n",
            "  Batch 20900/26069 | Loss: 0.9875 | LR: 9.47e-05\n",
            "  Batch 21000/26069 | Loss: 0.9875 | LR: 9.47e-05\n",
            "  Batch 21100/26069 | Loss: 0.9874 | LR: 9.47e-05\n",
            "  Batch 21200/26069 | Loss: 0.9873 | LR: 9.47e-05\n",
            "  Batch 21300/26069 | Loss: 0.9872 | LR: 9.47e-05\n",
            "  Batch 21400/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 21500/26069 | Loss: 0.9872 | LR: 9.47e-05\n",
            "  Batch 21600/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 21700/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 21800/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 21900/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 22000/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 22100/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 22200/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 22300/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 22400/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 22500/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 22600/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 22700/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 22800/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 22900/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 23000/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 23100/26069 | Loss: 0.9872 | LR: 9.47e-05\n",
            "  Batch 23200/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 23300/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 23400/26069 | Loss: 0.9869 | LR: 9.47e-05\n",
            "  Batch 23500/26069 | Loss: 0.9869 | LR: 9.47e-05\n",
            "  Batch 23600/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 23700/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 23800/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 23900/26069 | Loss: 0.9870 | LR: 9.47e-05\n",
            "  Batch 24000/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 24100/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 24200/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 24300/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 24400/26069 | Loss: 0.9871 | LR: 9.47e-05\n",
            "  Batch 24500/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 24600/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 24700/26069 | Loss: 0.9871 | LR: 9.46e-05\n",
            "  Batch 24800/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 24900/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 25000/26069 | Loss: 0.9871 | LR: 9.46e-05\n",
            "  Batch 25100/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 25200/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 25300/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 25400/26069 | Loss: 0.9871 | LR: 9.46e-05\n",
            "  Batch 25500/26069 | Loss: 0.9871 | LR: 9.46e-05\n",
            "  Batch 25600/26069 | Loss: 0.9871 | LR: 9.46e-05\n",
            "  Batch 25700/26069 | Loss: 0.9871 | LR: 9.46e-05\n",
            "  Batch 25800/26069 | Loss: 0.9871 | LR: 9.46e-05\n",
            "  Batch 25900/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "  Batch 26000/26069 | Loss: 0.9870 | LR: 9.46e-05\n",
            "\n",
            "  Train Loss: 0.9870\n",
            "  Val Loss:   0.9621 [BEST]\n",
            "  Time:       314.0s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_15.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 16/100\n",
            "  Batch 100/26069 | Loss: 0.9826 | LR: 9.46e-05\n",
            "  Batch 200/26069 | Loss: 0.9830 | LR: 9.46e-05\n",
            "  Batch 300/26069 | Loss: 0.9831 | LR: 9.46e-05\n",
            "  Batch 400/26069 | Loss: 0.9830 | LR: 9.46e-05\n",
            "  Batch 500/26069 | Loss: 0.9819 | LR: 9.46e-05\n",
            "  Batch 600/26069 | Loss: 0.9814 | LR: 9.46e-05\n",
            "  Batch 700/26069 | Loss: 0.9838 | LR: 9.46e-05\n",
            "  Batch 800/26069 | Loss: 0.9865 | LR: 9.46e-05\n",
            "  Batch 900/26069 | Loss: 0.9868 | LR: 9.46e-05\n",
            "  Batch 1000/26069 | Loss: 0.9876 | LR: 9.46e-05\n",
            "  Batch 1100/26069 | Loss: 0.9868 | LR: 9.46e-05\n",
            "  Batch 1200/26069 | Loss: 0.9857 | LR: 9.46e-05\n",
            "  Batch 1300/26069 | Loss: 0.9869 | LR: 9.46e-05\n",
            "  Batch 1400/26069 | Loss: 0.9864 | LR: 9.46e-05\n",
            "  Batch 1500/26069 | Loss: 0.9865 | LR: 9.46e-05\n",
            "  Batch 1600/26069 | Loss: 0.9854 | LR: 9.46e-05\n",
            "  Batch 1700/26069 | Loss: 0.9856 | LR: 9.46e-05\n",
            "  Batch 1800/26069 | Loss: 0.9866 | LR: 9.46e-05\n",
            "  Batch 1900/26069 | Loss: 0.9864 | LR: 9.46e-05\n",
            "  Batch 2000/26069 | Loss: 0.9860 | LR: 9.46e-05\n",
            "  Batch 2100/26069 | Loss: 0.9852 | LR: 9.45e-05\n",
            "  Batch 2200/26069 | Loss: 0.9847 | LR: 9.45e-05\n",
            "  Batch 2300/26069 | Loss: 0.9856 | LR: 9.45e-05\n",
            "  Batch 2400/26069 | Loss: 0.9854 | LR: 9.45e-05\n",
            "  Batch 2500/26069 | Loss: 0.9864 | LR: 9.45e-05\n",
            "  Batch 2600/26069 | Loss: 0.9865 | LR: 9.45e-05\n",
            "  Batch 2700/26069 | Loss: 0.9865 | LR: 9.45e-05\n",
            "  Batch 2800/26069 | Loss: 0.9859 | LR: 9.45e-05\n",
            "  Batch 2900/26069 | Loss: 0.9859 | LR: 9.45e-05\n",
            "  Batch 3000/26069 | Loss: 0.9851 | LR: 9.45e-05\n",
            "  Batch 3100/26069 | Loss: 0.9856 | LR: 9.45e-05\n",
            "  Batch 3200/26069 | Loss: 0.9855 | LR: 9.45e-05\n",
            "  Batch 3300/26069 | Loss: 0.9860 | LR: 9.45e-05\n",
            "  Batch 3400/26069 | Loss: 0.9865 | LR: 9.45e-05\n",
            "  Batch 3500/26069 | Loss: 0.9868 | LR: 9.45e-05\n",
            "  Batch 3600/26069 | Loss: 0.9872 | LR: 9.45e-05\n",
            "  Batch 3700/26069 | Loss: 0.9877 | LR: 9.45e-05\n",
            "  Batch 3800/26069 | Loss: 0.9882 | LR: 9.45e-05\n",
            "  Batch 3900/26069 | Loss: 0.9880 | LR: 9.45e-05\n",
            "  Batch 4000/26069 | Loss: 0.9881 | LR: 9.45e-05\n",
            "  Batch 4100/26069 | Loss: 0.9881 | LR: 9.45e-05\n",
            "  Batch 4200/26069 | Loss: 0.9877 | LR: 9.45e-05\n",
            "  Batch 4300/26069 | Loss: 0.9872 | LR: 9.45e-05\n",
            "  Batch 4400/26069 | Loss: 0.9873 | LR: 9.45e-05\n",
            "  Batch 4500/26069 | Loss: 0.9871 | LR: 9.45e-05\n",
            "  Batch 4600/26069 | Loss: 0.9870 | LR: 9.45e-05\n",
            "  Batch 4700/26069 | Loss: 0.9865 | LR: 9.45e-05\n",
            "  Batch 4800/26069 | Loss: 0.9867 | LR: 9.45e-05\n",
            "  Batch 4900/26069 | Loss: 0.9863 | LR: 9.45e-05\n",
            "  Batch 5000/26069 | Loss: 0.9862 | LR: 9.45e-05\n",
            "  Batch 5100/26069 | Loss: 0.9861 | LR: 9.45e-05\n",
            "  Batch 5200/26069 | Loss: 0.9859 | LR: 9.45e-05\n",
            "  Batch 5300/26069 | Loss: 0.9858 | LR: 9.45e-05\n",
            "  Batch 5400/26069 | Loss: 0.9858 | LR: 9.45e-05\n",
            "  Batch 5500/26069 | Loss: 0.9857 | LR: 9.45e-05\n",
            "  Batch 5600/26069 | Loss: 0.9859 | LR: 9.45e-05\n",
            "  Batch 5700/26069 | Loss: 0.9861 | LR: 9.44e-05\n",
            "  Batch 5800/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 5900/26069 | Loss: 0.9863 | LR: 9.44e-05\n",
            "  Batch 6000/26069 | Loss: 0.9862 | LR: 9.44e-05\n",
            "  Batch 6100/26069 | Loss: 0.9862 | LR: 9.44e-05\n",
            "  Batch 6200/26069 | Loss: 0.9864 | LR: 9.44e-05\n",
            "  Batch 6300/26069 | Loss: 0.9863 | LR: 9.44e-05\n",
            "  Batch 6400/26069 | Loss: 0.9867 | LR: 9.44e-05\n",
            "  Batch 6500/26069 | Loss: 0.9865 | LR: 9.44e-05\n",
            "  Batch 6600/26069 | Loss: 0.9863 | LR: 9.44e-05\n",
            "  Batch 6700/26069 | Loss: 0.9863 | LR: 9.44e-05\n",
            "  Batch 6800/26069 | Loss: 0.9863 | LR: 9.44e-05\n",
            "  Batch 6900/26069 | Loss: 0.9862 | LR: 9.44e-05\n",
            "  Batch 7000/26069 | Loss: 0.9862 | LR: 9.44e-05\n",
            "  Batch 7100/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 7200/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 7300/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 7400/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 7500/26069 | Loss: 0.9858 | LR: 9.44e-05\n",
            "  Batch 7600/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 7700/26069 | Loss: 0.9858 | LR: 9.44e-05\n",
            "  Batch 7800/26069 | Loss: 0.9858 | LR: 9.44e-05\n",
            "  Batch 7900/26069 | Loss: 0.9861 | LR: 9.44e-05\n",
            "  Batch 8000/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 8100/26069 | Loss: 0.9859 | LR: 9.44e-05\n",
            "  Batch 8200/26069 | Loss: 0.9856 | LR: 9.44e-05\n",
            "  Batch 8300/26069 | Loss: 0.9856 | LR: 9.44e-05\n",
            "  Batch 8400/26069 | Loss: 0.9854 | LR: 9.44e-05\n",
            "  Batch 8500/26069 | Loss: 0.9852 | LR: 9.44e-05\n",
            "  Batch 8600/26069 | Loss: 0.9854 | LR: 9.44e-05\n",
            "  Batch 8700/26069 | Loss: 0.9854 | LR: 9.44e-05\n",
            "  Batch 8800/26069 | Loss: 0.9853 | LR: 9.44e-05\n",
            "  Batch 8900/26069 | Loss: 0.9852 | LR: 9.44e-05\n",
            "  Batch 9000/26069 | Loss: 0.9854 | LR: 9.44e-05\n",
            "  Batch 9100/26069 | Loss: 0.9856 | LR: 9.44e-05\n",
            "  Batch 9200/26069 | Loss: 0.9858 | LR: 9.44e-05\n",
            "  Batch 9300/26069 | Loss: 0.9858 | LR: 9.43e-05\n",
            "  Batch 9400/26069 | Loss: 0.9858 | LR: 9.43e-05\n",
            "  Batch 9500/26069 | Loss: 0.9858 | LR: 9.43e-05\n",
            "  Batch 9600/26069 | Loss: 0.9857 | LR: 9.43e-05\n",
            "  Batch 9700/26069 | Loss: 0.9860 | LR: 9.43e-05\n",
            "  Batch 9800/26069 | Loss: 0.9859 | LR: 9.43e-05\n",
            "  Batch 9900/26069 | Loss: 0.9859 | LR: 9.43e-05\n",
            "  Batch 10000/26069 | Loss: 0.9861 | LR: 9.43e-05\n",
            "  Batch 10100/26069 | Loss: 0.9861 | LR: 9.43e-05\n",
            "  Batch 10200/26069 | Loss: 0.9861 | LR: 9.43e-05\n",
            "  Batch 10300/26069 | Loss: 0.9862 | LR: 9.43e-05\n",
            "  Batch 10400/26069 | Loss: 0.9861 | LR: 9.43e-05\n",
            "  Batch 10500/26069 | Loss: 0.9860 | LR: 9.43e-05\n",
            "  Batch 10600/26069 | Loss: 0.9861 | LR: 9.43e-05\n",
            "  Batch 10700/26069 | Loss: 0.9863 | LR: 9.43e-05\n",
            "  Batch 10800/26069 | Loss: 0.9865 | LR: 9.43e-05\n",
            "  Batch 10900/26069 | Loss: 0.9868 | LR: 9.43e-05\n",
            "  Batch 11000/26069 | Loss: 0.9866 | LR: 9.43e-05\n",
            "  Batch 11100/26069 | Loss: 0.9868 | LR: 9.43e-05\n",
            "  Batch 11200/26069 | Loss: 0.9865 | LR: 9.43e-05\n",
            "  Batch 11300/26069 | Loss: 0.9865 | LR: 9.43e-05\n",
            "  Batch 11400/26069 | Loss: 0.9866 | LR: 9.43e-05\n",
            "  Batch 11500/26069 | Loss: 0.9866 | LR: 9.43e-05\n",
            "  Batch 11600/26069 | Loss: 0.9865 | LR: 9.43e-05\n",
            "  Batch 11700/26069 | Loss: 0.9865 | LR: 9.43e-05\n",
            "  Batch 11800/26069 | Loss: 0.9865 | LR: 9.43e-05\n",
            "  Batch 11900/26069 | Loss: 0.9866 | LR: 9.43e-05\n",
            "  Batch 12000/26069 | Loss: 0.9867 | LR: 9.43e-05\n",
            "  Batch 12100/26069 | Loss: 0.9866 | LR: 9.43e-05\n",
            "  Batch 12200/26069 | Loss: 0.9866 | LR: 9.43e-05\n",
            "  Batch 12300/26069 | Loss: 0.9867 | LR: 9.43e-05\n",
            "  Batch 12400/26069 | Loss: 0.9867 | LR: 9.43e-05\n",
            "  Batch 12500/26069 | Loss: 0.9864 | LR: 9.43e-05\n",
            "  Batch 12600/26069 | Loss: 0.9864 | LR: 9.43e-05\n",
            "  Batch 12700/26069 | Loss: 0.9865 | LR: 9.43e-05\n",
            "  Batch 12800/26069 | Loss: 0.9866 | LR: 9.43e-05\n",
            "  Batch 12900/26069 | Loss: 0.9867 | LR: 9.42e-05\n",
            "  Batch 13000/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 13100/26069 | Loss: 0.9865 | LR: 9.42e-05\n",
            "  Batch 13200/26069 | Loss: 0.9866 | LR: 9.42e-05\n",
            "  Batch 13300/26069 | Loss: 0.9867 | LR: 9.42e-05\n",
            "  Batch 13400/26069 | Loss: 0.9865 | LR: 9.42e-05\n",
            "  Batch 13500/26069 | Loss: 0.9865 | LR: 9.42e-05\n",
            "  Batch 13600/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 13700/26069 | Loss: 0.9865 | LR: 9.42e-05\n",
            "  Batch 13800/26069 | Loss: 0.9865 | LR: 9.42e-05\n",
            "  Batch 13900/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14000/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14100/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14200/26069 | Loss: 0.9863 | LR: 9.42e-05\n",
            "  Batch 14300/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14400/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14500/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14600/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14700/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14800/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 14900/26069 | Loss: 0.9866 | LR: 9.42e-05\n",
            "  Batch 15000/26069 | Loss: 0.9865 | LR: 9.42e-05\n",
            "  Batch 15100/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 15200/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 15300/26069 | Loss: 0.9865 | LR: 9.42e-05\n",
            "  Batch 15400/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 15500/26069 | Loss: 0.9864 | LR: 9.42e-05\n",
            "  Batch 15600/26069 | Loss: 0.9861 | LR: 9.42e-05\n",
            "  Batch 15700/26069 | Loss: 0.9861 | LR: 9.42e-05\n",
            "  Batch 15800/26069 | Loss: 0.9861 | LR: 9.42e-05\n",
            "  Batch 15900/26069 | Loss: 0.9862 | LR: 9.42e-05\n",
            "  Batch 16000/26069 | Loss: 0.9863 | LR: 9.42e-05\n",
            "  Batch 16100/26069 | Loss: 0.9863 | LR: 9.42e-05\n",
            "  Batch 16200/26069 | Loss: 0.9863 | LR: 9.42e-05\n",
            "  Batch 16300/26069 | Loss: 0.9862 | LR: 9.42e-05\n",
            "  Batch 16400/26069 | Loss: 0.9862 | LR: 9.41e-05\n",
            "  Batch 16500/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 16600/26069 | Loss: 0.9862 | LR: 9.41e-05\n",
            "  Batch 16700/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 16800/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 16900/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 17000/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 17100/26069 | Loss: 0.9862 | LR: 9.41e-05\n",
            "  Batch 17200/26069 | Loss: 0.9862 | LR: 9.41e-05\n",
            "  Batch 17300/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 17400/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 17500/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 17600/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 17700/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 17800/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 17900/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 18000/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 18100/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 18200/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 18300/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 18400/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 18500/26069 | Loss: 0.9865 | LR: 9.41e-05\n",
            "  Batch 18600/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 18700/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 18800/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 18900/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 19000/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 19100/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 19200/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 19300/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 19400/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 19500/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 19600/26069 | Loss: 0.9864 | LR: 9.41e-05\n",
            "  Batch 19700/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 19800/26069 | Loss: 0.9863 | LR: 9.41e-05\n",
            "  Batch 19900/26069 | Loss: 0.9862 | LR: 9.41e-05\n",
            "  Batch 20000/26069 | Loss: 0.9863 | LR: 9.40e-05\n",
            "  Batch 20100/26069 | Loss: 0.9862 | LR: 9.40e-05\n",
            "  Batch 20200/26069 | Loss: 0.9860 | LR: 9.40e-05\n",
            "  Batch 20300/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 20400/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 20500/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 20600/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 20700/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 20800/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 20900/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 21000/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 21100/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 21200/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 21300/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 21400/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 21500/26069 | Loss: 0.9859 | LR: 9.40e-05\n",
            "  Batch 21600/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 21700/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 21800/26069 | Loss: 0.9857 | LR: 9.40e-05\n",
            "  Batch 21900/26069 | Loss: 0.9857 | LR: 9.40e-05\n",
            "  Batch 22000/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 22100/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 22200/26069 | Loss: 0.9858 | LR: 9.40e-05\n",
            "  Batch 22300/26069 | Loss: 0.9857 | LR: 9.40e-05\n",
            "  Batch 22400/26069 | Loss: 0.9857 | LR: 9.40e-05\n",
            "  Batch 22500/26069 | Loss: 0.9857 | LR: 9.40e-05\n",
            "  Batch 22600/26069 | Loss: 0.9856 | LR: 9.40e-05\n",
            "  Batch 22700/26069 | Loss: 0.9856 | LR: 9.40e-05\n",
            "  Batch 22800/26069 | Loss: 0.9856 | LR: 9.40e-05\n",
            "  Batch 22900/26069 | Loss: 0.9855 | LR: 9.40e-05\n",
            "  Batch 23000/26069 | Loss: 0.9856 | LR: 9.40e-05\n",
            "  Batch 23100/26069 | Loss: 0.9854 | LR: 9.40e-05\n",
            "  Batch 23200/26069 | Loss: 0.9854 | LR: 9.40e-05\n",
            "  Batch 23300/26069 | Loss: 0.9853 | LR: 9.40e-05\n",
            "  Batch 23400/26069 | Loss: 0.9853 | LR: 9.40e-05\n",
            "  Batch 23500/26069 | Loss: 0.9852 | LR: 9.39e-05\n",
            "  Batch 23600/26069 | Loss: 0.9853 | LR: 9.39e-05\n",
            "  Batch 23700/26069 | Loss: 0.9854 | LR: 9.39e-05\n",
            "  Batch 23800/26069 | Loss: 0.9853 | LR: 9.39e-05\n",
            "  Batch 23900/26069 | Loss: 0.9853 | LR: 9.39e-05\n",
            "  Batch 24000/26069 | Loss: 0.9853 | LR: 9.39e-05\n",
            "  Batch 24100/26069 | Loss: 0.9853 | LR: 9.39e-05\n",
            "  Batch 24200/26069 | Loss: 0.9854 | LR: 9.39e-05\n",
            "  Batch 24300/26069 | Loss: 0.9854 | LR: 9.39e-05\n",
            "  Batch 24400/26069 | Loss: 0.9854 | LR: 9.39e-05\n",
            "  Batch 24500/26069 | Loss: 0.9853 | LR: 9.39e-05\n",
            "  Batch 24600/26069 | Loss: 0.9852 | LR: 9.39e-05\n",
            "  Batch 24700/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 24800/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 24900/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 25000/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 25100/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 25200/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 25300/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 25400/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 25500/26069 | Loss: 0.9851 | LR: 9.39e-05\n",
            "  Batch 25600/26069 | Loss: 0.9850 | LR: 9.39e-05\n",
            "  Batch 25700/26069 | Loss: 0.9850 | LR: 9.39e-05\n",
            "  Batch 25800/26069 | Loss: 0.9849 | LR: 9.39e-05\n",
            "  Batch 25900/26069 | Loss: 0.9850 | LR: 9.39e-05\n",
            "  Batch 26000/26069 | Loss: 0.9850 | LR: 9.39e-05\n",
            "\n",
            "  Train Loss: 0.9850\n",
            "  Val Loss:   0.9646 \n",
            "  Time:       314.5s\n",
            "\n",
            "Epoch 17/100\n",
            "  Batch 100/26069 | Loss: 0.9683 | LR: 9.39e-05\n",
            "  Batch 200/26069 | Loss: 0.9717 | LR: 9.39e-05\n",
            "  Batch 300/26069 | Loss: 0.9782 | LR: 9.39e-05\n",
            "  Batch 400/26069 | Loss: 0.9780 | LR: 9.39e-05\n",
            "  Batch 500/26069 | Loss: 0.9800 | LR: 9.39e-05\n",
            "  Batch 600/26069 | Loss: 0.9815 | LR: 9.39e-05\n",
            "  Batch 700/26069 | Loss: 0.9815 | LR: 9.39e-05\n",
            "  Batch 800/26069 | Loss: 0.9827 | LR: 9.39e-05\n",
            "  Batch 900/26069 | Loss: 0.9841 | LR: 9.38e-05\n",
            "  Batch 1000/26069 | Loss: 0.9820 | LR: 9.38e-05\n",
            "  Batch 1100/26069 | Loss: 0.9812 | LR: 9.38e-05\n",
            "  Batch 1200/26069 | Loss: 0.9813 | LR: 9.38e-05\n",
            "  Batch 1300/26069 | Loss: 0.9810 | LR: 9.38e-05\n",
            "  Batch 1400/26069 | Loss: 0.9807 | LR: 9.38e-05\n",
            "  Batch 1500/26069 | Loss: 0.9811 | LR: 9.38e-05\n",
            "  Batch 1600/26069 | Loss: 0.9819 | LR: 9.38e-05\n",
            "  Batch 1700/26069 | Loss: 0.9816 | LR: 9.38e-05\n",
            "  Batch 1800/26069 | Loss: 0.9815 | LR: 9.38e-05\n",
            "  Batch 1900/26069 | Loss: 0.9821 | LR: 9.38e-05\n",
            "  Batch 2000/26069 | Loss: 0.9830 | LR: 9.38e-05\n",
            "  Batch 2100/26069 | Loss: 0.9833 | LR: 9.38e-05\n",
            "  Batch 2200/26069 | Loss: 0.9835 | LR: 9.38e-05\n",
            "  Batch 2300/26069 | Loss: 0.9835 | LR: 9.38e-05\n",
            "  Batch 2400/26069 | Loss: 0.9828 | LR: 9.38e-05\n",
            "  Batch 2500/26069 | Loss: 0.9837 | LR: 9.38e-05\n",
            "  Batch 2600/26069 | Loss: 0.9837 | LR: 9.38e-05\n",
            "  Batch 2700/26069 | Loss: 0.9836 | LR: 9.38e-05\n",
            "  Batch 2800/26069 | Loss: 0.9834 | LR: 9.38e-05\n",
            "  Batch 2900/26069 | Loss: 0.9836 | LR: 9.38e-05\n",
            "  Batch 3000/26069 | Loss: 0.9839 | LR: 9.38e-05\n",
            "  Batch 3100/26069 | Loss: 0.9843 | LR: 9.38e-05\n",
            "  Batch 3200/26069 | Loss: 0.9845 | LR: 9.38e-05\n",
            "  Batch 3300/26069 | Loss: 0.9841 | LR: 9.38e-05\n",
            "  Batch 3400/26069 | Loss: 0.9844 | LR: 9.38e-05\n",
            "  Batch 3500/26069 | Loss: 0.9845 | LR: 9.38e-05\n",
            "  Batch 3600/26069 | Loss: 0.9849 | LR: 9.38e-05\n",
            "  Batch 3700/26069 | Loss: 0.9847 | LR: 9.38e-05\n",
            "  Batch 3800/26069 | Loss: 0.9849 | LR: 9.38e-05\n",
            "  Batch 3900/26069 | Loss: 0.9849 | LR: 9.38e-05\n",
            "  Batch 4000/26069 | Loss: 0.9848 | LR: 9.38e-05\n",
            "  Batch 4100/26069 | Loss: 0.9851 | LR: 9.38e-05\n",
            "  Batch 4200/26069 | Loss: 0.9852 | LR: 9.38e-05\n",
            "  Batch 4300/26069 | Loss: 0.9856 | LR: 9.37e-05\n",
            "  Batch 4400/26069 | Loss: 0.9851 | LR: 9.37e-05\n",
            "  Batch 4500/26069 | Loss: 0.9850 | LR: 9.37e-05\n",
            "  Batch 4600/26069 | Loss: 0.9846 | LR: 9.37e-05\n",
            "  Batch 4700/26069 | Loss: 0.9846 | LR: 9.37e-05\n",
            "  Batch 4800/26069 | Loss: 0.9846 | LR: 9.37e-05\n",
            "  Batch 4900/26069 | Loss: 0.9847 | LR: 9.37e-05\n",
            "  Batch 5000/26069 | Loss: 0.9847 | LR: 9.37e-05\n",
            "  Batch 5100/26069 | Loss: 0.9846 | LR: 9.37e-05\n",
            "  Batch 5200/26069 | Loss: 0.9841 | LR: 9.37e-05\n",
            "  Batch 5300/26069 | Loss: 0.9843 | LR: 9.37e-05\n",
            "  Batch 5400/26069 | Loss: 0.9846 | LR: 9.37e-05\n",
            "  Batch 5500/26069 | Loss: 0.9850 | LR: 9.37e-05\n",
            "  Batch 5600/26069 | Loss: 0.9850 | LR: 9.37e-05\n",
            "  Batch 5700/26069 | Loss: 0.9849 | LR: 9.37e-05\n",
            "  Batch 5800/26069 | Loss: 0.9848 | LR: 9.37e-05\n",
            "  Batch 5900/26069 | Loss: 0.9846 | LR: 9.37e-05\n",
            "  Batch 6000/26069 | Loss: 0.9845 | LR: 9.37e-05\n",
            "  Batch 6100/26069 | Loss: 0.9842 | LR: 9.37e-05\n",
            "  Batch 6200/26069 | Loss: 0.9838 | LR: 9.37e-05\n",
            "  Batch 6300/26069 | Loss: 0.9841 | LR: 9.37e-05\n",
            "  Batch 6400/26069 | Loss: 0.9842 | LR: 9.37e-05\n",
            "  Batch 6500/26069 | Loss: 0.9842 | LR: 9.37e-05\n",
            "  Batch 6600/26069 | Loss: 0.9843 | LR: 9.37e-05\n",
            "  Batch 6700/26069 | Loss: 0.9844 | LR: 9.37e-05\n",
            "  Batch 6800/26069 | Loss: 0.9847 | LR: 9.37e-05\n",
            "  Batch 6900/26069 | Loss: 0.9847 | LR: 9.37e-05\n",
            "  Batch 7000/26069 | Loss: 0.9849 | LR: 9.37e-05\n",
            "  Batch 7100/26069 | Loss: 0.9848 | LR: 9.37e-05\n",
            "  Batch 7200/26069 | Loss: 0.9848 | LR: 9.37e-05\n",
            "  Batch 7300/26069 | Loss: 0.9843 | LR: 9.37e-05\n",
            "  Batch 7400/26069 | Loss: 0.9845 | LR: 9.37e-05\n",
            "  Batch 7500/26069 | Loss: 0.9845 | LR: 9.37e-05\n",
            "  Batch 7600/26069 | Loss: 0.9846 | LR: 9.37e-05\n",
            "  Batch 7700/26069 | Loss: 0.9846 | LR: 9.36e-05\n",
            "  Batch 7800/26069 | Loss: 0.9848 | LR: 9.36e-05\n",
            "  Batch 7900/26069 | Loss: 0.9847 | LR: 9.36e-05\n",
            "  Batch 8000/26069 | Loss: 0.9845 | LR: 9.36e-05\n",
            "  Batch 8100/26069 | Loss: 0.9846 | LR: 9.36e-05\n",
            "  Batch 8200/26069 | Loss: 0.9846 | LR: 9.36e-05\n",
            "  Batch 8300/26069 | Loss: 0.9848 | LR: 9.36e-05\n",
            "  Batch 8400/26069 | Loss: 0.9844 | LR: 9.36e-05\n",
            "  Batch 8500/26069 | Loss: 0.9842 | LR: 9.36e-05\n",
            "  Batch 8600/26069 | Loss: 0.9842 | LR: 9.36e-05\n",
            "  Batch 8700/26069 | Loss: 0.9843 | LR: 9.36e-05\n",
            "  Batch 8800/26069 | Loss: 0.9842 | LR: 9.36e-05\n",
            "  Batch 8900/26069 | Loss: 0.9843 | LR: 9.36e-05\n",
            "  Batch 9000/26069 | Loss: 0.9846 | LR: 9.36e-05\n",
            "  Batch 9100/26069 | Loss: 0.9847 | LR: 9.36e-05\n",
            "  Batch 9200/26069 | Loss: 0.9848 | LR: 9.36e-05\n",
            "  Batch 9300/26069 | Loss: 0.9847 | LR: 9.36e-05\n",
            "  Batch 9400/26069 | Loss: 0.9847 | LR: 9.36e-05\n",
            "  Batch 9500/26069 | Loss: 0.9845 | LR: 9.36e-05\n",
            "  Batch 9600/26069 | Loss: 0.9844 | LR: 9.36e-05\n",
            "  Batch 9700/26069 | Loss: 0.9842 | LR: 9.36e-05\n",
            "  Batch 9800/26069 | Loss: 0.9839 | LR: 9.36e-05\n",
            "  Batch 9900/26069 | Loss: 0.9839 | LR: 9.36e-05\n",
            "  Batch 10000/26069 | Loss: 0.9840 | LR: 9.36e-05\n",
            "  Batch 10100/26069 | Loss: 0.9839 | LR: 9.36e-05\n",
            "  Batch 10200/26069 | Loss: 0.9837 | LR: 9.36e-05\n",
            "  Batch 10300/26069 | Loss: 0.9836 | LR: 9.36e-05\n",
            "  Batch 10400/26069 | Loss: 0.9835 | LR: 9.36e-05\n",
            "  Batch 10500/26069 | Loss: 0.9835 | LR: 9.36e-05\n",
            "  Batch 10600/26069 | Loss: 0.9833 | LR: 9.36e-05\n",
            "  Batch 10700/26069 | Loss: 0.9833 | LR: 9.36e-05\n",
            "  Batch 10800/26069 | Loss: 0.9834 | LR: 9.36e-05\n",
            "  Batch 10900/26069 | Loss: 0.9832 | LR: 9.36e-05\n",
            "  Batch 11000/26069 | Loss: 0.9834 | LR: 9.36e-05\n",
            "  Batch 11100/26069 | Loss: 0.9833 | LR: 9.35e-05\n",
            "  Batch 11200/26069 | Loss: 0.9833 | LR: 9.35e-05\n",
            "  Batch 11300/26069 | Loss: 0.9833 | LR: 9.35e-05\n",
            "  Batch 11400/26069 | Loss: 0.9833 | LR: 9.35e-05\n",
            "  Batch 11500/26069 | Loss: 0.9834 | LR: 9.35e-05\n",
            "  Batch 11600/26069 | Loss: 0.9835 | LR: 9.35e-05\n",
            "  Batch 11700/26069 | Loss: 0.9837 | LR: 9.35e-05\n",
            "  Batch 11800/26069 | Loss: 0.9837 | LR: 9.35e-05\n",
            "  Batch 11900/26069 | Loss: 0.9836 | LR: 9.35e-05\n",
            "  Batch 12000/26069 | Loss: 0.9836 | LR: 9.35e-05\n",
            "  Batch 12100/26069 | Loss: 0.9837 | LR: 9.35e-05\n",
            "  Batch 12200/26069 | Loss: 0.9836 | LR: 9.35e-05\n",
            "  Batch 12300/26069 | Loss: 0.9836 | LR: 9.35e-05\n",
            "  Batch 12400/26069 | Loss: 0.9835 | LR: 9.35e-05\n",
            "  Batch 12500/26069 | Loss: 0.9833 | LR: 9.35e-05\n",
            "  Batch 12600/26069 | Loss: 0.9833 | LR: 9.35e-05\n",
            "  Batch 12700/26069 | Loss: 0.9832 | LR: 9.35e-05\n",
            "  Batch 12800/26069 | Loss: 0.9831 | LR: 9.35e-05\n",
            "  Batch 12900/26069 | Loss: 0.9831 | LR: 9.35e-05\n",
            "  Batch 13000/26069 | Loss: 0.9832 | LR: 9.35e-05\n",
            "  Batch 13100/26069 | Loss: 0.9831 | LR: 9.35e-05\n",
            "  Batch 13200/26069 | Loss: 0.9830 | LR: 9.35e-05\n",
            "  Batch 13300/26069 | Loss: 0.9829 | LR: 9.35e-05\n",
            "  Batch 13400/26069 | Loss: 0.9829 | LR: 9.35e-05\n",
            "  Batch 13500/26069 | Loss: 0.9830 | LR: 9.35e-05\n",
            "  Batch 13600/26069 | Loss: 0.9830 | LR: 9.35e-05\n",
            "  Batch 13700/26069 | Loss: 0.9829 | LR: 9.35e-05\n",
            "  Batch 13800/26069 | Loss: 0.9828 | LR: 9.35e-05\n",
            "  Batch 13900/26069 | Loss: 0.9828 | LR: 9.35e-05\n",
            "  Batch 14000/26069 | Loss: 0.9828 | LR: 9.35e-05\n",
            "  Batch 14100/26069 | Loss: 0.9827 | LR: 9.35e-05\n",
            "  Batch 14200/26069 | Loss: 0.9828 | LR: 9.35e-05\n",
            "  Batch 14300/26069 | Loss: 0.9829 | LR: 9.35e-05\n",
            "  Batch 14400/26069 | Loss: 0.9830 | LR: 9.35e-05\n",
            "  Batch 14500/26069 | Loss: 0.9830 | LR: 9.34e-05\n",
            "  Batch 14600/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 14700/26069 | Loss: 0.9828 | LR: 9.34e-05\n",
            "  Batch 14800/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 14900/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 15000/26069 | Loss: 0.9832 | LR: 9.34e-05\n",
            "  Batch 15100/26069 | Loss: 0.9831 | LR: 9.34e-05\n",
            "  Batch 15200/26069 | Loss: 0.9831 | LR: 9.34e-05\n",
            "  Batch 15300/26069 | Loss: 0.9830 | LR: 9.34e-05\n",
            "  Batch 15400/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 15500/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 15600/26069 | Loss: 0.9828 | LR: 9.34e-05\n",
            "  Batch 15700/26069 | Loss: 0.9828 | LR: 9.34e-05\n",
            "  Batch 15800/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 15900/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 16000/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 16100/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 16200/26069 | Loss: 0.9830 | LR: 9.34e-05\n",
            "  Batch 16300/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 16400/26069 | Loss: 0.9830 | LR: 9.34e-05\n",
            "  Batch 16500/26069 | Loss: 0.9830 | LR: 9.34e-05\n",
            "  Batch 16600/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 16700/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 16800/26069 | Loss: 0.9829 | LR: 9.34e-05\n",
            "  Batch 16900/26069 | Loss: 0.9830 | LR: 9.34e-05\n",
            "  Batch 17000/26069 | Loss: 0.9830 | LR: 9.34e-05\n",
            "  Batch 17100/26069 | Loss: 0.9831 | LR: 9.34e-05\n",
            "  Batch 17200/26069 | Loss: 0.9832 | LR: 9.34e-05\n",
            "  Batch 17300/26069 | Loss: 0.9832 | LR: 9.34e-05\n",
            "  Batch 17400/26069 | Loss: 0.9832 | LR: 9.34e-05\n",
            "  Batch 17500/26069 | Loss: 0.9831 | LR: 9.34e-05\n",
            "  Batch 17600/26069 | Loss: 0.9832 | LR: 9.34e-05\n",
            "  Batch 17700/26069 | Loss: 0.9832 | LR: 9.34e-05\n",
            "  Batch 17800/26069 | Loss: 0.9834 | LR: 9.33e-05\n",
            "  Batch 17900/26069 | Loss: 0.9833 | LR: 9.33e-05\n",
            "  Batch 18000/26069 | Loss: 0.9833 | LR: 9.33e-05\n",
            "  Batch 18100/26069 | Loss: 0.9833 | LR: 9.33e-05\n",
            "  Batch 18200/26069 | Loss: 0.9833 | LR: 9.33e-05\n",
            "  Batch 18300/26069 | Loss: 0.9834 | LR: 9.33e-05\n",
            "  Batch 18400/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 18500/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 18600/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 18700/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 18800/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 18900/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 19000/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 19100/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 19200/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 19300/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 19400/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 19500/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 19600/26069 | Loss: 0.9835 | LR: 9.33e-05\n",
            "  Batch 19700/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 19800/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 19900/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 20000/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 20100/26069 | Loss: 0.9837 | LR: 9.33e-05\n",
            "  Batch 20200/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 20300/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 20400/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 20500/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 20600/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 20700/26069 | Loss: 0.9837 | LR: 9.33e-05\n",
            "  Batch 20800/26069 | Loss: 0.9837 | LR: 9.33e-05\n",
            "  Batch 20900/26069 | Loss: 0.9837 | LR: 9.33e-05\n",
            "  Batch 21000/26069 | Loss: 0.9836 | LR: 9.33e-05\n",
            "  Batch 21100/26069 | Loss: 0.9837 | LR: 9.32e-05\n",
            "  Batch 21200/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 21300/26069 | Loss: 0.9837 | LR: 9.32e-05\n",
            "  Batch 21400/26069 | Loss: 0.9837 | LR: 9.32e-05\n",
            "  Batch 21500/26069 | Loss: 0.9838 | LR: 9.32e-05\n",
            "  Batch 21600/26069 | Loss: 0.9838 | LR: 9.32e-05\n",
            "  Batch 21700/26069 | Loss: 0.9838 | LR: 9.32e-05\n",
            "  Batch 21800/26069 | Loss: 0.9838 | LR: 9.32e-05\n",
            "  Batch 21900/26069 | Loss: 0.9837 | LR: 9.32e-05\n",
            "  Batch 22000/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 22100/26069 | Loss: 0.9837 | LR: 9.32e-05\n",
            "  Batch 22200/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 22300/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 22400/26069 | Loss: 0.9835 | LR: 9.32e-05\n",
            "  Batch 22500/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 22600/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 22700/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 22800/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 22900/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 23000/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 23100/26069 | Loss: 0.9835 | LR: 9.32e-05\n",
            "  Batch 23200/26069 | Loss: 0.9834 | LR: 9.32e-05\n",
            "  Batch 23300/26069 | Loss: 0.9834 | LR: 9.32e-05\n",
            "  Batch 23400/26069 | Loss: 0.9833 | LR: 9.32e-05\n",
            "  Batch 23500/26069 | Loss: 0.9833 | LR: 9.32e-05\n",
            "  Batch 23600/26069 | Loss: 0.9834 | LR: 9.32e-05\n",
            "  Batch 23700/26069 | Loss: 0.9834 | LR: 9.32e-05\n",
            "  Batch 23800/26069 | Loss: 0.9835 | LR: 9.32e-05\n",
            "  Batch 23900/26069 | Loss: 0.9835 | LR: 9.32e-05\n",
            "  Batch 24000/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 24100/26069 | Loss: 0.9835 | LR: 9.32e-05\n",
            "  Batch 24200/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 24300/26069 | Loss: 0.9836 | LR: 9.32e-05\n",
            "  Batch 24400/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 24500/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 24600/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 24700/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 24800/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 24900/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 25000/26069 | Loss: 0.9836 | LR: 9.31e-05\n",
            "  Batch 25100/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 25200/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 25300/26069 | Loss: 0.9836 | LR: 9.31e-05\n",
            "  Batch 25400/26069 | Loss: 0.9836 | LR: 9.31e-05\n",
            "  Batch 25500/26069 | Loss: 0.9836 | LR: 9.31e-05\n",
            "  Batch 25600/26069 | Loss: 0.9837 | LR: 9.31e-05\n",
            "  Batch 25700/26069 | Loss: 0.9836 | LR: 9.31e-05\n",
            "  Batch 25800/26069 | Loss: 0.9836 | LR: 9.31e-05\n",
            "  Batch 25900/26069 | Loss: 0.9836 | LR: 9.31e-05\n",
            "  Batch 26000/26069 | Loss: 0.9835 | LR: 9.31e-05\n",
            "\n",
            "  Train Loss: 0.9835\n",
            "  Val Loss:   0.9605 [BEST]\n",
            "  Time:       315.0s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_17.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 18/100\n",
            "  Batch 100/26069 | Loss: 0.9890 | LR: 9.31e-05\n",
            "  Batch 200/26069 | Loss: 0.9915 | LR: 9.31e-05\n",
            "  Batch 300/26069 | Loss: 0.9893 | LR: 9.31e-05\n",
            "  Batch 400/26069 | Loss: 0.9877 | LR: 9.31e-05\n",
            "  Batch 500/26069 | Loss: 0.9850 | LR: 9.31e-05\n",
            "  Batch 600/26069 | Loss: 0.9869 | LR: 9.31e-05\n",
            "  Batch 700/26069 | Loss: 0.9888 | LR: 9.31e-05\n",
            "  Batch 800/26069 | Loss: 0.9881 | LR: 9.31e-05\n",
            "  Batch 900/26069 | Loss: 0.9883 | LR: 9.31e-05\n",
            "  Batch 1000/26069 | Loss: 0.9862 | LR: 9.31e-05\n",
            "  Batch 1100/26069 | Loss: 0.9888 | LR: 9.31e-05\n",
            "  Batch 1200/26069 | Loss: 0.9887 | LR: 9.31e-05\n",
            "  Batch 1300/26069 | Loss: 0.9891 | LR: 9.31e-05\n",
            "  Batch 1400/26069 | Loss: 0.9877 | LR: 9.31e-05\n",
            "  Batch 1500/26069 | Loss: 0.9871 | LR: 9.31e-05\n",
            "  Batch 1600/26069 | Loss: 0.9867 | LR: 9.30e-05\n",
            "  Batch 1700/26069 | Loss: 0.9864 | LR: 9.30e-05\n",
            "  Batch 1800/26069 | Loss: 0.9849 | LR: 9.30e-05\n",
            "  Batch 1900/26069 | Loss: 0.9852 | LR: 9.30e-05\n",
            "  Batch 2000/26069 | Loss: 0.9855 | LR: 9.30e-05\n",
            "  Batch 2100/26069 | Loss: 0.9859 | LR: 9.30e-05\n",
            "  Batch 2200/26069 | Loss: 0.9853 | LR: 9.30e-05\n",
            "  Batch 2300/26069 | Loss: 0.9849 | LR: 9.30e-05\n",
            "  Batch 2400/26069 | Loss: 0.9846 | LR: 9.30e-05\n",
            "  Batch 2500/26069 | Loss: 0.9853 | LR: 9.30e-05\n",
            "  Batch 2600/26069 | Loss: 0.9852 | LR: 9.30e-05\n",
            "  Batch 2700/26069 | Loss: 0.9849 | LR: 9.30e-05\n",
            "  Batch 2800/26069 | Loss: 0.9854 | LR: 9.30e-05\n",
            "  Batch 2900/26069 | Loss: 0.9857 | LR: 9.30e-05\n",
            "  Batch 3000/26069 | Loss: 0.9850 | LR: 9.30e-05\n",
            "  Batch 3100/26069 | Loss: 0.9847 | LR: 9.30e-05\n",
            "  Batch 3200/26069 | Loss: 0.9842 | LR: 9.30e-05\n",
            "  Batch 3300/26069 | Loss: 0.9840 | LR: 9.30e-05\n",
            "  Batch 3400/26069 | Loss: 0.9838 | LR: 9.30e-05\n",
            "  Batch 3500/26069 | Loss: 0.9836 | LR: 9.30e-05\n",
            "  Batch 3600/26069 | Loss: 0.9835 | LR: 9.30e-05\n",
            "  Batch 3700/26069 | Loss: 0.9831 | LR: 9.30e-05\n",
            "  Batch 3800/26069 | Loss: 0.9828 | LR: 9.30e-05\n",
            "  Batch 3900/26069 | Loss: 0.9821 | LR: 9.30e-05\n",
            "  Batch 4000/26069 | Loss: 0.9817 | LR: 9.30e-05\n",
            "  Batch 4100/26069 | Loss: 0.9818 | LR: 9.30e-05\n",
            "  Batch 4200/26069 | Loss: 0.9812 | LR: 9.30e-05\n",
            "  Batch 4300/26069 | Loss: 0.9814 | LR: 9.30e-05\n",
            "  Batch 4400/26069 | Loss: 0.9814 | LR: 9.30e-05\n",
            "  Batch 4500/26069 | Loss: 0.9813 | LR: 9.30e-05\n",
            "  Batch 4600/26069 | Loss: 0.9812 | LR: 9.30e-05\n",
            "  Batch 4700/26069 | Loss: 0.9809 | LR: 9.30e-05\n",
            "  Batch 4800/26069 | Loss: 0.9808 | LR: 9.30e-05\n",
            "  Batch 4900/26069 | Loss: 0.9808 | LR: 9.29e-05\n",
            "  Batch 5000/26069 | Loss: 0.9809 | LR: 9.29e-05\n",
            "  Batch 5100/26069 | Loss: 0.9813 | LR: 9.29e-05\n",
            "  Batch 5200/26069 | Loss: 0.9813 | LR: 9.29e-05\n",
            "  Batch 5300/26069 | Loss: 0.9817 | LR: 9.29e-05\n",
            "  Batch 5400/26069 | Loss: 0.9815 | LR: 9.29e-05\n",
            "  Batch 5500/26069 | Loss: 0.9814 | LR: 9.29e-05\n",
            "  Batch 5600/26069 | Loss: 0.9818 | LR: 9.29e-05\n",
            "  Batch 5700/26069 | Loss: 0.9819 | LR: 9.29e-05\n",
            "  Batch 5800/26069 | Loss: 0.9820 | LR: 9.29e-05\n",
            "  Batch 5900/26069 | Loss: 0.9821 | LR: 9.29e-05\n",
            "  Batch 6000/26069 | Loss: 0.9820 | LR: 9.29e-05\n",
            "  Batch 6100/26069 | Loss: 0.9821 | LR: 9.29e-05\n",
            "  Batch 6200/26069 | Loss: 0.9820 | LR: 9.29e-05\n",
            "  Batch 6300/26069 | Loss: 0.9821 | LR: 9.29e-05\n",
            "  Batch 6400/26069 | Loss: 0.9823 | LR: 9.29e-05\n",
            "  Batch 6500/26069 | Loss: 0.9820 | LR: 9.29e-05\n",
            "  Batch 6600/26069 | Loss: 0.9820 | LR: 9.29e-05\n",
            "  Batch 6700/26069 | Loss: 0.9820 | LR: 9.29e-05\n",
            "  Batch 6800/26069 | Loss: 0.9824 | LR: 9.29e-05\n",
            "  Batch 6900/26069 | Loss: 0.9825 | LR: 9.29e-05\n",
            "  Batch 7000/26069 | Loss: 0.9827 | LR: 9.29e-05\n",
            "  Batch 7100/26069 | Loss: 0.9828 | LR: 9.29e-05\n",
            "  Batch 7200/26069 | Loss: 0.9828 | LR: 9.29e-05\n",
            "  Batch 7300/26069 | Loss: 0.9829 | LR: 9.29e-05\n",
            "  Batch 7400/26069 | Loss: 0.9830 | LR: 9.29e-05\n",
            "  Batch 7500/26069 | Loss: 0.9830 | LR: 9.29e-05\n",
            "  Batch 7600/26069 | Loss: 0.9829 | LR: 9.29e-05\n",
            "  Batch 7700/26069 | Loss: 0.9827 | LR: 9.29e-05\n",
            "  Batch 7800/26069 | Loss: 0.9827 | LR: 9.29e-05\n",
            "  Batch 7900/26069 | Loss: 0.9827 | LR: 9.29e-05\n",
            "  Batch 8000/26069 | Loss: 0.9830 | LR: 9.29e-05\n",
            "  Batch 8100/26069 | Loss: 0.9832 | LR: 9.28e-05\n",
            "  Batch 8200/26069 | Loss: 0.9833 | LR: 9.28e-05\n",
            "  Batch 8300/26069 | Loss: 0.9833 | LR: 9.28e-05\n",
            "  Batch 8400/26069 | Loss: 0.9833 | LR: 9.28e-05\n",
            "  Batch 8500/26069 | Loss: 0.9834 | LR: 9.28e-05\n",
            "  Batch 8600/26069 | Loss: 0.9833 | LR: 9.28e-05\n",
            "  Batch 8700/26069 | Loss: 0.9836 | LR: 9.28e-05\n",
            "  Batch 8800/26069 | Loss: 0.9836 | LR: 9.28e-05\n",
            "  Batch 8900/26069 | Loss: 0.9836 | LR: 9.28e-05\n",
            "  Batch 9000/26069 | Loss: 0.9834 | LR: 9.28e-05\n",
            "  Batch 9100/26069 | Loss: 0.9833 | LR: 9.28e-05\n",
            "  Batch 9200/26069 | Loss: 0.9834 | LR: 9.28e-05\n",
            "  Batch 9300/26069 | Loss: 0.9833 | LR: 9.28e-05\n",
            "  Batch 9400/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 9500/26069 | Loss: 0.9832 | LR: 9.28e-05\n",
            "  Batch 9600/26069 | Loss: 0.9832 | LR: 9.28e-05\n",
            "  Batch 9700/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 9800/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 9900/26069 | Loss: 0.9830 | LR: 9.28e-05\n",
            "  Batch 10000/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 10100/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 10200/26069 | Loss: 0.9832 | LR: 9.28e-05\n",
            "  Batch 10300/26069 | Loss: 0.9832 | LR: 9.28e-05\n",
            "  Batch 10400/26069 | Loss: 0.9832 | LR: 9.28e-05\n",
            "  Batch 10500/26069 | Loss: 0.9832 | LR: 9.28e-05\n",
            "  Batch 10600/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 10700/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 10800/26069 | Loss: 0.9830 | LR: 9.28e-05\n",
            "  Batch 10900/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 11000/26069 | Loss: 0.9831 | LR: 9.28e-05\n",
            "  Batch 11100/26069 | Loss: 0.9829 | LR: 9.28e-05\n",
            "  Batch 11200/26069 | Loss: 0.9828 | LR: 9.28e-05\n",
            "  Batch 11300/26069 | Loss: 0.9829 | LR: 9.27e-05\n",
            "  Batch 11400/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 11500/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 11600/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 11700/26069 | Loss: 0.9829 | LR: 9.27e-05\n",
            "  Batch 11800/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 11900/26069 | Loss: 0.9829 | LR: 9.27e-05\n",
            "  Batch 12000/26069 | Loss: 0.9827 | LR: 9.27e-05\n",
            "  Batch 12100/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 12200/26069 | Loss: 0.9827 | LR: 9.27e-05\n",
            "  Batch 12300/26069 | Loss: 0.9826 | LR: 9.27e-05\n",
            "  Batch 12400/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 12500/26069 | Loss: 0.9829 | LR: 9.27e-05\n",
            "  Batch 12600/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 12700/26069 | Loss: 0.9829 | LR: 9.27e-05\n",
            "  Batch 12800/26069 | Loss: 0.9829 | LR: 9.27e-05\n",
            "  Batch 12900/26069 | Loss: 0.9830 | LR: 9.27e-05\n",
            "  Batch 13000/26069 | Loss: 0.9830 | LR: 9.27e-05\n",
            "  Batch 13100/26069 | Loss: 0.9830 | LR: 9.27e-05\n",
            "  Batch 13200/26069 | Loss: 0.9831 | LR: 9.27e-05\n",
            "  Batch 13300/26069 | Loss: 0.9832 | LR: 9.27e-05\n",
            "  Batch 13400/26069 | Loss: 0.9831 | LR: 9.27e-05\n",
            "  Batch 13500/26069 | Loss: 0.9831 | LR: 9.27e-05\n",
            "  Batch 13600/26069 | Loss: 0.9830 | LR: 9.27e-05\n",
            "  Batch 13700/26069 | Loss: 0.9830 | LR: 9.27e-05\n",
            "  Batch 13800/26069 | Loss: 0.9829 | LR: 9.27e-05\n",
            "  Batch 13900/26069 | Loss: 0.9828 | LR: 9.27e-05\n",
            "  Batch 14000/26069 | Loss: 0.9827 | LR: 9.27e-05\n",
            "  Batch 14100/26069 | Loss: 0.9827 | LR: 9.27e-05\n",
            "  Batch 14200/26069 | Loss: 0.9826 | LR: 9.27e-05\n",
            "  Batch 14300/26069 | Loss: 0.9825 | LR: 9.27e-05\n",
            "  Batch 14400/26069 | Loss: 0.9825 | LR: 9.27e-05\n",
            "  Batch 14500/26069 | Loss: 0.9824 | LR: 9.26e-05\n",
            "  Batch 14600/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 14700/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 14800/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 14900/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 15000/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 15100/26069 | Loss: 0.9827 | LR: 9.26e-05\n",
            "  Batch 15200/26069 | Loss: 0.9827 | LR: 9.26e-05\n",
            "  Batch 15300/26069 | Loss: 0.9827 | LR: 9.26e-05\n",
            "  Batch 15400/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 15500/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 15600/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 15700/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 15800/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 15900/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 16000/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 16100/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 16200/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 16300/26069 | Loss: 0.9824 | LR: 9.26e-05\n",
            "  Batch 16400/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 16500/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 16600/26069 | Loss: 0.9827 | LR: 9.26e-05\n",
            "  Batch 16700/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 16800/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 16900/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 17000/26069 | Loss: 0.9826 | LR: 9.26e-05\n",
            "  Batch 17100/26069 | Loss: 0.9825 | LR: 9.26e-05\n",
            "  Batch 17200/26069 | Loss: 0.9824 | LR: 9.26e-05\n",
            "  Batch 17300/26069 | Loss: 0.9824 | LR: 9.26e-05\n",
            "  Batch 17400/26069 | Loss: 0.9824 | LR: 9.26e-05\n",
            "  Batch 17500/26069 | Loss: 0.9824 | LR: 9.26e-05\n",
            "  Batch 17600/26069 | Loss: 0.9822 | LR: 9.26e-05\n",
            "  Batch 17700/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 17800/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 17900/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18000/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18100/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18200/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 18300/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 18400/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18500/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18600/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18700/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18800/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 18900/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 19000/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 19100/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 19200/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 19300/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 19400/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 19500/26069 | Loss: 0.9820 | LR: 9.25e-05\n",
            "  Batch 19600/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 19700/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 19800/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 19900/26069 | Loss: 0.9821 | LR: 9.25e-05\n",
            "  Batch 20000/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 20100/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 20200/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 20300/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 20400/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 20500/26069 | Loss: 0.9823 | LR: 9.25e-05\n",
            "  Batch 20600/26069 | Loss: 0.9822 | LR: 9.25e-05\n",
            "  Batch 20700/26069 | Loss: 0.9823 | LR: 9.25e-05\n",
            "  Batch 20800/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 20900/26069 | Loss: 0.9824 | LR: 9.24e-05\n",
            "  Batch 21000/26069 | Loss: 0.9824 | LR: 9.24e-05\n",
            "  Batch 21100/26069 | Loss: 0.9824 | LR: 9.24e-05\n",
            "  Batch 21200/26069 | Loss: 0.9825 | LR: 9.24e-05\n",
            "  Batch 21300/26069 | Loss: 0.9824 | LR: 9.24e-05\n",
            "  Batch 21400/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 21500/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 21600/26069 | Loss: 0.9824 | LR: 9.24e-05\n",
            "  Batch 21700/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 21800/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 21900/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 22000/26069 | Loss: 0.9824 | LR: 9.24e-05\n",
            "  Batch 22100/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 22200/26069 | Loss: 0.9823 | LR: 9.24e-05\n",
            "  Batch 22300/26069 | Loss: 0.9822 | LR: 9.24e-05\n",
            "  Batch 22400/26069 | Loss: 0.9822 | LR: 9.24e-05\n",
            "  Batch 22500/26069 | Loss: 0.9822 | LR: 9.24e-05\n",
            "  Batch 22600/26069 | Loss: 0.9822 | LR: 9.24e-05\n",
            "  Batch 22700/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 22800/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 22900/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 23000/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 23100/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 23200/26069 | Loss: 0.9820 | LR: 9.24e-05\n",
            "  Batch 23300/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 23400/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 23500/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 23600/26069 | Loss: 0.9820 | LR: 9.24e-05\n",
            "  Batch 23700/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 23800/26069 | Loss: 0.9822 | LR: 9.24e-05\n",
            "  Batch 23900/26069 | Loss: 0.9821 | LR: 9.24e-05\n",
            "  Batch 24000/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 24100/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 24200/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 24300/26069 | Loss: 0.9819 | LR: 9.23e-05\n",
            "  Batch 24400/26069 | Loss: 0.9819 | LR: 9.23e-05\n",
            "  Batch 24500/26069 | Loss: 0.9819 | LR: 9.23e-05\n",
            "  Batch 24600/26069 | Loss: 0.9819 | LR: 9.23e-05\n",
            "  Batch 24700/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 24800/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 24900/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 25000/26069 | Loss: 0.9819 | LR: 9.23e-05\n",
            "  Batch 25100/26069 | Loss: 0.9819 | LR: 9.23e-05\n",
            "  Batch 25200/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 25300/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 25400/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 25500/26069 | Loss: 0.9821 | LR: 9.23e-05\n",
            "  Batch 25600/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 25700/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "  Batch 25800/26069 | Loss: 0.9821 | LR: 9.23e-05\n",
            "  Batch 25900/26069 | Loss: 0.9821 | LR: 9.23e-05\n",
            "  Batch 26000/26069 | Loss: 0.9820 | LR: 9.23e-05\n",
            "\n",
            "  Train Loss: 0.9820\n",
            "  Val Loss:   0.9627 \n",
            "  Time:       314.0s\n",
            "\n",
            "Epoch 19/100\n",
            "  Batch 100/26069 | Loss: 0.9765 | LR: 9.23e-05\n",
            "  Batch 200/26069 | Loss: 0.9763 | LR: 9.23e-05\n",
            "  Batch 300/26069 | Loss: 0.9729 | LR: 9.23e-05\n",
            "  Batch 400/26069 | Loss: 0.9737 | LR: 9.23e-05\n",
            "  Batch 500/26069 | Loss: 0.9705 | LR: 9.23e-05\n",
            "  Batch 600/26069 | Loss: 0.9712 | LR: 9.23e-05\n",
            "  Batch 700/26069 | Loss: 0.9724 | LR: 9.23e-05\n",
            "  Batch 800/26069 | Loss: 0.9754 | LR: 9.23e-05\n",
            "  Batch 900/26069 | Loss: 0.9764 | LR: 9.23e-05\n",
            "  Batch 1000/26069 | Loss: 0.9760 | LR: 9.22e-05\n",
            "  Batch 1100/26069 | Loss: 0.9760 | LR: 9.22e-05\n",
            "  Batch 1200/26069 | Loss: 0.9776 | LR: 9.22e-05\n",
            "  Batch 1300/26069 | Loss: 0.9770 | LR: 9.22e-05\n",
            "  Batch 1400/26069 | Loss: 0.9760 | LR: 9.22e-05\n",
            "  Batch 1500/26069 | Loss: 0.9762 | LR: 9.22e-05\n",
            "  Batch 1600/26069 | Loss: 0.9777 | LR: 9.22e-05\n",
            "  Batch 1700/26069 | Loss: 0.9760 | LR: 9.22e-05\n",
            "  Batch 1800/26069 | Loss: 0.9756 | LR: 9.22e-05\n",
            "  Batch 1900/26069 | Loss: 0.9767 | LR: 9.22e-05\n",
            "  Batch 2000/26069 | Loss: 0.9766 | LR: 9.22e-05\n",
            "  Batch 2100/26069 | Loss: 0.9759 | LR: 9.22e-05\n",
            "  Batch 2200/26069 | Loss: 0.9764 | LR: 9.22e-05\n",
            "  Batch 2300/26069 | Loss: 0.9756 | LR: 9.22e-05\n",
            "  Batch 2400/26069 | Loss: 0.9768 | LR: 9.22e-05\n",
            "  Batch 2500/26069 | Loss: 0.9769 | LR: 9.22e-05\n",
            "  Batch 2600/26069 | Loss: 0.9777 | LR: 9.22e-05\n",
            "  Batch 2700/26069 | Loss: 0.9782 | LR: 9.22e-05\n",
            "  Batch 2800/26069 | Loss: 0.9785 | LR: 9.22e-05\n",
            "  Batch 2900/26069 | Loss: 0.9787 | LR: 9.22e-05\n",
            "  Batch 3000/26069 | Loss: 0.9792 | LR: 9.22e-05\n",
            "  Batch 3100/26069 | Loss: 0.9796 | LR: 9.22e-05\n",
            "  Batch 3200/26069 | Loss: 0.9801 | LR: 9.22e-05\n",
            "  Batch 3300/26069 | Loss: 0.9800 | LR: 9.22e-05\n",
            "  Batch 3400/26069 | Loss: 0.9807 | LR: 9.22e-05\n",
            "  Batch 3500/26069 | Loss: 0.9808 | LR: 9.22e-05\n",
            "  Batch 3600/26069 | Loss: 0.9809 | LR: 9.22e-05\n",
            "  Batch 3700/26069 | Loss: 0.9802 | LR: 9.22e-05\n",
            "  Batch 3800/26069 | Loss: 0.9797 | LR: 9.22e-05\n",
            "  Batch 3900/26069 | Loss: 0.9801 | LR: 9.22e-05\n",
            "  Batch 4000/26069 | Loss: 0.9800 | LR: 9.22e-05\n",
            "  Batch 4100/26069 | Loss: 0.9801 | LR: 9.21e-05\n",
            "  Batch 4200/26069 | Loss: 0.9801 | LR: 9.21e-05\n",
            "  Batch 4300/26069 | Loss: 0.9807 | LR: 9.21e-05\n",
            "  Batch 4400/26069 | Loss: 0.9803 | LR: 9.21e-05\n",
            "  Batch 4500/26069 | Loss: 0.9805 | LR: 9.21e-05\n",
            "  Batch 4600/26069 | Loss: 0.9806 | LR: 9.21e-05\n",
            "  Batch 4700/26069 | Loss: 0.9809 | LR: 9.21e-05\n",
            "  Batch 4800/26069 | Loss: 0.9814 | LR: 9.21e-05\n",
            "  Batch 4900/26069 | Loss: 0.9816 | LR: 9.21e-05\n",
            "  Batch 5000/26069 | Loss: 0.9818 | LR: 9.21e-05\n",
            "  Batch 5100/26069 | Loss: 0.9819 | LR: 9.21e-05\n",
            "  Batch 5200/26069 | Loss: 0.9821 | LR: 9.21e-05\n",
            "  Batch 5300/26069 | Loss: 0.9818 | LR: 9.21e-05\n",
            "  Batch 5400/26069 | Loss: 0.9817 | LR: 9.21e-05\n",
            "  Batch 5500/26069 | Loss: 0.9817 | LR: 9.21e-05\n",
            "  Batch 5600/26069 | Loss: 0.9818 | LR: 9.21e-05\n",
            "  Batch 5700/26069 | Loss: 0.9817 | LR: 9.21e-05\n",
            "  Batch 5800/26069 | Loss: 0.9816 | LR: 9.21e-05\n",
            "  Batch 5900/26069 | Loss: 0.9819 | LR: 9.21e-05\n",
            "  Batch 6000/26069 | Loss: 0.9818 | LR: 9.21e-05\n",
            "  Batch 6100/26069 | Loss: 0.9818 | LR: 9.21e-05\n",
            "  Batch 6200/26069 | Loss: 0.9816 | LR: 9.21e-05\n",
            "  Batch 6300/26069 | Loss: 0.9812 | LR: 9.21e-05\n",
            "  Batch 6400/26069 | Loss: 0.9809 | LR: 9.21e-05\n",
            "  Batch 6500/26069 | Loss: 0.9812 | LR: 9.21e-05\n",
            "  Batch 6600/26069 | Loss: 0.9815 | LR: 9.21e-05\n",
            "  Batch 6700/26069 | Loss: 0.9817 | LR: 9.21e-05\n",
            "  Batch 6800/26069 | Loss: 0.9819 | LR: 9.21e-05\n",
            "  Batch 6900/26069 | Loss: 0.9816 | LR: 9.21e-05\n",
            "  Batch 7000/26069 | Loss: 0.9819 | LR: 9.21e-05\n",
            "  Batch 7100/26069 | Loss: 0.9816 | LR: 9.21e-05\n",
            "  Batch 7200/26069 | Loss: 0.9819 | LR: 9.20e-05\n",
            "  Batch 7300/26069 | Loss: 0.9818 | LR: 9.20e-05\n",
            "  Batch 7400/26069 | Loss: 0.9819 | LR: 9.20e-05\n",
            "  Batch 7500/26069 | Loss: 0.9820 | LR: 9.20e-05\n",
            "  Batch 7600/26069 | Loss: 0.9821 | LR: 9.20e-05\n",
            "  Batch 7700/26069 | Loss: 0.9821 | LR: 9.20e-05\n",
            "  Batch 7800/26069 | Loss: 0.9820 | LR: 9.20e-05\n",
            "  Batch 7900/26069 | Loss: 0.9820 | LR: 9.20e-05\n",
            "  Batch 8000/26069 | Loss: 0.9817 | LR: 9.20e-05\n",
            "  Batch 8100/26069 | Loss: 0.9815 | LR: 9.20e-05\n",
            "  Batch 8200/26069 | Loss: 0.9815 | LR: 9.20e-05\n",
            "  Batch 8300/26069 | Loss: 0.9814 | LR: 9.20e-05\n",
            "  Batch 8400/26069 | Loss: 0.9813 | LR: 9.20e-05\n",
            "  Batch 8500/26069 | Loss: 0.9812 | LR: 9.20e-05\n",
            "  Batch 8600/26069 | Loss: 0.9811 | LR: 9.20e-05\n",
            "  Batch 8700/26069 | Loss: 0.9812 | LR: 9.20e-05\n",
            "  Batch 8800/26069 | Loss: 0.9812 | LR: 9.20e-05\n",
            "  Batch 8900/26069 | Loss: 0.9812 | LR: 9.20e-05\n",
            "  Batch 9000/26069 | Loss: 0.9810 | LR: 9.20e-05\n",
            "  Batch 9100/26069 | Loss: 0.9811 | LR: 9.20e-05\n",
            "  Batch 9200/26069 | Loss: 0.9809 | LR: 9.20e-05\n",
            "  Batch 9300/26069 | Loss: 0.9808 | LR: 9.20e-05\n",
            "  Batch 9400/26069 | Loss: 0.9808 | LR: 9.20e-05\n",
            "  Batch 9500/26069 | Loss: 0.9809 | LR: 9.20e-05\n",
            "  Batch 9600/26069 | Loss: 0.9811 | LR: 9.20e-05\n",
            "  Batch 9700/26069 | Loss: 0.9808 | LR: 9.20e-05\n",
            "  Batch 9800/26069 | Loss: 0.9809 | LR: 9.20e-05\n",
            "  Batch 9900/26069 | Loss: 0.9808 | LR: 9.20e-05\n",
            "  Batch 10000/26069 | Loss: 0.9808 | LR: 9.20e-05\n",
            "  Batch 10100/26069 | Loss: 0.9809 | LR: 9.20e-05\n",
            "  Batch 10200/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 10300/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 10400/26069 | Loss: 0.9810 | LR: 9.19e-05\n",
            "  Batch 10500/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 10600/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 10700/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 10800/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 10900/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 11000/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 11100/26069 | Loss: 0.9810 | LR: 9.19e-05\n",
            "  Batch 11200/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 11300/26069 | Loss: 0.9807 | LR: 9.19e-05\n",
            "  Batch 11400/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 11500/26069 | Loss: 0.9807 | LR: 9.19e-05\n",
            "  Batch 11600/26069 | Loss: 0.9807 | LR: 9.19e-05\n",
            "  Batch 11700/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 11800/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 11900/26069 | Loss: 0.9808 | LR: 9.19e-05\n",
            "  Batch 12000/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 12100/26069 | Loss: 0.9811 | LR: 9.19e-05\n",
            "  Batch 12200/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 12300/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 12400/26069 | Loss: 0.9809 | LR: 9.19e-05\n",
            "  Batch 12500/26069 | Loss: 0.9811 | LR: 9.19e-05\n",
            "  Batch 12600/26069 | Loss: 0.9811 | LR: 9.19e-05\n",
            "  Batch 12700/26069 | Loss: 0.9812 | LR: 9.19e-05\n",
            "  Batch 12800/26069 | Loss: 0.9812 | LR: 9.19e-05\n",
            "  Batch 12900/26069 | Loss: 0.9811 | LR: 9.19e-05\n",
            "  Batch 13000/26069 | Loss: 0.9812 | LR: 9.19e-05\n",
            "  Batch 13100/26069 | Loss: 0.9813 | LR: 9.19e-05\n",
            "  Batch 13200/26069 | Loss: 0.9813 | LR: 9.19e-05\n",
            "  Batch 13300/26069 | Loss: 0.9814 | LR: 9.18e-05\n",
            "  Batch 13400/26069 | Loss: 0.9814 | LR: 9.18e-05\n",
            "  Batch 13500/26069 | Loss: 0.9815 | LR: 9.18e-05\n",
            "  Batch 13600/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 13700/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 13800/26069 | Loss: 0.9814 | LR: 9.18e-05\n",
            "  Batch 13900/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14000/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14100/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14200/26069 | Loss: 0.9817 | LR: 9.18e-05\n",
            "  Batch 14300/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14400/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14500/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14600/26069 | Loss: 0.9817 | LR: 9.18e-05\n",
            "  Batch 14700/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14800/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 14900/26069 | Loss: 0.9816 | LR: 9.18e-05\n",
            "  Batch 15000/26069 | Loss: 0.9814 | LR: 9.18e-05\n",
            "  Batch 15100/26069 | Loss: 0.9814 | LR: 9.18e-05\n",
            "  Batch 15200/26069 | Loss: 0.9813 | LR: 9.18e-05\n",
            "  Batch 15300/26069 | Loss: 0.9813 | LR: 9.18e-05\n",
            "  Batch 15400/26069 | Loss: 0.9813 | LR: 9.18e-05\n",
            "  Batch 15500/26069 | Loss: 0.9814 | LR: 9.18e-05\n",
            "  Batch 15600/26069 | Loss: 0.9813 | LR: 9.18e-05\n",
            "  Batch 15700/26069 | Loss: 0.9815 | LR: 9.18e-05\n",
            "  Batch 15800/26069 | Loss: 0.9815 | LR: 9.18e-05\n",
            "  Batch 15900/26069 | Loss: 0.9815 | LR: 9.18e-05\n",
            "  Batch 16000/26069 | Loss: 0.9815 | LR: 9.18e-05\n",
            "  Batch 16100/26069 | Loss: 0.9815 | LR: 9.18e-05\n",
            "  Batch 16200/26069 | Loss: 0.9815 | LR: 9.18e-05\n",
            "  Batch 16300/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 16400/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 16500/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 16600/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 16700/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 16800/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 16900/26069 | Loss: 0.9815 | LR: 9.17e-05\n",
            "  Batch 17000/26069 | Loss: 0.9816 | LR: 9.17e-05\n",
            "  Batch 17100/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 17200/26069 | Loss: 0.9815 | LR: 9.17e-05\n",
            "  Batch 17300/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 17400/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 17500/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 17600/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 17700/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 17800/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 17900/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 18000/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 18100/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 18200/26069 | Loss: 0.9815 | LR: 9.17e-05\n",
            "  Batch 18300/26069 | Loss: 0.9815 | LR: 9.17e-05\n",
            "  Batch 18400/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 18500/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 18600/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 18700/26069 | Loss: 0.9815 | LR: 9.17e-05\n",
            "  Batch 18800/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 18900/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 19000/26069 | Loss: 0.9813 | LR: 9.17e-05\n",
            "  Batch 19100/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 19200/26069 | Loss: 0.9814 | LR: 9.17e-05\n",
            "  Batch 19300/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 19400/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 19500/26069 | Loss: 0.9816 | LR: 9.16e-05\n",
            "  Batch 19600/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 19700/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 19800/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 19900/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 20000/26069 | Loss: 0.9816 | LR: 9.16e-05\n",
            "  Batch 20100/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 20200/26069 | Loss: 0.9814 | LR: 9.16e-05\n",
            "  Batch 20300/26069 | Loss: 0.9814 | LR: 9.16e-05\n",
            "  Batch 20400/26069 | Loss: 0.9814 | LR: 9.16e-05\n",
            "  Batch 20500/26069 | Loss: 0.9815 | LR: 9.16e-05\n",
            "  Batch 20600/26069 | Loss: 0.9814 | LR: 9.16e-05\n",
            "  Batch 20700/26069 | Loss: 0.9814 | LR: 9.16e-05\n",
            "  Batch 20800/26069 | Loss: 0.9814 | LR: 9.16e-05\n",
            "  Batch 20900/26069 | Loss: 0.9814 | LR: 9.16e-05\n",
            "  Batch 21000/26069 | Loss: 0.9813 | LR: 9.16e-05\n",
            "  Batch 21100/26069 | Loss: 0.9813 | LR: 9.16e-05\n",
            "  Batch 21200/26069 | Loss: 0.9813 | LR: 9.16e-05\n",
            "  Batch 21300/26069 | Loss: 0.9812 | LR: 9.16e-05\n",
            "  Batch 21400/26069 | Loss: 0.9811 | LR: 9.16e-05\n",
            "  Batch 21500/26069 | Loss: 0.9810 | LR: 9.16e-05\n",
            "  Batch 21600/26069 | Loss: 0.9811 | LR: 9.16e-05\n",
            "  Batch 21700/26069 | Loss: 0.9810 | LR: 9.16e-05\n",
            "  Batch 21800/26069 | Loss: 0.9810 | LR: 9.16e-05\n",
            "  Batch 21900/26069 | Loss: 0.9809 | LR: 9.16e-05\n",
            "  Batch 22000/26069 | Loss: 0.9809 | LR: 9.16e-05\n",
            "  Batch 22100/26069 | Loss: 0.9809 | LR: 9.16e-05\n",
            "  Batch 22200/26069 | Loss: 0.9809 | LR: 9.16e-05\n",
            "  Batch 22300/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 22400/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 22500/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 22600/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 22700/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 22800/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 22900/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 23000/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 23100/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 23200/26069 | Loss: 0.9810 | LR: 9.15e-05\n",
            "  Batch 23300/26069 | Loss: 0.9810 | LR: 9.15e-05\n",
            "  Batch 23400/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 23500/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 23600/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 23700/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 23800/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 23900/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 24000/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 24100/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 24200/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 24300/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 24400/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 24500/26069 | Loss: 0.9809 | LR: 9.15e-05\n",
            "  Batch 24600/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 24700/26069 | Loss: 0.9808 | LR: 9.15e-05\n",
            "  Batch 24800/26069 | Loss: 0.9807 | LR: 9.15e-05\n",
            "  Batch 24900/26069 | Loss: 0.9807 | LR: 9.15e-05\n",
            "  Batch 25000/26069 | Loss: 0.9807 | LR: 9.15e-05\n",
            "  Batch 25100/26069 | Loss: 0.9807 | LR: 9.15e-05\n",
            "  Batch 25200/26069 | Loss: 0.9807 | LR: 9.15e-05\n",
            "  Batch 25300/26069 | Loss: 0.9808 | LR: 9.14e-05\n",
            "  Batch 25400/26069 | Loss: 0.9808 | LR: 9.14e-05\n",
            "  Batch 25500/26069 | Loss: 0.9808 | LR: 9.14e-05\n",
            "  Batch 25600/26069 | Loss: 0.9808 | LR: 9.14e-05\n",
            "  Batch 25700/26069 | Loss: 0.9807 | LR: 9.14e-05\n",
            "  Batch 25800/26069 | Loss: 0.9807 | LR: 9.14e-05\n",
            "  Batch 25900/26069 | Loss: 0.9807 | LR: 9.14e-05\n",
            "  Batch 26000/26069 | Loss: 0.9807 | LR: 9.14e-05\n",
            "\n",
            "  Train Loss: 0.9807\n",
            "  Val Loss:   0.9606 \n",
            "  Time:       313.2s\n",
            "\n",
            "Epoch 20/100\n",
            "  Batch 100/26069 | Loss: 0.9997 | LR: 9.14e-05\n",
            "  Batch 200/26069 | Loss: 0.9905 | LR: 9.14e-05\n",
            "  Batch 300/26069 | Loss: 0.9927 | LR: 9.14e-05\n",
            "  Batch 400/26069 | Loss: 0.9882 | LR: 9.14e-05\n",
            "  Batch 500/26069 | Loss: 0.9823 | LR: 9.14e-05\n",
            "  Batch 600/26069 | Loss: 0.9853 | LR: 9.14e-05\n",
            "  Batch 700/26069 | Loss: 0.9802 | LR: 9.14e-05\n",
            "  Batch 800/26069 | Loss: 0.9812 | LR: 9.14e-05\n",
            "  Batch 900/26069 | Loss: 0.9783 | LR: 9.14e-05\n",
            "  Batch 1000/26069 | Loss: 0.9803 | LR: 9.14e-05\n",
            "  Batch 1100/26069 | Loss: 0.9808 | LR: 9.14e-05\n",
            "  Batch 1200/26069 | Loss: 0.9808 | LR: 9.14e-05\n",
            "  Batch 1300/26069 | Loss: 0.9798 | LR: 9.14e-05\n",
            "  Batch 1400/26069 | Loss: 0.9810 | LR: 9.14e-05\n",
            "  Batch 1500/26069 | Loss: 0.9816 | LR: 9.14e-05\n",
            "  Batch 1600/26069 | Loss: 0.9821 | LR: 9.14e-05\n",
            "  Batch 1700/26069 | Loss: 0.9817 | LR: 9.14e-05\n",
            "  Batch 1800/26069 | Loss: 0.9803 | LR: 9.14e-05\n",
            "  Batch 1900/26069 | Loss: 0.9812 | LR: 9.14e-05\n",
            "  Batch 2000/26069 | Loss: 0.9806 | LR: 9.14e-05\n",
            "  Batch 2100/26069 | Loss: 0.9805 | LR: 9.14e-05\n",
            "  Batch 2200/26069 | Loss: 0.9807 | LR: 9.13e-05\n",
            "  Batch 2300/26069 | Loss: 0.9799 | LR: 9.13e-05\n",
            "  Batch 2400/26069 | Loss: 0.9795 | LR: 9.13e-05\n",
            "  Batch 2500/26069 | Loss: 0.9790 | LR: 9.13e-05\n",
            "  Batch 2600/26069 | Loss: 0.9791 | LR: 9.13e-05\n",
            "  Batch 2700/26069 | Loss: 0.9794 | LR: 9.13e-05\n",
            "  Batch 2800/26069 | Loss: 0.9800 | LR: 9.13e-05\n",
            "  Batch 2900/26069 | Loss: 0.9802 | LR: 9.13e-05\n",
            "  Batch 3000/26069 | Loss: 0.9799 | LR: 9.13e-05\n",
            "  Batch 3100/26069 | Loss: 0.9804 | LR: 9.13e-05\n",
            "  Batch 3200/26069 | Loss: 0.9807 | LR: 9.13e-05\n",
            "  Batch 3300/26069 | Loss: 0.9812 | LR: 9.13e-05\n",
            "  Batch 3400/26069 | Loss: 0.9806 | LR: 9.13e-05\n",
            "  Batch 3500/26069 | Loss: 0.9807 | LR: 9.13e-05\n",
            "  Batch 3600/26069 | Loss: 0.9800 | LR: 9.13e-05\n",
            "  Batch 3700/26069 | Loss: 0.9800 | LR: 9.13e-05\n",
            "  Batch 3800/26069 | Loss: 0.9795 | LR: 9.13e-05\n",
            "  Batch 3900/26069 | Loss: 0.9787 | LR: 9.13e-05\n",
            "  Batch 4000/26069 | Loss: 0.9788 | LR: 9.13e-05\n",
            "  Batch 4100/26069 | Loss: 0.9789 | LR: 9.13e-05\n",
            "  Batch 4200/26069 | Loss: 0.9789 | LR: 9.13e-05\n",
            "  Batch 4300/26069 | Loss: 0.9792 | LR: 9.13e-05\n",
            "  Batch 4400/26069 | Loss: 0.9794 | LR: 9.13e-05\n",
            "  Batch 4500/26069 | Loss: 0.9789 | LR: 9.13e-05\n",
            "  Batch 4600/26069 | Loss: 0.9788 | LR: 9.13e-05\n",
            "  Batch 4700/26069 | Loss: 0.9788 | LR: 9.13e-05\n",
            "  Batch 4800/26069 | Loss: 0.9789 | LR: 9.13e-05\n",
            "  Batch 4900/26069 | Loss: 0.9790 | LR: 9.13e-05\n",
            "  Batch 5000/26069 | Loss: 0.9794 | LR: 9.13e-05\n",
            "  Batch 5100/26069 | Loss: 0.9794 | LR: 9.12e-05\n",
            "  Batch 5200/26069 | Loss: 0.9790 | LR: 9.12e-05\n",
            "  Batch 5300/26069 | Loss: 0.9789 | LR: 9.12e-05\n",
            "  Batch 5400/26069 | Loss: 0.9787 | LR: 9.12e-05\n",
            "  Batch 5500/26069 | Loss: 0.9785 | LR: 9.12e-05\n",
            "  Batch 5600/26069 | Loss: 0.9788 | LR: 9.12e-05\n",
            "  Batch 5700/26069 | Loss: 0.9786 | LR: 9.12e-05\n",
            "  Batch 5800/26069 | Loss: 0.9787 | LR: 9.12e-05\n",
            "  Batch 5900/26069 | Loss: 0.9787 | LR: 9.12e-05\n",
            "  Batch 6000/26069 | Loss: 0.9784 | LR: 9.12e-05\n",
            "  Batch 6100/26069 | Loss: 0.9782 | LR: 9.12e-05\n",
            "  Batch 6200/26069 | Loss: 0.9786 | LR: 9.12e-05\n",
            "  Batch 6300/26069 | Loss: 0.9787 | LR: 9.12e-05\n",
            "  Batch 6400/26069 | Loss: 0.9785 | LR: 9.12e-05\n",
            "  Batch 6500/26069 | Loss: 0.9785 | LR: 9.12e-05\n",
            "  Batch 6600/26069 | Loss: 0.9786 | LR: 9.12e-05\n",
            "  Batch 6700/26069 | Loss: 0.9787 | LR: 9.12e-05\n",
            "  Batch 6800/26069 | Loss: 0.9788 | LR: 9.12e-05\n",
            "  Batch 6900/26069 | Loss: 0.9784 | LR: 9.12e-05\n",
            "  Batch 7000/26069 | Loss: 0.9783 | LR: 9.12e-05\n",
            "  Batch 7100/26069 | Loss: 0.9784 | LR: 9.12e-05\n",
            "  Batch 7200/26069 | Loss: 0.9782 | LR: 9.12e-05\n",
            "  Batch 7300/26069 | Loss: 0.9782 | LR: 9.12e-05\n",
            "  Batch 7400/26069 | Loss: 0.9785 | LR: 9.12e-05\n",
            "  Batch 7500/26069 | Loss: 0.9784 | LR: 9.12e-05\n",
            "  Batch 7600/26069 | Loss: 0.9785 | LR: 9.12e-05\n",
            "  Batch 7700/26069 | Loss: 0.9788 | LR: 9.12e-05\n",
            "  Batch 7800/26069 | Loss: 0.9786 | LR: 9.12e-05\n",
            "  Batch 7900/26069 | Loss: 0.9786 | LR: 9.12e-05\n",
            "  Batch 8000/26069 | Loss: 0.9787 | LR: 9.11e-05\n",
            "  Batch 8100/26069 | Loss: 0.9788 | LR: 9.11e-05\n",
            "  Batch 8200/26069 | Loss: 0.9787 | LR: 9.11e-05\n",
            "  Batch 8300/26069 | Loss: 0.9785 | LR: 9.11e-05\n",
            "  Batch 8400/26069 | Loss: 0.9787 | LR: 9.11e-05\n",
            "  Batch 8500/26069 | Loss: 0.9788 | LR: 9.11e-05\n",
            "  Batch 8600/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 8700/26069 | Loss: 0.9788 | LR: 9.11e-05\n",
            "  Batch 8800/26069 | Loss: 0.9787 | LR: 9.11e-05\n",
            "  Batch 8900/26069 | Loss: 0.9788 | LR: 9.11e-05\n",
            "  Batch 9000/26069 | Loss: 0.9788 | LR: 9.11e-05\n",
            "  Batch 9100/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 9200/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 9300/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 9400/26069 | Loss: 0.9792 | LR: 9.11e-05\n",
            "  Batch 9500/26069 | Loss: 0.9791 | LR: 9.11e-05\n",
            "  Batch 9600/26069 | Loss: 0.9792 | LR: 9.11e-05\n",
            "  Batch 9700/26069 | Loss: 0.9791 | LR: 9.11e-05\n",
            "  Batch 9800/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 9900/26069 | Loss: 0.9793 | LR: 9.11e-05\n",
            "  Batch 10000/26069 | Loss: 0.9791 | LR: 9.11e-05\n",
            "  Batch 10100/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 10200/26069 | Loss: 0.9789 | LR: 9.11e-05\n",
            "  Batch 10300/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 10400/26069 | Loss: 0.9790 | LR: 9.11e-05\n",
            "  Batch 10500/26069 | Loss: 0.9788 | LR: 9.11e-05\n",
            "  Batch 10600/26069 | Loss: 0.9788 | LR: 9.11e-05\n",
            "  Batch 10700/26069 | Loss: 0.9789 | LR: 9.11e-05\n",
            "  Batch 10800/26069 | Loss: 0.9789 | LR: 9.11e-05\n",
            "  Batch 10900/26069 | Loss: 0.9789 | LR: 9.11e-05\n",
            "  Batch 11000/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 11100/26069 | Loss: 0.9791 | LR: 9.10e-05\n",
            "  Batch 11200/26069 | Loss: 0.9790 | LR: 9.10e-05\n",
            "  Batch 11300/26069 | Loss: 0.9793 | LR: 9.10e-05\n",
            "  Batch 11400/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 11500/26069 | Loss: 0.9791 | LR: 9.10e-05\n",
            "  Batch 11600/26069 | Loss: 0.9791 | LR: 9.10e-05\n",
            "  Batch 11700/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 11800/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 11900/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 12000/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 12100/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 12200/26069 | Loss: 0.9793 | LR: 9.10e-05\n",
            "  Batch 12300/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 12400/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 12500/26069 | Loss: 0.9792 | LR: 9.10e-05\n",
            "  Batch 12600/26069 | Loss: 0.9791 | LR: 9.10e-05\n",
            "  Batch 12700/26069 | Loss: 0.9791 | LR: 9.10e-05\n",
            "  Batch 12800/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 12900/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 13000/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 13100/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 13200/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 13300/26069 | Loss: 0.9788 | LR: 9.10e-05\n",
            "  Batch 13400/26069 | Loss: 0.9788 | LR: 9.10e-05\n",
            "  Batch 13500/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 13600/26069 | Loss: 0.9789 | LR: 9.10e-05\n",
            "  Batch 13700/26069 | Loss: 0.9788 | LR: 9.10e-05\n",
            "  Batch 13800/26069 | Loss: 0.9787 | LR: 9.10e-05\n",
            "  Batch 13900/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 14000/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 14100/26069 | Loss: 0.9789 | LR: 9.09e-05\n",
            "  Batch 14200/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 14300/26069 | Loss: 0.9790 | LR: 9.09e-05\n",
            "  Batch 14400/26069 | Loss: 0.9789 | LR: 9.09e-05\n",
            "  Batch 14500/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 14600/26069 | Loss: 0.9786 | LR: 9.09e-05\n",
            "  Batch 14700/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 14800/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 14900/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 15000/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 15100/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 15200/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 15300/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 15400/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 15500/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 15600/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 15700/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 15800/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 15900/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 16000/26069 | Loss: 0.9786 | LR: 9.09e-05\n",
            "  Batch 16100/26069 | Loss: 0.9786 | LR: 9.09e-05\n",
            "  Batch 16200/26069 | Loss: 0.9786 | LR: 9.09e-05\n",
            "  Batch 16300/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 16400/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 16500/26069 | Loss: 0.9788 | LR: 9.09e-05\n",
            "  Batch 16600/26069 | Loss: 0.9787 | LR: 9.09e-05\n",
            "  Batch 16700/26069 | Loss: 0.9788 | LR: 9.08e-05\n",
            "  Batch 16800/26069 | Loss: 0.9786 | LR: 9.08e-05\n",
            "  Batch 16900/26069 | Loss: 0.9786 | LR: 9.08e-05\n",
            "  Batch 17000/26069 | Loss: 0.9787 | LR: 9.08e-05\n",
            "  Batch 17100/26069 | Loss: 0.9787 | LR: 9.08e-05\n",
            "  Batch 17200/26069 | Loss: 0.9787 | LR: 9.08e-05\n",
            "  Batch 17300/26069 | Loss: 0.9787 | LR: 9.08e-05\n",
            "  Batch 17400/26069 | Loss: 0.9788 | LR: 9.08e-05\n",
            "  Batch 17500/26069 | Loss: 0.9789 | LR: 9.08e-05\n",
            "  Batch 17600/26069 | Loss: 0.9788 | LR: 9.08e-05\n",
            "  Batch 17700/26069 | Loss: 0.9788 | LR: 9.08e-05\n",
            "  Batch 17800/26069 | Loss: 0.9788 | LR: 9.08e-05\n",
            "  Batch 17900/26069 | Loss: 0.9788 | LR: 9.08e-05\n",
            "  Batch 18000/26069 | Loss: 0.9789 | LR: 9.08e-05\n",
            "  Batch 18100/26069 | Loss: 0.9790 | LR: 9.08e-05\n",
            "  Batch 18200/26069 | Loss: 0.9790 | LR: 9.08e-05\n",
            "  Batch 18300/26069 | Loss: 0.9790 | LR: 9.08e-05\n",
            "  Batch 18400/26069 | Loss: 0.9792 | LR: 9.08e-05\n",
            "  Batch 18500/26069 | Loss: 0.9792 | LR: 9.08e-05\n",
            "  Batch 18600/26069 | Loss: 0.9792 | LR: 9.08e-05\n",
            "  Batch 18700/26069 | Loss: 0.9792 | LR: 9.08e-05\n",
            "  Batch 18800/26069 | Loss: 0.9792 | LR: 9.08e-05\n",
            "  Batch 18900/26069 | Loss: 0.9790 | LR: 9.08e-05\n",
            "  Batch 19000/26069 | Loss: 0.9790 | LR: 9.08e-05\n",
            "  Batch 19100/26069 | Loss: 0.9789 | LR: 9.08e-05\n",
            "  Batch 19200/26069 | Loss: 0.9789 | LR: 9.08e-05\n",
            "  Batch 19300/26069 | Loss: 0.9789 | LR: 9.08e-05\n",
            "  Batch 19400/26069 | Loss: 0.9791 | LR: 9.08e-05\n",
            "  Batch 19500/26069 | Loss: 0.9791 | LR: 9.08e-05\n",
            "  Batch 19600/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 19700/26069 | Loss: 0.9791 | LR: 9.07e-05\n",
            "  Batch 19800/26069 | Loss: 0.9791 | LR: 9.07e-05\n",
            "  Batch 19900/26069 | Loss: 0.9790 | LR: 9.07e-05\n",
            "  Batch 20000/26069 | Loss: 0.9790 | LR: 9.07e-05\n",
            "  Batch 20100/26069 | Loss: 0.9790 | LR: 9.07e-05\n",
            "  Batch 20200/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 20300/26069 | Loss: 0.9788 | LR: 9.07e-05\n",
            "  Batch 20400/26069 | Loss: 0.9788 | LR: 9.07e-05\n",
            "  Batch 20500/26069 | Loss: 0.9788 | LR: 9.07e-05\n",
            "  Batch 20600/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 20700/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 20800/26069 | Loss: 0.9788 | LR: 9.07e-05\n",
            "  Batch 20900/26069 | Loss: 0.9790 | LR: 9.07e-05\n",
            "  Batch 21000/26069 | Loss: 0.9790 | LR: 9.07e-05\n",
            "  Batch 21100/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21200/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21300/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21400/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21500/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21600/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21700/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21800/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 21900/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 22000/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 22100/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 22200/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 22300/26069 | Loss: 0.9789 | LR: 9.07e-05\n",
            "  Batch 22400/26069 | Loss: 0.9790 | LR: 9.07e-05\n",
            "  Batch 22500/26069 | Loss: 0.9789 | LR: 9.06e-05\n",
            "  Batch 22600/26069 | Loss: 0.9789 | LR: 9.06e-05\n",
            "  Batch 22700/26069 | Loss: 0.9788 | LR: 9.06e-05\n",
            "  Batch 22800/26069 | Loss: 0.9789 | LR: 9.06e-05\n",
            "  Batch 22900/26069 | Loss: 0.9788 | LR: 9.06e-05\n",
            "  Batch 23000/26069 | Loss: 0.9788 | LR: 9.06e-05\n",
            "  Batch 23100/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 23200/26069 | Loss: 0.9788 | LR: 9.06e-05\n",
            "  Batch 23300/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 23400/26069 | Loss: 0.9786 | LR: 9.06e-05\n",
            "  Batch 23500/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 23600/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 23700/26069 | Loss: 0.9786 | LR: 9.06e-05\n",
            "  Batch 23800/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 23900/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 24000/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 24100/26069 | Loss: 0.9786 | LR: 9.06e-05\n",
            "  Batch 24200/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 24300/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 24400/26069 | Loss: 0.9786 | LR: 9.06e-05\n",
            "  Batch 24500/26069 | Loss: 0.9787 | LR: 9.06e-05\n",
            "  Batch 24600/26069 | Loss: 0.9788 | LR: 9.06e-05\n",
            "  Batch 24700/26069 | Loss: 0.9788 | LR: 9.06e-05\n",
            "  Batch 24800/26069 | Loss: 0.9788 | LR: 9.06e-05\n",
            "  Batch 24900/26069 | Loss: 0.9789 | LR: 9.06e-05\n",
            "  Batch 25000/26069 | Loss: 0.9789 | LR: 9.06e-05\n",
            "  Batch 25100/26069 | Loss: 0.9789 | LR: 9.06e-05\n",
            "  Batch 25200/26069 | Loss: 0.9789 | LR: 9.06e-05\n",
            "  Batch 25300/26069 | Loss: 0.9790 | LR: 9.05e-05\n",
            "  Batch 25400/26069 | Loss: 0.9791 | LR: 9.05e-05\n",
            "  Batch 25500/26069 | Loss: 0.9791 | LR: 9.05e-05\n",
            "  Batch 25600/26069 | Loss: 0.9790 | LR: 9.05e-05\n",
            "  Batch 25700/26069 | Loss: 0.9792 | LR: 9.05e-05\n",
            "  Batch 25800/26069 | Loss: 0.9792 | LR: 9.05e-05\n",
            "  Batch 25900/26069 | Loss: 0.9791 | LR: 9.05e-05\n",
            "  Batch 26000/26069 | Loss: 0.9791 | LR: 9.05e-05\n",
            "\n",
            "  Train Loss: 0.9791\n",
            "  Val Loss:   0.9567 [BEST]\n",
            "  Time:       313.0s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_20.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 21/100\n",
            "  Batch 100/26069 | Loss: 0.9775 | LR: 9.05e-05\n",
            "  Batch 200/26069 | Loss: 0.9829 | LR: 9.05e-05\n",
            "  Batch 300/26069 | Loss: 0.9794 | LR: 9.05e-05\n",
            "  Batch 400/26069 | Loss: 0.9798 | LR: 9.05e-05\n",
            "  Batch 500/26069 | Loss: 0.9805 | LR: 9.05e-05\n",
            "  Batch 600/26069 | Loss: 0.9774 | LR: 9.05e-05\n",
            "  Batch 700/26069 | Loss: 0.9799 | LR: 9.05e-05\n",
            "  Batch 800/26069 | Loss: 0.9809 | LR: 9.05e-05\n",
            "  Batch 900/26069 | Loss: 0.9823 | LR: 9.05e-05\n",
            "  Batch 1000/26069 | Loss: 0.9808 | LR: 9.05e-05\n",
            "  Batch 1100/26069 | Loss: 0.9814 | LR: 9.05e-05\n",
            "  Batch 1200/26069 | Loss: 0.9828 | LR: 9.05e-05\n",
            "  Batch 1300/26069 | Loss: 0.9826 | LR: 9.05e-05\n",
            "  Batch 1400/26069 | Loss: 0.9826 | LR: 9.05e-05\n",
            "  Batch 1500/26069 | Loss: 0.9823 | LR: 9.05e-05\n",
            "  Batch 1600/26069 | Loss: 0.9815 | LR: 9.05e-05\n",
            "  Batch 1700/26069 | Loss: 0.9820 | LR: 9.05e-05\n",
            "  Batch 1800/26069 | Loss: 0.9810 | LR: 9.05e-05\n",
            "  Batch 1900/26069 | Loss: 0.9815 | LR: 9.05e-05\n",
            "  Batch 2000/26069 | Loss: 0.9804 | LR: 9.05e-05\n",
            "  Batch 2100/26069 | Loss: 0.9805 | LR: 9.04e-05\n",
            "  Batch 2200/26069 | Loss: 0.9814 | LR: 9.04e-05\n",
            "  Batch 2300/26069 | Loss: 0.9818 | LR: 9.04e-05\n",
            "  Batch 2400/26069 | Loss: 0.9816 | LR: 9.04e-05\n",
            "  Batch 2500/26069 | Loss: 0.9814 | LR: 9.04e-05\n",
            "  Batch 2600/26069 | Loss: 0.9812 | LR: 9.04e-05\n",
            "  Batch 2700/26069 | Loss: 0.9812 | LR: 9.04e-05\n",
            "  Batch 2800/26069 | Loss: 0.9810 | LR: 9.04e-05\n",
            "  Batch 2900/26069 | Loss: 0.9813 | LR: 9.04e-05\n",
            "  Batch 3000/26069 | Loss: 0.9811 | LR: 9.04e-05\n",
            "  Batch 3100/26069 | Loss: 0.9809 | LR: 9.04e-05\n",
            "  Batch 3200/26069 | Loss: 0.9807 | LR: 9.04e-05\n",
            "  Batch 3300/26069 | Loss: 0.9809 | LR: 9.04e-05\n",
            "  Batch 3400/26069 | Loss: 0.9812 | LR: 9.04e-05\n",
            "  Batch 3500/26069 | Loss: 0.9812 | LR: 9.04e-05\n",
            "  Batch 3600/26069 | Loss: 0.9811 | LR: 9.04e-05\n",
            "  Batch 3700/26069 | Loss: 0.9812 | LR: 9.04e-05\n",
            "  Batch 3800/26069 | Loss: 0.9809 | LR: 9.04e-05\n",
            "  Batch 3900/26069 | Loss: 0.9807 | LR: 9.04e-05\n",
            "  Batch 4000/26069 | Loss: 0.9809 | LR: 9.04e-05\n",
            "  Batch 4100/26069 | Loss: 0.9808 | LR: 9.04e-05\n",
            "  Batch 4200/26069 | Loss: 0.9805 | LR: 9.04e-05\n",
            "  Batch 4300/26069 | Loss: 0.9805 | LR: 9.04e-05\n",
            "  Batch 4400/26069 | Loss: 0.9806 | LR: 9.04e-05\n",
            "  Batch 4500/26069 | Loss: 0.9809 | LR: 9.04e-05\n",
            "  Batch 4600/26069 | Loss: 0.9806 | LR: 9.04e-05\n",
            "  Batch 4700/26069 | Loss: 0.9808 | LR: 9.04e-05\n",
            "  Batch 4800/26069 | Loss: 0.9802 | LR: 9.04e-05\n",
            "  Batch 4900/26069 | Loss: 0.9801 | LR: 9.03e-05\n",
            "  Batch 5000/26069 | Loss: 0.9801 | LR: 9.03e-05\n",
            "  Batch 5100/26069 | Loss: 0.9806 | LR: 9.03e-05\n",
            "  Batch 5200/26069 | Loss: 0.9806 | LR: 9.03e-05\n",
            "  Batch 5300/26069 | Loss: 0.9809 | LR: 9.03e-05\n",
            "  Batch 5400/26069 | Loss: 0.9805 | LR: 9.03e-05\n",
            "  Batch 5500/26069 | Loss: 0.9806 | LR: 9.03e-05\n",
            "  Batch 5600/26069 | Loss: 0.9802 | LR: 9.03e-05\n",
            "  Batch 5700/26069 | Loss: 0.9802 | LR: 9.03e-05\n",
            "  Batch 5800/26069 | Loss: 0.9804 | LR: 9.03e-05\n",
            "  Batch 5900/26069 | Loss: 0.9804 | LR: 9.03e-05\n",
            "  Batch 6000/26069 | Loss: 0.9802 | LR: 9.03e-05\n",
            "  Batch 6100/26069 | Loss: 0.9803 | LR: 9.03e-05\n",
            "  Batch 6200/26069 | Loss: 0.9802 | LR: 9.03e-05\n",
            "  Batch 6300/26069 | Loss: 0.9801 | LR: 9.03e-05\n",
            "  Batch 6400/26069 | Loss: 0.9801 | LR: 9.03e-05\n",
            "  Batch 6500/26069 | Loss: 0.9800 | LR: 9.03e-05\n",
            "  Batch 6600/26069 | Loss: 0.9797 | LR: 9.03e-05\n",
            "  Batch 6700/26069 | Loss: 0.9799 | LR: 9.03e-05\n",
            "  Batch 6800/26069 | Loss: 0.9799 | LR: 9.03e-05\n",
            "  Batch 6900/26069 | Loss: 0.9798 | LR: 9.03e-05\n",
            "  Batch 7000/26069 | Loss: 0.9800 | LR: 9.03e-05\n",
            "  Batch 7100/26069 | Loss: 0.9800 | LR: 9.03e-05\n",
            "  Batch 7200/26069 | Loss: 0.9801 | LR: 9.03e-05\n",
            "  Batch 7300/26069 | Loss: 0.9800 | LR: 9.03e-05\n",
            "  Batch 7400/26069 | Loss: 0.9799 | LR: 9.03e-05\n",
            "  Batch 7500/26069 | Loss: 0.9800 | LR: 9.03e-05\n",
            "  Batch 7600/26069 | Loss: 0.9799 | LR: 9.03e-05\n",
            "  Batch 7700/26069 | Loss: 0.9797 | LR: 9.02e-05\n",
            "  Batch 7800/26069 | Loss: 0.9795 | LR: 9.02e-05\n",
            "  Batch 7900/26069 | Loss: 0.9796 | LR: 9.02e-05\n",
            "  Batch 8000/26069 | Loss: 0.9795 | LR: 9.02e-05\n",
            "  Batch 8100/26069 | Loss: 0.9795 | LR: 9.02e-05\n",
            "  Batch 8200/26069 | Loss: 0.9795 | LR: 9.02e-05\n",
            "  Batch 8300/26069 | Loss: 0.9794 | LR: 9.02e-05\n",
            "  Batch 8400/26069 | Loss: 0.9795 | LR: 9.02e-05\n",
            "  Batch 8500/26069 | Loss: 0.9797 | LR: 9.02e-05\n",
            "  Batch 8600/26069 | Loss: 0.9796 | LR: 9.02e-05\n",
            "  Batch 8700/26069 | Loss: 0.9794 | LR: 9.02e-05\n",
            "  Batch 8800/26069 | Loss: 0.9791 | LR: 9.02e-05\n",
            "  Batch 8900/26069 | Loss: 0.9789 | LR: 9.02e-05\n",
            "  Batch 9000/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 9100/26069 | Loss: 0.9787 | LR: 9.02e-05\n",
            "  Batch 9200/26069 | Loss: 0.9789 | LR: 9.02e-05\n",
            "  Batch 9300/26069 | Loss: 0.9789 | LR: 9.02e-05\n",
            "  Batch 9400/26069 | Loss: 0.9788 | LR: 9.02e-05\n",
            "  Batch 9500/26069 | Loss: 0.9788 | LR: 9.02e-05\n",
            "  Batch 9600/26069 | Loss: 0.9789 | LR: 9.02e-05\n",
            "  Batch 9700/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 9800/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 9900/26069 | Loss: 0.9791 | LR: 9.02e-05\n",
            "  Batch 10000/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 10100/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 10200/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 10300/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 10400/26069 | Loss: 0.9790 | LR: 9.02e-05\n",
            "  Batch 10500/26069 | Loss: 0.9790 | LR: 9.01e-05\n",
            "  Batch 10600/26069 | Loss: 0.9791 | LR: 9.01e-05\n",
            "  Batch 10700/26069 | Loss: 0.9788 | LR: 9.01e-05\n",
            "  Batch 10800/26069 | Loss: 0.9788 | LR: 9.01e-05\n",
            "  Batch 10900/26069 | Loss: 0.9787 | LR: 9.01e-05\n",
            "  Batch 11000/26069 | Loss: 0.9787 | LR: 9.01e-05\n",
            "  Batch 11100/26069 | Loss: 0.9787 | LR: 9.01e-05\n",
            "  Batch 11200/26069 | Loss: 0.9786 | LR: 9.01e-05\n",
            "  Batch 11300/26069 | Loss: 0.9787 | LR: 9.01e-05\n",
            "  Batch 11400/26069 | Loss: 0.9787 | LR: 9.01e-05\n",
            "  Batch 11500/26069 | Loss: 0.9787 | LR: 9.01e-05\n",
            "  Batch 11600/26069 | Loss: 0.9786 | LR: 9.01e-05\n",
            "  Batch 11700/26069 | Loss: 0.9785 | LR: 9.01e-05\n",
            "  Batch 11800/26069 | Loss: 0.9783 | LR: 9.01e-05\n",
            "  Batch 11900/26069 | Loss: 0.9784 | LR: 9.01e-05\n",
            "  Batch 12000/26069 | Loss: 0.9782 | LR: 9.01e-05\n",
            "  Batch 12100/26069 | Loss: 0.9782 | LR: 9.01e-05\n",
            "  Batch 12200/26069 | Loss: 0.9783 | LR: 9.01e-05\n",
            "  Batch 12300/26069 | Loss: 0.9785 | LR: 9.01e-05\n",
            "  Batch 12400/26069 | Loss: 0.9785 | LR: 9.01e-05\n",
            "  Batch 12500/26069 | Loss: 0.9785 | LR: 9.01e-05\n",
            "  Batch 12600/26069 | Loss: 0.9785 | LR: 9.01e-05\n",
            "  Batch 12700/26069 | Loss: 0.9784 | LR: 9.01e-05\n",
            "  Batch 12800/26069 | Loss: 0.9785 | LR: 9.01e-05\n",
            "  Batch 12900/26069 | Loss: 0.9786 | LR: 9.01e-05\n",
            "  Batch 13000/26069 | Loss: 0.9787 | LR: 9.01e-05\n",
            "  Batch 13100/26069 | Loss: 0.9788 | LR: 9.01e-05\n",
            "  Batch 13200/26069 | Loss: 0.9788 | LR: 9.01e-05\n",
            "  Batch 13300/26069 | Loss: 0.9787 | LR: 9.00e-05\n",
            "  Batch 13400/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 13500/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 13600/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 13700/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 13800/26069 | Loss: 0.9784 | LR: 9.00e-05\n",
            "  Batch 13900/26069 | Loss: 0.9787 | LR: 9.00e-05\n",
            "  Batch 14000/26069 | Loss: 0.9787 | LR: 9.00e-05\n",
            "  Batch 14100/26069 | Loss: 0.9787 | LR: 9.00e-05\n",
            "  Batch 14200/26069 | Loss: 0.9787 | LR: 9.00e-05\n",
            "  Batch 14300/26069 | Loss: 0.9787 | LR: 9.00e-05\n",
            "  Batch 14400/26069 | Loss: 0.9786 | LR: 9.00e-05\n",
            "  Batch 14500/26069 | Loss: 0.9786 | LR: 9.00e-05\n",
            "  Batch 14600/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 14700/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 14800/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 14900/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 15000/26069 | Loss: 0.9785 | LR: 9.00e-05\n",
            "  Batch 15100/26069 | Loss: 0.9783 | LR: 9.00e-05\n",
            "  Batch 15200/26069 | Loss: 0.9782 | LR: 9.00e-05\n",
            "  Batch 15300/26069 | Loss: 0.9781 | LR: 9.00e-05\n",
            "  Batch 15400/26069 | Loss: 0.9781 | LR: 9.00e-05\n",
            "  Batch 15500/26069 | Loss: 0.9781 | LR: 9.00e-05\n",
            "  Batch 15600/26069 | Loss: 0.9782 | LR: 9.00e-05\n",
            "  Batch 15700/26069 | Loss: 0.9782 | LR: 9.00e-05\n",
            "  Batch 15800/26069 | Loss: 0.9781 | LR: 9.00e-05\n",
            "  Batch 15900/26069 | Loss: 0.9780 | LR: 9.00e-05\n",
            "  Batch 16000/26069 | Loss: 0.9780 | LR: 8.99e-05\n",
            "  Batch 16100/26069 | Loss: 0.9779 | LR: 8.99e-05\n",
            "  Batch 16200/26069 | Loss: 0.9779 | LR: 8.99e-05\n",
            "  Batch 16300/26069 | Loss: 0.9778 | LR: 8.99e-05\n",
            "  Batch 16400/26069 | Loss: 0.9777 | LR: 8.99e-05\n",
            "  Batch 16500/26069 | Loss: 0.9776 | LR: 8.99e-05\n",
            "  Batch 16600/26069 | Loss: 0.9777 | LR: 8.99e-05\n",
            "  Batch 16700/26069 | Loss: 0.9776 | LR: 8.99e-05\n",
            "  Batch 16800/26069 | Loss: 0.9774 | LR: 8.99e-05\n",
            "  Batch 16900/26069 | Loss: 0.9773 | LR: 8.99e-05\n",
            "  Batch 17000/26069 | Loss: 0.9772 | LR: 8.99e-05\n",
            "  Batch 17100/26069 | Loss: 0.9773 | LR: 8.99e-05\n",
            "  Batch 17200/26069 | Loss: 0.9771 | LR: 8.99e-05\n",
            "  Batch 17300/26069 | Loss: 0.9771 | LR: 8.99e-05\n",
            "  Batch 17400/26069 | Loss: 0.9770 | LR: 8.99e-05\n",
            "  Batch 17500/26069 | Loss: 0.9769 | LR: 8.99e-05\n",
            "  Batch 17600/26069 | Loss: 0.9770 | LR: 8.99e-05\n",
            "  Batch 17700/26069 | Loss: 0.9770 | LR: 8.99e-05\n",
            "  Batch 17800/26069 | Loss: 0.9771 | LR: 8.99e-05\n",
            "  Batch 17900/26069 | Loss: 0.9771 | LR: 8.99e-05\n",
            "  Batch 18000/26069 | Loss: 0.9770 | LR: 8.99e-05\n",
            "  Batch 18100/26069 | Loss: 0.9769 | LR: 8.99e-05\n",
            "  Batch 18200/26069 | Loss: 0.9770 | LR: 8.99e-05\n",
            "  Batch 18300/26069 | Loss: 0.9770 | LR: 8.99e-05\n",
            "  Batch 18400/26069 | Loss: 0.9771 | LR: 8.99e-05\n",
            "  Batch 18500/26069 | Loss: 0.9771 | LR: 8.99e-05\n",
            "  Batch 18600/26069 | Loss: 0.9772 | LR: 8.99e-05\n",
            "  Batch 18700/26069 | Loss: 0.9771 | LR: 8.99e-05\n",
            "  Batch 18800/26069 | Loss: 0.9772 | LR: 8.98e-05\n",
            "  Batch 18900/26069 | Loss: 0.9772 | LR: 8.98e-05\n",
            "  Batch 19000/26069 | Loss: 0.9771 | LR: 8.98e-05\n",
            "  Batch 19100/26069 | Loss: 0.9772 | LR: 8.98e-05\n",
            "  Batch 19200/26069 | Loss: 0.9773 | LR: 8.98e-05\n",
            "  Batch 19300/26069 | Loss: 0.9771 | LR: 8.98e-05\n",
            "  Batch 19400/26069 | Loss: 0.9772 | LR: 8.98e-05\n",
            "  Batch 19500/26069 | Loss: 0.9773 | LR: 8.98e-05\n",
            "  Batch 19600/26069 | Loss: 0.9774 | LR: 8.98e-05\n",
            "  Batch 19700/26069 | Loss: 0.9774 | LR: 8.98e-05\n",
            "  Batch 19800/26069 | Loss: 0.9775 | LR: 8.98e-05\n",
            "  Batch 19900/26069 | Loss: 0.9774 | LR: 8.98e-05\n",
            "  Batch 20000/26069 | Loss: 0.9775 | LR: 8.98e-05\n",
            "  Batch 20100/26069 | Loss: 0.9775 | LR: 8.98e-05\n",
            "  Batch 20200/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 20300/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 20400/26069 | Loss: 0.9777 | LR: 8.98e-05\n",
            "  Batch 20500/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 20600/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 20700/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 20800/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 20900/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 21000/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 21100/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 21200/26069 | Loss: 0.9776 | LR: 8.98e-05\n",
            "  Batch 21300/26069 | Loss: 0.9777 | LR: 8.98e-05\n",
            "  Batch 21400/26069 | Loss: 0.9777 | LR: 8.98e-05\n",
            "  Batch 21500/26069 | Loss: 0.9776 | LR: 8.97e-05\n",
            "  Batch 21600/26069 | Loss: 0.9777 | LR: 8.97e-05\n",
            "  Batch 21700/26069 | Loss: 0.9777 | LR: 8.97e-05\n",
            "  Batch 21800/26069 | Loss: 0.9777 | LR: 8.97e-05\n",
            "  Batch 21900/26069 | Loss: 0.9777 | LR: 8.97e-05\n",
            "  Batch 22000/26069 | Loss: 0.9776 | LR: 8.97e-05\n",
            "  Batch 22100/26069 | Loss: 0.9775 | LR: 8.97e-05\n",
            "  Batch 22200/26069 | Loss: 0.9775 | LR: 8.97e-05\n",
            "  Batch 22300/26069 | Loss: 0.9775 | LR: 8.97e-05\n",
            "  Batch 22400/26069 | Loss: 0.9776 | LR: 8.97e-05\n",
            "  Batch 22500/26069 | Loss: 0.9776 | LR: 8.97e-05\n",
            "  Batch 22600/26069 | Loss: 0.9776 | LR: 8.97e-05\n",
            "  Batch 22700/26069 | Loss: 0.9776 | LR: 8.97e-05\n",
            "  Batch 22800/26069 | Loss: 0.9777 | LR: 8.97e-05\n",
            "  Batch 22900/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23000/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23100/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23200/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23300/26069 | Loss: 0.9779 | LR: 8.97e-05\n",
            "  Batch 23400/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23500/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23600/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23700/26069 | Loss: 0.9778 | LR: 8.97e-05\n",
            "  Batch 23800/26069 | Loss: 0.9779 | LR: 8.97e-05\n",
            "  Batch 23900/26069 | Loss: 0.9779 | LR: 8.97e-05\n",
            "  Batch 24000/26069 | Loss: 0.9780 | LR: 8.97e-05\n",
            "  Batch 24100/26069 | Loss: 0.9780 | LR: 8.97e-05\n",
            "  Batch 24200/26069 | Loss: 0.9780 | LR: 8.97e-05\n",
            "  Batch 24300/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 24400/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 24500/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 24600/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 24700/26069 | Loss: 0.9779 | LR: 8.96e-05\n",
            "  Batch 24800/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 24900/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 25000/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 25100/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 25200/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 25300/26069 | Loss: 0.9781 | LR: 8.96e-05\n",
            "  Batch 25400/26069 | Loss: 0.9781 | LR: 8.96e-05\n",
            "  Batch 25500/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 25600/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 25700/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "  Batch 25800/26069 | Loss: 0.9779 | LR: 8.96e-05\n",
            "  Batch 25900/26069 | Loss: 0.9779 | LR: 8.96e-05\n",
            "  Batch 26000/26069 | Loss: 0.9780 | LR: 8.96e-05\n",
            "\n",
            "  Train Loss: 0.9780\n",
            "  Val Loss:   0.9567 \n",
            "  Time:       312.7s\n",
            "\n",
            "Epoch 22/100\n",
            "  Batch 100/26069 | Loss: 0.9696 | LR: 8.96e-05\n",
            "  Batch 200/26069 | Loss: 0.9720 | LR: 8.96e-05\n",
            "  Batch 300/26069 | Loss: 0.9740 | LR: 8.96e-05\n",
            "  Batch 400/26069 | Loss: 0.9800 | LR: 8.96e-05\n",
            "  Batch 500/26069 | Loss: 0.9803 | LR: 8.96e-05\n",
            "  Batch 600/26069 | Loss: 0.9799 | LR: 8.96e-05\n",
            "  Batch 700/26069 | Loss: 0.9779 | LR: 8.96e-05\n",
            "  Batch 800/26069 | Loss: 0.9795 | LR: 8.96e-05\n",
            "  Batch 900/26069 | Loss: 0.9807 | LR: 8.95e-05\n",
            "  Batch 1000/26069 | Loss: 0.9827 | LR: 8.95e-05\n",
            "  Batch 1100/26069 | Loss: 0.9804 | LR: 8.95e-05\n",
            "  Batch 1200/26069 | Loss: 0.9804 | LR: 8.95e-05\n",
            "  Batch 1300/26069 | Loss: 0.9796 | LR: 8.95e-05\n",
            "  Batch 1400/26069 | Loss: 0.9797 | LR: 8.95e-05\n",
            "  Batch 1500/26069 | Loss: 0.9822 | LR: 8.95e-05\n",
            "  Batch 1600/26069 | Loss: 0.9798 | LR: 8.95e-05\n",
            "  Batch 1700/26069 | Loss: 0.9797 | LR: 8.95e-05\n",
            "  Batch 1800/26069 | Loss: 0.9792 | LR: 8.95e-05\n",
            "  Batch 1900/26069 | Loss: 0.9791 | LR: 8.95e-05\n",
            "  Batch 2000/26069 | Loss: 0.9789 | LR: 8.95e-05\n",
            "  Batch 2100/26069 | Loss: 0.9789 | LR: 8.95e-05\n",
            "  Batch 2200/26069 | Loss: 0.9787 | LR: 8.95e-05\n",
            "  Batch 2300/26069 | Loss: 0.9791 | LR: 8.95e-05\n",
            "  Batch 2400/26069 | Loss: 0.9789 | LR: 8.95e-05\n",
            "  Batch 2500/26069 | Loss: 0.9784 | LR: 8.95e-05\n",
            "  Batch 2600/26069 | Loss: 0.9780 | LR: 8.95e-05\n",
            "  Batch 2700/26069 | Loss: 0.9786 | LR: 8.95e-05\n",
            "  Batch 2800/26069 | Loss: 0.9788 | LR: 8.95e-05\n",
            "  Batch 2900/26069 | Loss: 0.9786 | LR: 8.95e-05\n",
            "  Batch 3000/26069 | Loss: 0.9786 | LR: 8.95e-05\n",
            "  Batch 3100/26069 | Loss: 0.9782 | LR: 8.95e-05\n",
            "  Batch 3200/26069 | Loss: 0.9782 | LR: 8.95e-05\n",
            "  Batch 3300/26069 | Loss: 0.9784 | LR: 8.95e-05\n",
            "  Batch 3400/26069 | Loss: 0.9785 | LR: 8.95e-05\n",
            "  Batch 3500/26069 | Loss: 0.9785 | LR: 8.95e-05\n",
            "  Batch 3600/26069 | Loss: 0.9786 | LR: 8.94e-05\n",
            "  Batch 3700/26069 | Loss: 0.9788 | LR: 8.94e-05\n",
            "  Batch 3800/26069 | Loss: 0.9787 | LR: 8.94e-05\n",
            "  Batch 3900/26069 | Loss: 0.9785 | LR: 8.94e-05\n",
            "  Batch 4000/26069 | Loss: 0.9781 | LR: 8.94e-05\n",
            "  Batch 4100/26069 | Loss: 0.9784 | LR: 8.94e-05\n",
            "  Batch 4200/26069 | Loss: 0.9787 | LR: 8.94e-05\n",
            "  Batch 4300/26069 | Loss: 0.9785 | LR: 8.94e-05\n",
            "  Batch 4400/26069 | Loss: 0.9780 | LR: 8.94e-05\n",
            "  Batch 4500/26069 | Loss: 0.9781 | LR: 8.94e-05\n",
            "  Batch 4600/26069 | Loss: 0.9776 | LR: 8.94e-05\n",
            "  Batch 4700/26069 | Loss: 0.9778 | LR: 8.94e-05\n",
            "  Batch 4800/26069 | Loss: 0.9779 | LR: 8.94e-05\n",
            "  Batch 4900/26069 | Loss: 0.9780 | LR: 8.94e-05\n",
            "  Batch 5000/26069 | Loss: 0.9778 | LR: 8.94e-05\n",
            "  Batch 5100/26069 | Loss: 0.9781 | LR: 8.94e-05\n",
            "  Batch 5200/26069 | Loss: 0.9782 | LR: 8.94e-05\n",
            "  Batch 5300/26069 | Loss: 0.9781 | LR: 8.94e-05\n",
            "  Batch 5400/26069 | Loss: 0.9782 | LR: 8.94e-05\n",
            "  Batch 5500/26069 | Loss: 0.9780 | LR: 8.94e-05\n",
            "  Batch 5600/26069 | Loss: 0.9781 | LR: 8.94e-05\n",
            "  Batch 5700/26069 | Loss: 0.9779 | LR: 8.94e-05\n",
            "  Batch 5800/26069 | Loss: 0.9780 | LR: 8.94e-05\n",
            "  Batch 5900/26069 | Loss: 0.9778 | LR: 8.94e-05\n",
            "  Batch 6000/26069 | Loss: 0.9778 | LR: 8.94e-05\n",
            "  Batch 6100/26069 | Loss: 0.9772 | LR: 8.94e-05\n",
            "  Batch 6200/26069 | Loss: 0.9776 | LR: 8.94e-05\n",
            "  Batch 6300/26069 | Loss: 0.9777 | LR: 8.93e-05\n",
            "  Batch 6400/26069 | Loss: 0.9776 | LR: 8.93e-05\n",
            "  Batch 6500/26069 | Loss: 0.9775 | LR: 8.93e-05\n",
            "  Batch 6600/26069 | Loss: 0.9775 | LR: 8.93e-05\n",
            "  Batch 6700/26069 | Loss: 0.9772 | LR: 8.93e-05\n",
            "  Batch 6800/26069 | Loss: 0.9771 | LR: 8.93e-05\n",
            "  Batch 6900/26069 | Loss: 0.9772 | LR: 8.93e-05\n",
            "  Batch 7000/26069 | Loss: 0.9772 | LR: 8.93e-05\n",
            "  Batch 7100/26069 | Loss: 0.9767 | LR: 8.93e-05\n",
            "  Batch 7200/26069 | Loss: 0.9765 | LR: 8.93e-05\n",
            "  Batch 7300/26069 | Loss: 0.9763 | LR: 8.93e-05\n",
            "  Batch 7400/26069 | Loss: 0.9763 | LR: 8.93e-05\n",
            "  Batch 7500/26069 | Loss: 0.9763 | LR: 8.93e-05\n",
            "  Batch 7600/26069 | Loss: 0.9764 | LR: 8.93e-05\n",
            "  Batch 7700/26069 | Loss: 0.9765 | LR: 8.93e-05\n",
            "  Batch 7800/26069 | Loss: 0.9766 | LR: 8.93e-05\n",
            "  Batch 7900/26069 | Loss: 0.9766 | LR: 8.93e-05\n",
            "  Batch 8000/26069 | Loss: 0.9769 | LR: 8.93e-05\n",
            "  Batch 8100/26069 | Loss: 0.9767 | LR: 8.93e-05\n",
            "  Batch 8200/26069 | Loss: 0.9768 | LR: 8.93e-05\n",
            "  Batch 8300/26069 | Loss: 0.9769 | LR: 8.93e-05\n",
            "  Batch 8400/26069 | Loss: 0.9768 | LR: 8.93e-05\n",
            "  Batch 8500/26069 | Loss: 0.9767 | LR: 8.93e-05\n",
            "  Batch 8600/26069 | Loss: 0.9767 | LR: 8.93e-05\n",
            "  Batch 8700/26069 | Loss: 0.9769 | LR: 8.93e-05\n",
            "  Batch 8800/26069 | Loss: 0.9772 | LR: 8.93e-05\n",
            "  Batch 8900/26069 | Loss: 0.9774 | LR: 8.93e-05\n",
            "  Batch 9000/26069 | Loss: 0.9775 | LR: 8.92e-05\n",
            "  Batch 9100/26069 | Loss: 0.9776 | LR: 8.92e-05\n",
            "  Batch 9200/26069 | Loss: 0.9776 | LR: 8.92e-05\n",
            "  Batch 9300/26069 | Loss: 0.9774 | LR: 8.92e-05\n",
            "  Batch 9400/26069 | Loss: 0.9773 | LR: 8.92e-05\n",
            "  Batch 9500/26069 | Loss: 0.9774 | LR: 8.92e-05\n",
            "  Batch 9600/26069 | Loss: 0.9774 | LR: 8.92e-05\n",
            "  Batch 9700/26069 | Loss: 0.9774 | LR: 8.92e-05\n",
            "  Batch 9800/26069 | Loss: 0.9773 | LR: 8.92e-05\n",
            "  Batch 9900/26069 | Loss: 0.9774 | LR: 8.92e-05\n",
            "  Batch 10000/26069 | Loss: 0.9773 | LR: 8.92e-05\n",
            "  Batch 10100/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 10200/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 10300/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 10400/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 10500/26069 | Loss: 0.9770 | LR: 8.92e-05\n",
            "  Batch 10600/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 10700/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 10800/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 10900/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 11000/26069 | Loss: 0.9771 | LR: 8.92e-05\n",
            "  Batch 11100/26069 | Loss: 0.9773 | LR: 8.92e-05\n",
            "  Batch 11200/26069 | Loss: 0.9772 | LR: 8.92e-05\n",
            "  Batch 11300/26069 | Loss: 0.9769 | LR: 8.92e-05\n",
            "  Batch 11400/26069 | Loss: 0.9769 | LR: 8.92e-05\n",
            "  Batch 11500/26069 | Loss: 0.9770 | LR: 8.92e-05\n",
            "  Batch 11600/26069 | Loss: 0.9770 | LR: 8.92e-05\n",
            "  Batch 11700/26069 | Loss: 0.9770 | LR: 8.91e-05\n",
            "  Batch 11800/26069 | Loss: 0.9768 | LR: 8.91e-05\n",
            "  Batch 11900/26069 | Loss: 0.9769 | LR: 8.91e-05\n",
            "  Batch 12000/26069 | Loss: 0.9769 | LR: 8.91e-05\n",
            "  Batch 12100/26069 | Loss: 0.9769 | LR: 8.91e-05\n",
            "  Batch 12200/26069 | Loss: 0.9768 | LR: 8.91e-05\n",
            "  Batch 12300/26069 | Loss: 0.9768 | LR: 8.91e-05\n",
            "  Batch 12400/26069 | Loss: 0.9768 | LR: 8.91e-05\n",
            "  Batch 12500/26069 | Loss: 0.9768 | LR: 8.91e-05\n",
            "  Batch 12600/26069 | Loss: 0.9768 | LR: 8.91e-05\n",
            "  Batch 12700/26069 | Loss: 0.9766 | LR: 8.91e-05\n",
            "  Batch 12800/26069 | Loss: 0.9766 | LR: 8.91e-05\n",
            "  Batch 12900/26069 | Loss: 0.9766 | LR: 8.91e-05\n",
            "  Batch 13000/26069 | Loss: 0.9768 | LR: 8.91e-05\n",
            "  Batch 13100/26069 | Loss: 0.9769 | LR: 8.91e-05\n",
            "  Batch 13200/26069 | Loss: 0.9770 | LR: 8.91e-05\n",
            "  Batch 13300/26069 | Loss: 0.9769 | LR: 8.91e-05\n",
            "  Batch 13400/26069 | Loss: 0.9770 | LR: 8.91e-05\n",
            "  Batch 13500/26069 | Loss: 0.9771 | LR: 8.91e-05\n",
            "  Batch 13600/26069 | Loss: 0.9772 | LR: 8.91e-05\n",
            "  Batch 13700/26069 | Loss: 0.9772 | LR: 8.91e-05\n",
            "  Batch 13800/26069 | Loss: 0.9774 | LR: 8.91e-05\n",
            "  Batch 13900/26069 | Loss: 0.9774 | LR: 8.91e-05\n",
            "  Batch 14000/26069 | Loss: 0.9775 | LR: 8.91e-05\n",
            "  Batch 14100/26069 | Loss: 0.9774 | LR: 8.91e-05\n",
            "  Batch 14200/26069 | Loss: 0.9774 | LR: 8.91e-05\n",
            "  Batch 14300/26069 | Loss: 0.9774 | LR: 8.90e-05\n",
            "  Batch 14400/26069 | Loss: 0.9773 | LR: 8.90e-05\n",
            "  Batch 14500/26069 | Loss: 0.9772 | LR: 8.90e-05\n",
            "  Batch 14600/26069 | Loss: 0.9772 | LR: 8.90e-05\n",
            "  Batch 14700/26069 | Loss: 0.9772 | LR: 8.90e-05\n",
            "  Batch 14800/26069 | Loss: 0.9773 | LR: 8.90e-05\n",
            "  Batch 14900/26069 | Loss: 0.9775 | LR: 8.90e-05\n",
            "  Batch 15000/26069 | Loss: 0.9774 | LR: 8.90e-05\n",
            "  Batch 15100/26069 | Loss: 0.9773 | LR: 8.90e-05\n",
            "  Batch 15200/26069 | Loss: 0.9772 | LR: 8.90e-05\n",
            "  Batch 15300/26069 | Loss: 0.9772 | LR: 8.90e-05\n",
            "  Batch 15400/26069 | Loss: 0.9773 | LR: 8.90e-05\n",
            "  Batch 15500/26069 | Loss: 0.9773 | LR: 8.90e-05\n",
            "  Batch 15600/26069 | Loss: 0.9772 | LR: 8.90e-05\n",
            "  Batch 15700/26069 | Loss: 0.9771 | LR: 8.90e-05\n",
            "  Batch 15800/26069 | Loss: 0.9772 | LR: 8.90e-05\n",
            "  Batch 15900/26069 | Loss: 0.9773 | LR: 8.90e-05\n",
            "  Batch 16000/26069 | Loss: 0.9774 | LR: 8.90e-05\n",
            "  Batch 16100/26069 | Loss: 0.9773 | LR: 8.90e-05\n",
            "  Batch 16200/26069 | Loss: 0.9771 | LR: 8.90e-05\n",
            "  Batch 16300/26069 | Loss: 0.9770 | LR: 8.90e-05\n",
            "  Batch 16400/26069 | Loss: 0.9771 | LR: 8.90e-05\n",
            "  Batch 16500/26069 | Loss: 0.9770 | LR: 8.90e-05\n",
            "  Batch 16600/26069 | Loss: 0.9770 | LR: 8.90e-05\n",
            "  Batch 16700/26069 | Loss: 0.9771 | LR: 8.90e-05\n",
            "  Batch 16800/26069 | Loss: 0.9770 | LR: 8.90e-05\n",
            "  Batch 16900/26069 | Loss: 0.9769 | LR: 8.90e-05\n",
            "  Batch 17000/26069 | Loss: 0.9770 | LR: 8.89e-05\n",
            "  Batch 17100/26069 | Loss: 0.9771 | LR: 8.89e-05\n",
            "  Batch 17200/26069 | Loss: 0.9770 | LR: 8.89e-05\n",
            "  Batch 17300/26069 | Loss: 0.9770 | LR: 8.89e-05\n",
            "  Batch 17400/26069 | Loss: 0.9770 | LR: 8.89e-05\n",
            "  Batch 17500/26069 | Loss: 0.9771 | LR: 8.89e-05\n",
            "  Batch 17600/26069 | Loss: 0.9771 | LR: 8.89e-05\n",
            "  Batch 17700/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 17800/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 17900/26069 | Loss: 0.9773 | LR: 8.89e-05\n",
            "  Batch 18000/26069 | Loss: 0.9774 | LR: 8.89e-05\n",
            "  Batch 18100/26069 | Loss: 0.9774 | LR: 8.89e-05\n",
            "  Batch 18200/26069 | Loss: 0.9774 | LR: 8.89e-05\n",
            "  Batch 18300/26069 | Loss: 0.9773 | LR: 8.89e-05\n",
            "  Batch 18400/26069 | Loss: 0.9773 | LR: 8.89e-05\n",
            "  Batch 18500/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 18600/26069 | Loss: 0.9773 | LR: 8.89e-05\n",
            "  Batch 18700/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 18800/26069 | Loss: 0.9771 | LR: 8.89e-05\n",
            "  Batch 18900/26069 | Loss: 0.9771 | LR: 8.89e-05\n",
            "  Batch 19000/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 19100/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 19200/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 19300/26069 | Loss: 0.9772 | LR: 8.89e-05\n",
            "  Batch 19400/26069 | Loss: 0.9771 | LR: 8.89e-05\n",
            "  Batch 19500/26069 | Loss: 0.9771 | LR: 8.89e-05\n",
            "  Batch 19600/26069 | Loss: 0.9771 | LR: 8.88e-05\n",
            "  Batch 19700/26069 | Loss: 0.9771 | LR: 8.88e-05\n",
            "  Batch 19800/26069 | Loss: 0.9771 | LR: 8.88e-05\n",
            "  Batch 19900/26069 | Loss: 0.9771 | LR: 8.88e-05\n",
            "  Batch 20000/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 20100/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 20200/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 20300/26069 | Loss: 0.9775 | LR: 8.88e-05\n",
            "  Batch 20400/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 20500/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 20600/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 20700/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 20800/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 20900/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 21000/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 21100/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 21200/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 21300/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 21400/26069 | Loss: 0.9773 | LR: 8.88e-05\n",
            "  Batch 21500/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 21600/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 21700/26069 | Loss: 0.9774 | LR: 8.88e-05\n",
            "  Batch 21800/26069 | Loss: 0.9776 | LR: 8.88e-05\n",
            "  Batch 21900/26069 | Loss: 0.9777 | LR: 8.88e-05\n",
            "  Batch 22000/26069 | Loss: 0.9777 | LR: 8.88e-05\n",
            "  Batch 22100/26069 | Loss: 0.9777 | LR: 8.88e-05\n",
            "  Batch 22200/26069 | Loss: 0.9776 | LR: 8.88e-05\n",
            "  Batch 22300/26069 | Loss: 0.9777 | LR: 8.87e-05\n",
            "  Batch 22400/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 22500/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 22600/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 22700/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 22800/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 22900/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 23000/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23100/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 23200/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23300/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23400/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23500/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23600/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23700/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23800/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 23900/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 24000/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 24100/26069 | Loss: 0.9779 | LR: 8.87e-05\n",
            "  Batch 24200/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 24300/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 24400/26069 | Loss: 0.9777 | LR: 8.87e-05\n",
            "  Batch 24500/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 24600/26069 | Loss: 0.9778 | LR: 8.87e-05\n",
            "  Batch 24700/26069 | Loss: 0.9777 | LR: 8.87e-05\n",
            "  Batch 24800/26069 | Loss: 0.9777 | LR: 8.87e-05\n",
            "  Batch 24900/26069 | Loss: 0.9778 | LR: 8.86e-05\n",
            "  Batch 25000/26069 | Loss: 0.9778 | LR: 8.86e-05\n",
            "  Batch 25100/26069 | Loss: 0.9778 | LR: 8.86e-05\n",
            "  Batch 25200/26069 | Loss: 0.9778 | LR: 8.86e-05\n",
            "  Batch 25300/26069 | Loss: 0.9777 | LR: 8.86e-05\n",
            "  Batch 25400/26069 | Loss: 0.9777 | LR: 8.86e-05\n",
            "  Batch 25500/26069 | Loss: 0.9776 | LR: 8.86e-05\n",
            "  Batch 25600/26069 | Loss: 0.9776 | LR: 8.86e-05\n",
            "  Batch 25700/26069 | Loss: 0.9776 | LR: 8.86e-05\n",
            "  Batch 25800/26069 | Loss: 0.9775 | LR: 8.86e-05\n",
            "  Batch 25900/26069 | Loss: 0.9774 | LR: 8.86e-05\n",
            "  Batch 26000/26069 | Loss: 0.9774 | LR: 8.86e-05\n",
            "\n",
            "  Train Loss: 0.9774\n",
            "  Val Loss:   0.9572 \n",
            "  Time:       313.4s\n",
            "\n",
            "Epoch 23/100\n",
            "  Batch 100/26069 | Loss: 0.9831 | LR: 8.86e-05\n",
            "  Batch 200/26069 | Loss: 0.9750 | LR: 8.86e-05\n",
            "  Batch 300/26069 | Loss: 0.9809 | LR: 8.86e-05\n",
            "  Batch 400/26069 | Loss: 0.9806 | LR: 8.86e-05\n",
            "  Batch 500/26069 | Loss: 0.9833 | LR: 8.86e-05\n",
            "  Batch 600/26069 | Loss: 0.9812 | LR: 8.86e-05\n",
            "  Batch 700/26069 | Loss: 0.9812 | LR: 8.86e-05\n",
            "  Batch 800/26069 | Loss: 0.9787 | LR: 8.86e-05\n",
            "  Batch 900/26069 | Loss: 0.9790 | LR: 8.86e-05\n",
            "  Batch 1000/26069 | Loss: 0.9779 | LR: 8.86e-05\n",
            "  Batch 1100/26069 | Loss: 0.9789 | LR: 8.86e-05\n",
            "  Batch 1200/26069 | Loss: 0.9797 | LR: 8.86e-05\n",
            "  Batch 1300/26069 | Loss: 0.9809 | LR: 8.86e-05\n",
            "  Batch 1400/26069 | Loss: 0.9794 | LR: 8.85e-05\n",
            "  Batch 1500/26069 | Loss: 0.9789 | LR: 8.85e-05\n",
            "  Batch 1600/26069 | Loss: 0.9772 | LR: 8.85e-05\n",
            "  Batch 1700/26069 | Loss: 0.9768 | LR: 8.85e-05\n",
            "  Batch 1800/26069 | Loss: 0.9772 | LR: 8.85e-05\n",
            "  Batch 1900/26069 | Loss: 0.9768 | LR: 8.85e-05\n",
            "  Batch 2000/26069 | Loss: 0.9769 | LR: 8.85e-05\n",
            "  Batch 2100/26069 | Loss: 0.9782 | LR: 8.85e-05\n",
            "  Batch 2200/26069 | Loss: 0.9785 | LR: 8.85e-05\n",
            "  Batch 2300/26069 | Loss: 0.9789 | LR: 8.85e-05\n",
            "  Batch 2400/26069 | Loss: 0.9781 | LR: 8.85e-05\n",
            "  Batch 2500/26069 | Loss: 0.9774 | LR: 8.85e-05\n",
            "  Batch 2600/26069 | Loss: 0.9776 | LR: 8.85e-05\n",
            "  Batch 2700/26069 | Loss: 0.9773 | LR: 8.85e-05\n",
            "  Batch 2800/26069 | Loss: 0.9785 | LR: 8.85e-05\n",
            "  Batch 2900/26069 | Loss: 0.9780 | LR: 8.85e-05\n",
            "  Batch 3000/26069 | Loss: 0.9778 | LR: 8.85e-05\n",
            "  Batch 3100/26069 | Loss: 0.9783 | LR: 8.85e-05\n",
            "  Batch 3200/26069 | Loss: 0.9788 | LR: 8.85e-05\n",
            "  Batch 3300/26069 | Loss: 0.9780 | LR: 8.85e-05\n",
            "  Batch 3400/26069 | Loss: 0.9780 | LR: 8.85e-05\n",
            "  Batch 3500/26069 | Loss: 0.9772 | LR: 8.85e-05\n",
            "  Batch 3600/26069 | Loss: 0.9768 | LR: 8.85e-05\n",
            "  Batch 3700/26069 | Loss: 0.9767 | LR: 8.85e-05\n",
            "  Batch 3800/26069 | Loss: 0.9770 | LR: 8.85e-05\n",
            "  Batch 3900/26069 | Loss: 0.9766 | LR: 8.85e-05\n",
            "  Batch 4000/26069 | Loss: 0.9762 | LR: 8.84e-05\n",
            "  Batch 4100/26069 | Loss: 0.9759 | LR: 8.84e-05\n",
            "  Batch 4200/26069 | Loss: 0.9765 | LR: 8.84e-05\n",
            "  Batch 4300/26069 | Loss: 0.9767 | LR: 8.84e-05\n",
            "  Batch 4400/26069 | Loss: 0.9769 | LR: 8.84e-05\n",
            "  Batch 4500/26069 | Loss: 0.9768 | LR: 8.84e-05\n",
            "  Batch 4600/26069 | Loss: 0.9768 | LR: 8.84e-05\n",
            "  Batch 4700/26069 | Loss: 0.9769 | LR: 8.84e-05\n",
            "  Batch 4800/26069 | Loss: 0.9771 | LR: 8.84e-05\n",
            "  Batch 4900/26069 | Loss: 0.9772 | LR: 8.84e-05\n",
            "  Batch 5000/26069 | Loss: 0.9770 | LR: 8.84e-05\n",
            "  Batch 5100/26069 | Loss: 0.9772 | LR: 8.84e-05\n",
            "  Batch 5200/26069 | Loss: 0.9775 | LR: 8.84e-05\n",
            "  Batch 5300/26069 | Loss: 0.9775 | LR: 8.84e-05\n",
            "  Batch 5400/26069 | Loss: 0.9772 | LR: 8.84e-05\n",
            "  Batch 5500/26069 | Loss: 0.9766 | LR: 8.84e-05\n",
            "  Batch 5600/26069 | Loss: 0.9768 | LR: 8.84e-05\n",
            "  Batch 5700/26069 | Loss: 0.9770 | LR: 8.84e-05\n",
            "  Batch 5800/26069 | Loss: 0.9771 | LR: 8.84e-05\n",
            "  Batch 5900/26069 | Loss: 0.9769 | LR: 8.84e-05\n",
            "  Batch 6000/26069 | Loss: 0.9768 | LR: 8.84e-05\n",
            "  Batch 6100/26069 | Loss: 0.9769 | LR: 8.84e-05\n",
            "  Batch 6200/26069 | Loss: 0.9767 | LR: 8.84e-05\n",
            "  Batch 6300/26069 | Loss: 0.9769 | LR: 8.84e-05\n",
            "  Batch 6400/26069 | Loss: 0.9768 | LR: 8.84e-05\n",
            "  Batch 6500/26069 | Loss: 0.9769 | LR: 8.84e-05\n",
            "  Batch 6600/26069 | Loss: 0.9771 | LR: 8.83e-05\n",
            "  Batch 6700/26069 | Loss: 0.9771 | LR: 8.83e-05\n",
            "  Batch 6800/26069 | Loss: 0.9773 | LR: 8.83e-05\n",
            "  Batch 6900/26069 | Loss: 0.9775 | LR: 8.83e-05\n",
            "  Batch 7000/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 7100/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 7200/26069 | Loss: 0.9775 | LR: 8.83e-05\n",
            "  Batch 7300/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 7400/26069 | Loss: 0.9775 | LR: 8.83e-05\n",
            "  Batch 7500/26069 | Loss: 0.9775 | LR: 8.83e-05\n",
            "  Batch 7600/26069 | Loss: 0.9773 | LR: 8.83e-05\n",
            "  Batch 7700/26069 | Loss: 0.9775 | LR: 8.83e-05\n",
            "  Batch 7800/26069 | Loss: 0.9775 | LR: 8.83e-05\n",
            "  Batch 7900/26069 | Loss: 0.9776 | LR: 8.83e-05\n",
            "  Batch 8000/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 8100/26069 | Loss: 0.9776 | LR: 8.83e-05\n",
            "  Batch 8200/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 8300/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 8400/26069 | Loss: 0.9771 | LR: 8.83e-05\n",
            "  Batch 8500/26069 | Loss: 0.9771 | LR: 8.83e-05\n",
            "  Batch 8600/26069 | Loss: 0.9773 | LR: 8.83e-05\n",
            "  Batch 8700/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 8800/26069 | Loss: 0.9774 | LR: 8.83e-05\n",
            "  Batch 8900/26069 | Loss: 0.9773 | LR: 8.83e-05\n",
            "  Batch 9000/26069 | Loss: 0.9770 | LR: 8.83e-05\n",
            "  Batch 9100/26069 | Loss: 0.9769 | LR: 8.83e-05\n",
            "  Batch 9200/26069 | Loss: 0.9768 | LR: 8.82e-05\n",
            "  Batch 9300/26069 | Loss: 0.9770 | LR: 8.82e-05\n",
            "  Batch 9400/26069 | Loss: 0.9771 | LR: 8.82e-05\n",
            "  Batch 9500/26069 | Loss: 0.9771 | LR: 8.82e-05\n",
            "  Batch 9600/26069 | Loss: 0.9772 | LR: 8.82e-05\n",
            "  Batch 9700/26069 | Loss: 0.9772 | LR: 8.82e-05\n",
            "  Batch 9800/26069 | Loss: 0.9773 | LR: 8.82e-05\n",
            "  Batch 9900/26069 | Loss: 0.9775 | LR: 8.82e-05\n",
            "  Batch 10000/26069 | Loss: 0.9776 | LR: 8.82e-05\n",
            "  Batch 10100/26069 | Loss: 0.9774 | LR: 8.82e-05\n",
            "  Batch 10200/26069 | Loss: 0.9774 | LR: 8.82e-05\n",
            "  Batch 10300/26069 | Loss: 0.9774 | LR: 8.82e-05\n",
            "  Batch 10400/26069 | Loss: 0.9772 | LR: 8.82e-05\n",
            "  Batch 10500/26069 | Loss: 0.9773 | LR: 8.82e-05\n",
            "  Batch 10600/26069 | Loss: 0.9772 | LR: 8.82e-05\n",
            "  Batch 10700/26069 | Loss: 0.9773 | LR: 8.82e-05\n",
            "  Batch 10800/26069 | Loss: 0.9773 | LR: 8.82e-05\n",
            "  Batch 10900/26069 | Loss: 0.9771 | LR: 8.82e-05\n",
            "  Batch 11000/26069 | Loss: 0.9769 | LR: 8.82e-05\n",
            "  Batch 11100/26069 | Loss: 0.9768 | LR: 8.82e-05\n",
            "  Batch 11200/26069 | Loss: 0.9769 | LR: 8.82e-05\n",
            "  Batch 11300/26069 | Loss: 0.9770 | LR: 8.82e-05\n",
            "  Batch 11400/26069 | Loss: 0.9771 | LR: 8.82e-05\n",
            "  Batch 11500/26069 | Loss: 0.9770 | LR: 8.82e-05\n",
            "  Batch 11600/26069 | Loss: 0.9771 | LR: 8.82e-05\n",
            "  Batch 11700/26069 | Loss: 0.9770 | LR: 8.82e-05\n",
            "  Batch 11800/26069 | Loss: 0.9767 | LR: 8.81e-05\n",
            "  Batch 11900/26069 | Loss: 0.9767 | LR: 8.81e-05\n",
            "  Batch 12000/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 12100/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 12200/26069 | Loss: 0.9772 | LR: 8.81e-05\n",
            "  Batch 12300/26069 | Loss: 0.9773 | LR: 8.81e-05\n",
            "  Batch 12400/26069 | Loss: 0.9773 | LR: 8.81e-05\n",
            "  Batch 12500/26069 | Loss: 0.9772 | LR: 8.81e-05\n",
            "  Batch 12600/26069 | Loss: 0.9770 | LR: 8.81e-05\n",
            "  Batch 12700/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 12800/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 12900/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 13000/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 13100/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 13200/26069 | Loss: 0.9769 | LR: 8.81e-05\n",
            "  Batch 13300/26069 | Loss: 0.9770 | LR: 8.81e-05\n",
            "  Batch 13400/26069 | Loss: 0.9770 | LR: 8.81e-05\n",
            "  Batch 13500/26069 | Loss: 0.9771 | LR: 8.81e-05\n",
            "  Batch 13600/26069 | Loss: 0.9771 | LR: 8.81e-05\n",
            "  Batch 13700/26069 | Loss: 0.9771 | LR: 8.81e-05\n",
            "  Batch 13800/26069 | Loss: 0.9770 | LR: 8.81e-05\n",
            "  Batch 13900/26069 | Loss: 0.9771 | LR: 8.81e-05\n",
            "  Batch 14000/26069 | Loss: 0.9771 | LR: 8.81e-05\n",
            "  Batch 14100/26069 | Loss: 0.9768 | LR: 8.81e-05\n",
            "  Batch 14200/26069 | Loss: 0.9768 | LR: 8.81e-05\n",
            "  Batch 14300/26069 | Loss: 0.9768 | LR: 8.80e-05\n",
            "  Batch 14400/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 14500/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 14600/26069 | Loss: 0.9765 | LR: 8.80e-05\n",
            "  Batch 14700/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 14800/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 14900/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 15000/26069 | Loss: 0.9768 | LR: 8.80e-05\n",
            "  Batch 15100/26069 | Loss: 0.9769 | LR: 8.80e-05\n",
            "  Batch 15200/26069 | Loss: 0.9768 | LR: 8.80e-05\n",
            "  Batch 15300/26069 | Loss: 0.9767 | LR: 8.80e-05\n",
            "  Batch 15400/26069 | Loss: 0.9768 | LR: 8.80e-05\n",
            "  Batch 15500/26069 | Loss: 0.9767 | LR: 8.80e-05\n",
            "  Batch 15600/26069 | Loss: 0.9767 | LR: 8.80e-05\n",
            "  Batch 15700/26069 | Loss: 0.9767 | LR: 8.80e-05\n",
            "  Batch 15800/26069 | Loss: 0.9768 | LR: 8.80e-05\n",
            "  Batch 15900/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 16000/26069 | Loss: 0.9767 | LR: 8.80e-05\n",
            "  Batch 16100/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 16200/26069 | Loss: 0.9765 | LR: 8.80e-05\n",
            "  Batch 16300/26069 | Loss: 0.9765 | LR: 8.80e-05\n",
            "  Batch 16400/26069 | Loss: 0.9766 | LR: 8.80e-05\n",
            "  Batch 16500/26069 | Loss: 0.9768 | LR: 8.80e-05\n",
            "  Batch 16600/26069 | Loss: 0.9768 | LR: 8.80e-05\n",
            "  Batch 16700/26069 | Loss: 0.9767 | LR: 8.80e-05\n",
            "  Batch 16800/26069 | Loss: 0.9767 | LR: 8.80e-05\n",
            "  Batch 16900/26069 | Loss: 0.9768 | LR: 8.79e-05\n",
            "  Batch 17000/26069 | Loss: 0.9769 | LR: 8.79e-05\n",
            "  Batch 17100/26069 | Loss: 0.9767 | LR: 8.79e-05\n",
            "  Batch 17200/26069 | Loss: 0.9767 | LR: 8.79e-05\n",
            "  Batch 17300/26069 | Loss: 0.9768 | LR: 8.79e-05\n",
            "  Batch 17400/26069 | Loss: 0.9768 | LR: 8.79e-05\n",
            "  Batch 17500/26069 | Loss: 0.9768 | LR: 8.79e-05\n",
            "  Batch 17600/26069 | Loss: 0.9768 | LR: 8.79e-05\n",
            "  Batch 17700/26069 | Loss: 0.9770 | LR: 8.79e-05\n",
            "  Batch 17800/26069 | Loss: 0.9770 | LR: 8.79e-05\n",
            "  Batch 17900/26069 | Loss: 0.9769 | LR: 8.79e-05\n",
            "  Batch 18000/26069 | Loss: 0.9769 | LR: 8.79e-05\n",
            "  Batch 18100/26069 | Loss: 0.9770 | LR: 8.79e-05\n",
            "  Batch 18200/26069 | Loss: 0.9770 | LR: 8.79e-05\n",
            "  Batch 18300/26069 | Loss: 0.9771 | LR: 8.79e-05\n",
            "  Batch 18400/26069 | Loss: 0.9771 | LR: 8.79e-05\n",
            "  Batch 18500/26069 | Loss: 0.9772 | LR: 8.79e-05\n",
            "  Batch 18600/26069 | Loss: 0.9771 | LR: 8.79e-05\n",
            "  Batch 18700/26069 | Loss: 0.9770 | LR: 8.79e-05\n",
            "  Batch 18800/26069 | Loss: 0.9770 | LR: 8.79e-05\n",
            "  Batch 18900/26069 | Loss: 0.9769 | LR: 8.79e-05\n",
            "  Batch 19000/26069 | Loss: 0.9769 | LR: 8.79e-05\n",
            "  Batch 19100/26069 | Loss: 0.9768 | LR: 8.79e-05\n",
            "  Batch 19200/26069 | Loss: 0.9767 | LR: 8.79e-05\n",
            "  Batch 19300/26069 | Loss: 0.9767 | LR: 8.79e-05\n",
            "  Batch 19400/26069 | Loss: 0.9766 | LR: 8.78e-05\n",
            "  Batch 19500/26069 | Loss: 0.9767 | LR: 8.78e-05\n",
            "  Batch 19600/26069 | Loss: 0.9767 | LR: 8.78e-05\n",
            "  Batch 19700/26069 | Loss: 0.9765 | LR: 8.78e-05\n",
            "  Batch 19800/26069 | Loss: 0.9765 | LR: 8.78e-05\n",
            "  Batch 19900/26069 | Loss: 0.9765 | LR: 8.78e-05\n",
            "  Batch 20000/26069 | Loss: 0.9764 | LR: 8.78e-05\n",
            "  Batch 20100/26069 | Loss: 0.9764 | LR: 8.78e-05\n",
            "  Batch 20200/26069 | Loss: 0.9764 | LR: 8.78e-05\n",
            "  Batch 20300/26069 | Loss: 0.9764 | LR: 8.78e-05\n",
            "  Batch 20400/26069 | Loss: 0.9764 | LR: 8.78e-05\n",
            "  Batch 20500/26069 | Loss: 0.9763 | LR: 8.78e-05\n",
            "  Batch 20600/26069 | Loss: 0.9763 | LR: 8.78e-05\n",
            "  Batch 20700/26069 | Loss: 0.9763 | LR: 8.78e-05\n",
            "  Batch 20800/26069 | Loss: 0.9763 | LR: 8.78e-05\n",
            "  Batch 20900/26069 | Loss: 0.9762 | LR: 8.78e-05\n",
            "  Batch 21000/26069 | Loss: 0.9762 | LR: 8.78e-05\n",
            "  Batch 21100/26069 | Loss: 0.9762 | LR: 8.78e-05\n",
            "  Batch 21200/26069 | Loss: 0.9762 | LR: 8.78e-05\n",
            "  Batch 21300/26069 | Loss: 0.9762 | LR: 8.78e-05\n",
            "  Batch 21400/26069 | Loss: 0.9762 | LR: 8.78e-05\n",
            "  Batch 21500/26069 | Loss: 0.9761 | LR: 8.78e-05\n",
            "  Batch 21600/26069 | Loss: 0.9761 | LR: 8.78e-05\n",
            "  Batch 21700/26069 | Loss: 0.9761 | LR: 8.78e-05\n",
            "  Batch 21800/26069 | Loss: 0.9761 | LR: 8.78e-05\n",
            "  Batch 21900/26069 | Loss: 0.9761 | LR: 8.78e-05\n",
            "  Batch 22000/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 22100/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 22200/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 22300/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 22400/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 22500/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 22600/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 22700/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 22800/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 22900/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 23000/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 23100/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 23200/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 23300/26069 | Loss: 0.9763 | LR: 8.77e-05\n",
            "  Batch 23400/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 23500/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 23600/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 23700/26069 | Loss: 0.9761 | LR: 8.77e-05\n",
            "  Batch 23800/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 23900/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 24000/26069 | Loss: 0.9763 | LR: 8.77e-05\n",
            "  Batch 24100/26069 | Loss: 0.9763 | LR: 8.77e-05\n",
            "  Batch 24200/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 24300/26069 | Loss: 0.9762 | LR: 8.77e-05\n",
            "  Batch 24400/26069 | Loss: 0.9763 | LR: 8.77e-05\n",
            "  Batch 24500/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 24600/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 24700/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 24800/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 24900/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 25000/26069 | Loss: 0.9764 | LR: 8.76e-05\n",
            "  Batch 25100/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 25200/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 25300/26069 | Loss: 0.9762 | LR: 8.76e-05\n",
            "  Batch 25400/26069 | Loss: 0.9761 | LR: 8.76e-05\n",
            "  Batch 25500/26069 | Loss: 0.9762 | LR: 8.76e-05\n",
            "  Batch 25600/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 25700/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 25800/26069 | Loss: 0.9764 | LR: 8.76e-05\n",
            "  Batch 25900/26069 | Loss: 0.9763 | LR: 8.76e-05\n",
            "  Batch 26000/26069 | Loss: 0.9764 | LR: 8.76e-05\n",
            "\n",
            "  Train Loss: 0.9763\n",
            "  Val Loss:   0.9576 \n",
            "  Time:       313.8s\n",
            "\n",
            "Epoch 24/100\n",
            "  Batch 100/26069 | Loss: 0.9622 | LR: 8.76e-05\n",
            "  Batch 200/26069 | Loss: 0.9804 | LR: 8.76e-05\n",
            "  Batch 300/26069 | Loss: 0.9826 | LR: 8.76e-05\n",
            "  Batch 400/26069 | Loss: 0.9836 | LR: 8.76e-05\n",
            "  Batch 500/26069 | Loss: 0.9803 | LR: 8.76e-05\n",
            "  Batch 600/26069 | Loss: 0.9810 | LR: 8.76e-05\n",
            "  Batch 700/26069 | Loss: 0.9820 | LR: 8.76e-05\n",
            "  Batch 800/26069 | Loss: 0.9816 | LR: 8.76e-05\n",
            "  Batch 900/26069 | Loss: 0.9809 | LR: 8.75e-05\n",
            "  Batch 1000/26069 | Loss: 0.9812 | LR: 8.75e-05\n",
            "  Batch 1100/26069 | Loss: 0.9803 | LR: 8.75e-05\n",
            "  Batch 1200/26069 | Loss: 0.9783 | LR: 8.75e-05\n",
            "  Batch 1300/26069 | Loss: 0.9771 | LR: 8.75e-05\n",
            "  Batch 1400/26069 | Loss: 0.9772 | LR: 8.75e-05\n",
            "  Batch 1500/26069 | Loss: 0.9764 | LR: 8.75e-05\n",
            "  Batch 1600/26069 | Loss: 0.9770 | LR: 8.75e-05\n",
            "  Batch 1700/26069 | Loss: 0.9769 | LR: 8.75e-05\n",
            "  Batch 1800/26069 | Loss: 0.9771 | LR: 8.75e-05\n",
            "  Batch 1900/26069 | Loss: 0.9768 | LR: 8.75e-05\n",
            "  Batch 2000/26069 | Loss: 0.9766 | LR: 8.75e-05\n",
            "  Batch 2100/26069 | Loss: 0.9771 | LR: 8.75e-05\n",
            "  Batch 2200/26069 | Loss: 0.9779 | LR: 8.75e-05\n",
            "  Batch 2300/26069 | Loss: 0.9791 | LR: 8.75e-05\n",
            "  Batch 2400/26069 | Loss: 0.9795 | LR: 8.75e-05\n",
            "  Batch 2500/26069 | Loss: 0.9790 | LR: 8.75e-05\n",
            "  Batch 2600/26069 | Loss: 0.9797 | LR: 8.75e-05\n",
            "  Batch 2700/26069 | Loss: 0.9802 | LR: 8.75e-05\n",
            "  Batch 2800/26069 | Loss: 0.9795 | LR: 8.75e-05\n",
            "  Batch 2900/26069 | Loss: 0.9790 | LR: 8.75e-05\n",
            "  Batch 3000/26069 | Loss: 0.9786 | LR: 8.75e-05\n",
            "  Batch 3100/26069 | Loss: 0.9785 | LR: 8.75e-05\n",
            "  Batch 3200/26069 | Loss: 0.9790 | LR: 8.75e-05\n",
            "  Batch 3300/26069 | Loss: 0.9784 | LR: 8.75e-05\n",
            "  Batch 3400/26069 | Loss: 0.9781 | LR: 8.74e-05\n",
            "  Batch 3500/26069 | Loss: 0.9786 | LR: 8.74e-05\n",
            "  Batch 3600/26069 | Loss: 0.9786 | LR: 8.74e-05\n",
            "  Batch 3700/26069 | Loss: 0.9781 | LR: 8.74e-05\n",
            "  Batch 3800/26069 | Loss: 0.9777 | LR: 8.74e-05\n",
            "  Batch 3900/26069 | Loss: 0.9776 | LR: 8.74e-05\n",
            "  Batch 4000/26069 | Loss: 0.9775 | LR: 8.74e-05\n",
            "  Batch 4100/26069 | Loss: 0.9775 | LR: 8.74e-05\n",
            "  Batch 4200/26069 | Loss: 0.9780 | LR: 8.74e-05\n",
            "  Batch 4300/26069 | Loss: 0.9777 | LR: 8.74e-05\n",
            "  Batch 4400/26069 | Loss: 0.9775 | LR: 8.74e-05\n",
            "  Batch 4500/26069 | Loss: 0.9774 | LR: 8.74e-05\n",
            "  Batch 4600/26069 | Loss: 0.9771 | LR: 8.74e-05\n",
            "  Batch 4700/26069 | Loss: 0.9773 | LR: 8.74e-05\n",
            "  Batch 4800/26069 | Loss: 0.9769 | LR: 8.74e-05\n",
            "  Batch 4900/26069 | Loss: 0.9765 | LR: 8.74e-05\n",
            "  Batch 5000/26069 | Loss: 0.9761 | LR: 8.74e-05\n",
            "  Batch 5100/26069 | Loss: 0.9761 | LR: 8.74e-05\n",
            "  Batch 5200/26069 | Loss: 0.9760 | LR: 8.74e-05\n",
            "  Batch 5300/26069 | Loss: 0.9760 | LR: 8.74e-05\n",
            "  Batch 5400/26069 | Loss: 0.9756 | LR: 8.74e-05\n",
            "  Batch 5500/26069 | Loss: 0.9757 | LR: 8.74e-05\n",
            "  Batch 5600/26069 | Loss: 0.9757 | LR: 8.74e-05\n",
            "  Batch 5700/26069 | Loss: 0.9756 | LR: 8.74e-05\n",
            "  Batch 5800/26069 | Loss: 0.9756 | LR: 8.74e-05\n",
            "  Batch 5900/26069 | Loss: 0.9759 | LR: 8.73e-05\n",
            "  Batch 6000/26069 | Loss: 0.9758 | LR: 8.73e-05\n",
            "  Batch 6100/26069 | Loss: 0.9757 | LR: 8.73e-05\n",
            "  Batch 6200/26069 | Loss: 0.9755 | LR: 8.73e-05\n",
            "  Batch 6300/26069 | Loss: 0.9753 | LR: 8.73e-05\n",
            "  Batch 6400/26069 | Loss: 0.9751 | LR: 8.73e-05\n",
            "  Batch 6500/26069 | Loss: 0.9753 | LR: 8.73e-05\n",
            "  Batch 6600/26069 | Loss: 0.9754 | LR: 8.73e-05\n",
            "  Batch 6700/26069 | Loss: 0.9750 | LR: 8.73e-05\n",
            "  Batch 6800/26069 | Loss: 0.9748 | LR: 8.73e-05\n",
            "  Batch 6900/26069 | Loss: 0.9750 | LR: 8.73e-05\n",
            "  Batch 7000/26069 | Loss: 0.9750 | LR: 8.73e-05\n",
            "  Batch 7100/26069 | Loss: 0.9749 | LR: 8.73e-05\n",
            "  Batch 7200/26069 | Loss: 0.9749 | LR: 8.73e-05\n",
            "  Batch 7300/26069 | Loss: 0.9751 | LR: 8.73e-05\n",
            "  Batch 7400/26069 | Loss: 0.9752 | LR: 8.73e-05\n",
            "  Batch 7500/26069 | Loss: 0.9751 | LR: 8.73e-05\n",
            "  Batch 7600/26069 | Loss: 0.9753 | LR: 8.73e-05\n",
            "  Batch 7700/26069 | Loss: 0.9753 | LR: 8.73e-05\n",
            "  Batch 7800/26069 | Loss: 0.9755 | LR: 8.73e-05\n",
            "  Batch 7900/26069 | Loss: 0.9754 | LR: 8.73e-05\n",
            "  Batch 8000/26069 | Loss: 0.9754 | LR: 8.73e-05\n",
            "  Batch 8100/26069 | Loss: 0.9755 | LR: 8.73e-05\n",
            "  Batch 8200/26069 | Loss: 0.9756 | LR: 8.73e-05\n",
            "  Batch 8300/26069 | Loss: 0.9756 | LR: 8.73e-05\n",
            "  Batch 8400/26069 | Loss: 0.9753 | LR: 8.72e-05\n",
            "  Batch 8500/26069 | Loss: 0.9753 | LR: 8.72e-05\n",
            "  Batch 8600/26069 | Loss: 0.9753 | LR: 8.72e-05\n",
            "  Batch 8700/26069 | Loss: 0.9749 | LR: 8.72e-05\n",
            "  Batch 8800/26069 | Loss: 0.9749 | LR: 8.72e-05\n",
            "  Batch 8900/26069 | Loss: 0.9751 | LR: 8.72e-05\n",
            "  Batch 9000/26069 | Loss: 0.9751 | LR: 8.72e-05\n",
            "  Batch 9100/26069 | Loss: 0.9750 | LR: 8.72e-05\n",
            "  Batch 9200/26069 | Loss: 0.9752 | LR: 8.72e-05\n",
            "  Batch 9300/26069 | Loss: 0.9752 | LR: 8.72e-05\n",
            "  Batch 9400/26069 | Loss: 0.9750 | LR: 8.72e-05\n",
            "  Batch 9500/26069 | Loss: 0.9751 | LR: 8.72e-05\n",
            "  Batch 9600/26069 | Loss: 0.9752 | LR: 8.72e-05\n",
            "  Batch 9700/26069 | Loss: 0.9750 | LR: 8.72e-05\n",
            "  Batch 9800/26069 | Loss: 0.9750 | LR: 8.72e-05\n",
            "  Batch 9900/26069 | Loss: 0.9753 | LR: 8.72e-05\n",
            "  Batch 10000/26069 | Loss: 0.9753 | LR: 8.72e-05\n",
            "  Batch 10100/26069 | Loss: 0.9755 | LR: 8.72e-05\n",
            "  Batch 10200/26069 | Loss: 0.9757 | LR: 8.72e-05\n",
            "  Batch 10300/26069 | Loss: 0.9758 | LR: 8.72e-05\n",
            "  Batch 10400/26069 | Loss: 0.9758 | LR: 8.72e-05\n",
            "  Batch 10500/26069 | Loss: 0.9757 | LR: 8.72e-05\n",
            "  Batch 10600/26069 | Loss: 0.9756 | LR: 8.72e-05\n",
            "  Batch 10700/26069 | Loss: 0.9756 | LR: 8.72e-05\n",
            "  Batch 10800/26069 | Loss: 0.9756 | LR: 8.72e-05\n",
            "  Batch 10900/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 11000/26069 | Loss: 0.9755 | LR: 8.71e-05\n",
            "  Batch 11100/26069 | Loss: 0.9755 | LR: 8.71e-05\n",
            "  Batch 11200/26069 | Loss: 0.9755 | LR: 8.71e-05\n",
            "  Batch 11300/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 11400/26069 | Loss: 0.9757 | LR: 8.71e-05\n",
            "  Batch 11500/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 11600/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 11700/26069 | Loss: 0.9757 | LR: 8.71e-05\n",
            "  Batch 11800/26069 | Loss: 0.9757 | LR: 8.71e-05\n",
            "  Batch 11900/26069 | Loss: 0.9757 | LR: 8.71e-05\n",
            "  Batch 12000/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 12100/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 12200/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 12300/26069 | Loss: 0.9754 | LR: 8.71e-05\n",
            "  Batch 12400/26069 | Loss: 0.9753 | LR: 8.71e-05\n",
            "  Batch 12500/26069 | Loss: 0.9752 | LR: 8.71e-05\n",
            "  Batch 12600/26069 | Loss: 0.9752 | LR: 8.71e-05\n",
            "  Batch 12700/26069 | Loss: 0.9753 | LR: 8.71e-05\n",
            "  Batch 12800/26069 | Loss: 0.9754 | LR: 8.71e-05\n",
            "  Batch 12900/26069 | Loss: 0.9754 | LR: 8.71e-05\n",
            "  Batch 13000/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 13100/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 13200/26069 | Loss: 0.9755 | LR: 8.71e-05\n",
            "  Batch 13300/26069 | Loss: 0.9756 | LR: 8.71e-05\n",
            "  Batch 13400/26069 | Loss: 0.9755 | LR: 8.70e-05\n",
            "  Batch 13500/26069 | Loss: 0.9755 | LR: 8.70e-05\n",
            "  Batch 13600/26069 | Loss: 0.9753 | LR: 8.70e-05\n",
            "  Batch 13700/26069 | Loss: 0.9751 | LR: 8.70e-05\n",
            "  Batch 13800/26069 | Loss: 0.9751 | LR: 8.70e-05\n",
            "  Batch 13900/26069 | Loss: 0.9750 | LR: 8.70e-05\n",
            "  Batch 14000/26069 | Loss: 0.9751 | LR: 8.70e-05\n",
            "  Batch 14100/26069 | Loss: 0.9750 | LR: 8.70e-05\n",
            "  Batch 14200/26069 | Loss: 0.9750 | LR: 8.70e-05\n",
            "  Batch 14300/26069 | Loss: 0.9750 | LR: 8.70e-05\n",
            "  Batch 14400/26069 | Loss: 0.9749 | LR: 8.70e-05\n",
            "  Batch 14500/26069 | Loss: 0.9750 | LR: 8.70e-05\n",
            "  Batch 14600/26069 | Loss: 0.9749 | LR: 8.70e-05\n",
            "  Batch 14700/26069 | Loss: 0.9748 | LR: 8.70e-05\n",
            "  Batch 14800/26069 | Loss: 0.9750 | LR: 8.70e-05\n",
            "  Batch 14900/26069 | Loss: 0.9749 | LR: 8.70e-05\n",
            "  Batch 15000/26069 | Loss: 0.9748 | LR: 8.70e-05\n",
            "  Batch 15100/26069 | Loss: 0.9748 | LR: 8.70e-05\n",
            "  Batch 15200/26069 | Loss: 0.9748 | LR: 8.70e-05\n",
            "  Batch 15300/26069 | Loss: 0.9748 | LR: 8.70e-05\n",
            "  Batch 15400/26069 | Loss: 0.9748 | LR: 8.70e-05\n",
            "  Batch 15500/26069 | Loss: 0.9749 | LR: 8.70e-05\n",
            "  Batch 15600/26069 | Loss: 0.9749 | LR: 8.70e-05\n",
            "  Batch 15700/26069 | Loss: 0.9749 | LR: 8.70e-05\n",
            "  Batch 15800/26069 | Loss: 0.9749 | LR: 8.70e-05\n",
            "  Batch 15900/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 16000/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 16100/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 16200/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 16300/26069 | Loss: 0.9747 | LR: 8.69e-05\n",
            "  Batch 16400/26069 | Loss: 0.9748 | LR: 8.69e-05\n",
            "  Batch 16500/26069 | Loss: 0.9748 | LR: 8.69e-05\n",
            "  Batch 16600/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 16700/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 16800/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 16900/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 17000/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 17100/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 17200/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 17300/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 17400/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 17500/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 17600/26069 | Loss: 0.9751 | LR: 8.69e-05\n",
            "  Batch 17700/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 17800/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 17900/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 18000/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 18100/26069 | Loss: 0.9750 | LR: 8.69e-05\n",
            "  Batch 18200/26069 | Loss: 0.9749 | LR: 8.69e-05\n",
            "  Batch 18300/26069 | Loss: 0.9750 | LR: 8.68e-05\n",
            "  Batch 18400/26069 | Loss: 0.9749 | LR: 8.68e-05\n",
            "  Batch 18500/26069 | Loss: 0.9748 | LR: 8.68e-05\n",
            "  Batch 18600/26069 | Loss: 0.9749 | LR: 8.68e-05\n",
            "  Batch 18700/26069 | Loss: 0.9750 | LR: 8.68e-05\n",
            "  Batch 18800/26069 | Loss: 0.9751 | LR: 8.68e-05\n",
            "  Batch 18900/26069 | Loss: 0.9751 | LR: 8.68e-05\n",
            "  Batch 19000/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 19100/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 19200/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 19300/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 19400/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 19500/26069 | Loss: 0.9754 | LR: 8.68e-05\n",
            "  Batch 19600/26069 | Loss: 0.9754 | LR: 8.68e-05\n",
            "  Batch 19700/26069 | Loss: 0.9753 | LR: 8.68e-05\n",
            "  Batch 19800/26069 | Loss: 0.9753 | LR: 8.68e-05\n",
            "  Batch 19900/26069 | Loss: 0.9753 | LR: 8.68e-05\n",
            "  Batch 20000/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 20100/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 20200/26069 | Loss: 0.9751 | LR: 8.68e-05\n",
            "  Batch 20300/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 20400/26069 | Loss: 0.9753 | LR: 8.68e-05\n",
            "  Batch 20500/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 20600/26069 | Loss: 0.9752 | LR: 8.68e-05\n",
            "  Batch 20700/26069 | Loss: 0.9754 | LR: 8.68e-05\n",
            "  Batch 20800/26069 | Loss: 0.9753 | LR: 8.67e-05\n",
            "  Batch 20900/26069 | Loss: 0.9753 | LR: 8.67e-05\n",
            "  Batch 21000/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21100/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21200/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21300/26069 | Loss: 0.9753 | LR: 8.67e-05\n",
            "  Batch 21400/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21500/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21600/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21700/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21800/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 21900/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22000/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22100/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22200/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 22300/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22400/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22500/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22600/26069 | Loss: 0.9754 | LR: 8.67e-05\n",
            "  Batch 22700/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22800/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 22900/26069 | Loss: 0.9755 | LR: 8.67e-05\n",
            "  Batch 23000/26069 | Loss: 0.9756 | LR: 8.67e-05\n",
            "  Batch 23100/26069 | Loss: 0.9758 | LR: 8.67e-05\n",
            "  Batch 23200/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 23300/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 23400/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 23500/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 23600/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 23700/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 23800/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 23900/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 24000/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 24100/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 24200/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 24300/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 24400/26069 | Loss: 0.9759 | LR: 8.66e-05\n",
            "  Batch 24500/26069 | Loss: 0.9759 | LR: 8.66e-05\n",
            "  Batch 24600/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 24700/26069 | Loss: 0.9759 | LR: 8.66e-05\n",
            "  Batch 24800/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 24900/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 25000/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 25100/26069 | Loss: 0.9758 | LR: 8.66e-05\n",
            "  Batch 25200/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 25300/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 25400/26069 | Loss: 0.9757 | LR: 8.66e-05\n",
            "  Batch 25500/26069 | Loss: 0.9756 | LR: 8.66e-05\n",
            "  Batch 25600/26069 | Loss: 0.9756 | LR: 8.66e-05\n",
            "  Batch 25700/26069 | Loss: 0.9756 | LR: 8.65e-05\n",
            "  Batch 25800/26069 | Loss: 0.9756 | LR: 8.65e-05\n",
            "  Batch 25900/26069 | Loss: 0.9756 | LR: 8.65e-05\n",
            "  Batch 26000/26069 | Loss: 0.9756 | LR: 8.65e-05\n",
            "\n",
            "  Train Loss: 0.9756\n",
            "  Val Loss:   0.9542 [BEST]\n",
            "  Time:       313.3s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_24.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 25/100\n",
            "  Batch 100/26069 | Loss: 0.9638 | LR: 8.65e-05\n",
            "  Batch 200/26069 | Loss: 0.9657 | LR: 8.65e-05\n",
            "  Batch 300/26069 | Loss: 0.9675 | LR: 8.65e-05\n",
            "  Batch 400/26069 | Loss: 0.9684 | LR: 8.65e-05\n",
            "  Batch 500/26069 | Loss: 0.9690 | LR: 8.65e-05\n",
            "  Batch 600/26069 | Loss: 0.9681 | LR: 8.65e-05\n",
            "  Batch 700/26069 | Loss: 0.9683 | LR: 8.65e-05\n",
            "  Batch 800/26069 | Loss: 0.9719 | LR: 8.65e-05\n",
            "  Batch 900/26069 | Loss: 0.9735 | LR: 8.65e-05\n",
            "  Batch 1000/26069 | Loss: 0.9736 | LR: 8.65e-05\n",
            "  Batch 1100/26069 | Loss: 0.9734 | LR: 8.65e-05\n",
            "  Batch 1200/26069 | Loss: 0.9733 | LR: 8.65e-05\n",
            "  Batch 1300/26069 | Loss: 0.9739 | LR: 8.65e-05\n",
            "  Batch 1400/26069 | Loss: 0.9738 | LR: 8.65e-05\n",
            "  Batch 1500/26069 | Loss: 0.9738 | LR: 8.65e-05\n",
            "  Batch 1600/26069 | Loss: 0.9751 | LR: 8.65e-05\n",
            "  Batch 1700/26069 | Loss: 0.9743 | LR: 8.65e-05\n",
            "  Batch 1800/26069 | Loss: 0.9749 | LR: 8.65e-05\n",
            "  Batch 1900/26069 | Loss: 0.9751 | LR: 8.65e-05\n",
            "  Batch 2000/26069 | Loss: 0.9746 | LR: 8.64e-05\n",
            "  Batch 2100/26069 | Loss: 0.9762 | LR: 8.64e-05\n",
            "  Batch 2200/26069 | Loss: 0.9760 | LR: 8.64e-05\n",
            "  Batch 2300/26069 | Loss: 0.9757 | LR: 8.64e-05\n",
            "  Batch 2400/26069 | Loss: 0.9751 | LR: 8.64e-05\n",
            "  Batch 2500/26069 | Loss: 0.9746 | LR: 8.64e-05\n",
            "  Batch 2600/26069 | Loss: 0.9750 | LR: 8.64e-05\n",
            "  Batch 2700/26069 | Loss: 0.9752 | LR: 8.64e-05\n",
            "  Batch 2800/26069 | Loss: 0.9756 | LR: 8.64e-05\n",
            "  Batch 2900/26069 | Loss: 0.9759 | LR: 8.64e-05\n",
            "  Batch 3000/26069 | Loss: 0.9755 | LR: 8.64e-05\n",
            "  Batch 3100/26069 | Loss: 0.9755 | LR: 8.64e-05\n",
            "  Batch 3200/26069 | Loss: 0.9755 | LR: 8.64e-05\n",
            "  Batch 3300/26069 | Loss: 0.9756 | LR: 8.64e-05\n",
            "  Batch 3400/26069 | Loss: 0.9759 | LR: 8.64e-05\n",
            "  Batch 3500/26069 | Loss: 0.9760 | LR: 8.64e-05\n",
            "  Batch 3600/26069 | Loss: 0.9754 | LR: 8.64e-05\n",
            "  Batch 3700/26069 | Loss: 0.9755 | LR: 8.64e-05\n",
            "  Batch 3800/26069 | Loss: 0.9759 | LR: 8.64e-05\n",
            "  Batch 3900/26069 | Loss: 0.9754 | LR: 8.64e-05\n",
            "  Batch 4000/26069 | Loss: 0.9752 | LR: 8.64e-05\n",
            "  Batch 4100/26069 | Loss: 0.9751 | LR: 8.64e-05\n",
            "  Batch 4200/26069 | Loss: 0.9752 | LR: 8.64e-05\n",
            "  Batch 4300/26069 | Loss: 0.9755 | LR: 8.64e-05\n",
            "  Batch 4400/26069 | Loss: 0.9757 | LR: 8.63e-05\n",
            "  Batch 4500/26069 | Loss: 0.9757 | LR: 8.63e-05\n",
            "  Batch 4600/26069 | Loss: 0.9753 | LR: 8.63e-05\n",
            "  Batch 4700/26069 | Loss: 0.9754 | LR: 8.63e-05\n",
            "  Batch 4800/26069 | Loss: 0.9759 | LR: 8.63e-05\n",
            "  Batch 4900/26069 | Loss: 0.9756 | LR: 8.63e-05\n",
            "  Batch 5000/26069 | Loss: 0.9754 | LR: 8.63e-05\n",
            "  Batch 5100/26069 | Loss: 0.9753 | LR: 8.63e-05\n",
            "  Batch 5200/26069 | Loss: 0.9751 | LR: 8.63e-05\n",
            "  Batch 5300/26069 | Loss: 0.9753 | LR: 8.63e-05\n",
            "  Batch 5400/26069 | Loss: 0.9753 | LR: 8.63e-05\n",
            "  Batch 5500/26069 | Loss: 0.9752 | LR: 8.63e-05\n",
            "  Batch 5600/26069 | Loss: 0.9750 | LR: 8.63e-05\n",
            "  Batch 5700/26069 | Loss: 0.9749 | LR: 8.63e-05\n",
            "  Batch 5800/26069 | Loss: 0.9750 | LR: 8.63e-05\n",
            "  Batch 5900/26069 | Loss: 0.9752 | LR: 8.63e-05\n",
            "  Batch 6000/26069 | Loss: 0.9754 | LR: 8.63e-05\n",
            "  Batch 6100/26069 | Loss: 0.9750 | LR: 8.63e-05\n",
            "  Batch 6200/26069 | Loss: 0.9751 | LR: 8.63e-05\n",
            "  Batch 6300/26069 | Loss: 0.9753 | LR: 8.63e-05\n",
            "  Batch 6400/26069 | Loss: 0.9752 | LR: 8.63e-05\n",
            "  Batch 6500/26069 | Loss: 0.9755 | LR: 8.63e-05\n",
            "  Batch 6600/26069 | Loss: 0.9754 | LR: 8.63e-05\n",
            "  Batch 6700/26069 | Loss: 0.9751 | LR: 8.63e-05\n",
            "  Batch 6800/26069 | Loss: 0.9753 | LR: 8.62e-05\n",
            "  Batch 6900/26069 | Loss: 0.9752 | LR: 8.62e-05\n",
            "  Batch 7000/26069 | Loss: 0.9752 | LR: 8.62e-05\n",
            "  Batch 7100/26069 | Loss: 0.9752 | LR: 8.62e-05\n",
            "  Batch 7200/26069 | Loss: 0.9754 | LR: 8.62e-05\n",
            "  Batch 7300/26069 | Loss: 0.9756 | LR: 8.62e-05\n",
            "  Batch 7400/26069 | Loss: 0.9755 | LR: 8.62e-05\n",
            "  Batch 7500/26069 | Loss: 0.9756 | LR: 8.62e-05\n",
            "  Batch 7600/26069 | Loss: 0.9753 | LR: 8.62e-05\n",
            "  Batch 7700/26069 | Loss: 0.9753 | LR: 8.62e-05\n",
            "  Batch 7800/26069 | Loss: 0.9753 | LR: 8.62e-05\n",
            "  Batch 7900/26069 | Loss: 0.9752 | LR: 8.62e-05\n",
            "  Batch 8000/26069 | Loss: 0.9752 | LR: 8.62e-05\n",
            "  Batch 8100/26069 | Loss: 0.9753 | LR: 8.62e-05\n",
            "  Batch 8200/26069 | Loss: 0.9753 | LR: 8.62e-05\n",
            "  Batch 8300/26069 | Loss: 0.9754 | LR: 8.62e-05\n",
            "  Batch 8400/26069 | Loss: 0.9755 | LR: 8.62e-05\n",
            "  Batch 8500/26069 | Loss: 0.9755 | LR: 8.62e-05\n",
            "  Batch 8600/26069 | Loss: 0.9755 | LR: 8.62e-05\n",
            "  Batch 8700/26069 | Loss: 0.9757 | LR: 8.62e-05\n",
            "  Batch 8800/26069 | Loss: 0.9758 | LR: 8.62e-05\n",
            "  Batch 8900/26069 | Loss: 0.9758 | LR: 8.62e-05\n",
            "  Batch 9000/26069 | Loss: 0.9758 | LR: 8.62e-05\n",
            "  Batch 9100/26069 | Loss: 0.9757 | LR: 8.62e-05\n",
            "  Batch 9200/26069 | Loss: 0.9756 | LR: 8.62e-05\n",
            "  Batch 9300/26069 | Loss: 0.9755 | LR: 8.61e-05\n",
            "  Batch 9400/26069 | Loss: 0.9752 | LR: 8.61e-05\n",
            "  Batch 9500/26069 | Loss: 0.9749 | LR: 8.61e-05\n",
            "  Batch 9600/26069 | Loss: 0.9748 | LR: 8.61e-05\n",
            "  Batch 9700/26069 | Loss: 0.9747 | LR: 8.61e-05\n",
            "  Batch 9800/26069 | Loss: 0.9748 | LR: 8.61e-05\n",
            "  Batch 9900/26069 | Loss: 0.9753 | LR: 8.61e-05\n",
            "  Batch 10000/26069 | Loss: 0.9754 | LR: 8.61e-05\n",
            "  Batch 10100/26069 | Loss: 0.9756 | LR: 8.61e-05\n",
            "  Batch 10200/26069 | Loss: 0.9758 | LR: 8.61e-05\n",
            "  Batch 10300/26069 | Loss: 0.9758 | LR: 8.61e-05\n",
            "  Batch 10400/26069 | Loss: 0.9757 | LR: 8.61e-05\n",
            "  Batch 10500/26069 | Loss: 0.9756 | LR: 8.61e-05\n",
            "  Batch 10600/26069 | Loss: 0.9759 | LR: 8.61e-05\n",
            "  Batch 10700/26069 | Loss: 0.9761 | LR: 8.61e-05\n",
            "  Batch 10800/26069 | Loss: 0.9762 | LR: 8.61e-05\n",
            "  Batch 10900/26069 | Loss: 0.9762 | LR: 8.61e-05\n",
            "  Batch 11000/26069 | Loss: 0.9762 | LR: 8.61e-05\n",
            "  Batch 11100/26069 | Loss: 0.9762 | LR: 8.61e-05\n",
            "  Batch 11200/26069 | Loss: 0.9762 | LR: 8.61e-05\n",
            "  Batch 11300/26069 | Loss: 0.9761 | LR: 8.61e-05\n",
            "  Batch 11400/26069 | Loss: 0.9761 | LR: 8.61e-05\n",
            "  Batch 11500/26069 | Loss: 0.9761 | LR: 8.61e-05\n",
            "  Batch 11600/26069 | Loss: 0.9762 | LR: 8.61e-05\n",
            "  Batch 11700/26069 | Loss: 0.9761 | LR: 8.60e-05\n",
            "  Batch 11800/26069 | Loss: 0.9759 | LR: 8.60e-05\n",
            "  Batch 11900/26069 | Loss: 0.9759 | LR: 8.60e-05\n",
            "  Batch 12000/26069 | Loss: 0.9760 | LR: 8.60e-05\n",
            "  Batch 12100/26069 | Loss: 0.9759 | LR: 8.60e-05\n",
            "  Batch 12200/26069 | Loss: 0.9758 | LR: 8.60e-05\n",
            "  Batch 12300/26069 | Loss: 0.9757 | LR: 8.60e-05\n",
            "  Batch 12400/26069 | Loss: 0.9757 | LR: 8.60e-05\n",
            "  Batch 12500/26069 | Loss: 0.9755 | LR: 8.60e-05\n",
            "  Batch 12600/26069 | Loss: 0.9755 | LR: 8.60e-05\n",
            "  Batch 12700/26069 | Loss: 0.9755 | LR: 8.60e-05\n",
            "  Batch 12800/26069 | Loss: 0.9756 | LR: 8.60e-05\n",
            "  Batch 12900/26069 | Loss: 0.9757 | LR: 8.60e-05\n",
            "  Batch 13000/26069 | Loss: 0.9757 | LR: 8.60e-05\n",
            "  Batch 13100/26069 | Loss: 0.9756 | LR: 8.60e-05\n",
            "  Batch 13200/26069 | Loss: 0.9754 | LR: 8.60e-05\n",
            "  Batch 13300/26069 | Loss: 0.9756 | LR: 8.60e-05\n",
            "  Batch 13400/26069 | Loss: 0.9754 | LR: 8.60e-05\n",
            "  Batch 13500/26069 | Loss: 0.9754 | LR: 8.60e-05\n",
            "  Batch 13600/26069 | Loss: 0.9751 | LR: 8.60e-05\n",
            "  Batch 13700/26069 | Loss: 0.9751 | LR: 8.60e-05\n",
            "  Batch 13800/26069 | Loss: 0.9751 | LR: 8.60e-05\n",
            "  Batch 13900/26069 | Loss: 0.9751 | LR: 8.60e-05\n",
            "  Batch 14000/26069 | Loss: 0.9750 | LR: 8.59e-05\n",
            "  Batch 14100/26069 | Loss: 0.9752 | LR: 8.59e-05\n",
            "  Batch 14200/26069 | Loss: 0.9752 | LR: 8.59e-05\n",
            "  Batch 14300/26069 | Loss: 0.9751 | LR: 8.59e-05\n",
            "  Batch 14400/26069 | Loss: 0.9753 | LR: 8.59e-05\n",
            "  Batch 14500/26069 | Loss: 0.9754 | LR: 8.59e-05\n",
            "  Batch 14600/26069 | Loss: 0.9754 | LR: 8.59e-05\n",
            "  Batch 14700/26069 | Loss: 0.9753 | LR: 8.59e-05\n",
            "  Batch 14800/26069 | Loss: 0.9755 | LR: 8.59e-05\n",
            "  Batch 14900/26069 | Loss: 0.9754 | LR: 8.59e-05\n",
            "  Batch 15000/26069 | Loss: 0.9755 | LR: 8.59e-05\n",
            "  Batch 15100/26069 | Loss: 0.9754 | LR: 8.59e-05\n",
            "  Batch 15200/26069 | Loss: 0.9753 | LR: 8.59e-05\n",
            "  Batch 15300/26069 | Loss: 0.9753 | LR: 8.59e-05\n",
            "  Batch 15400/26069 | Loss: 0.9754 | LR: 8.59e-05\n",
            "  Batch 15500/26069 | Loss: 0.9755 | LR: 8.59e-05\n",
            "  Batch 15600/26069 | Loss: 0.9755 | LR: 8.59e-05\n",
            "  Batch 15700/26069 | Loss: 0.9755 | LR: 8.59e-05\n",
            "  Batch 15800/26069 | Loss: 0.9755 | LR: 8.59e-05\n",
            "  Batch 15900/26069 | Loss: 0.9755 | LR: 8.59e-05\n",
            "  Batch 16000/26069 | Loss: 0.9754 | LR: 8.59e-05\n",
            "  Batch 16100/26069 | Loss: 0.9754 | LR: 8.59e-05\n",
            "  Batch 16200/26069 | Loss: 0.9753 | LR: 8.59e-05\n",
            "  Batch 16300/26069 | Loss: 0.9753 | LR: 8.59e-05\n",
            "  Batch 16400/26069 | Loss: 0.9755 | LR: 8.58e-05\n",
            "  Batch 16500/26069 | Loss: 0.9754 | LR: 8.58e-05\n",
            "  Batch 16600/26069 | Loss: 0.9753 | LR: 8.58e-05\n",
            "  Batch 16700/26069 | Loss: 0.9753 | LR: 8.58e-05\n",
            "  Batch 16800/26069 | Loss: 0.9753 | LR: 8.58e-05\n",
            "  Batch 16900/26069 | Loss: 0.9754 | LR: 8.58e-05\n",
            "  Batch 17000/26069 | Loss: 0.9755 | LR: 8.58e-05\n",
            "  Batch 17100/26069 | Loss: 0.9755 | LR: 8.58e-05\n",
            "  Batch 17200/26069 | Loss: 0.9755 | LR: 8.58e-05\n",
            "  Batch 17300/26069 | Loss: 0.9754 | LR: 8.58e-05\n",
            "  Batch 17400/26069 | Loss: 0.9753 | LR: 8.58e-05\n",
            "  Batch 17500/26069 | Loss: 0.9753 | LR: 8.58e-05\n",
            "  Batch 17600/26069 | Loss: 0.9752 | LR: 8.58e-05\n",
            "  Batch 17700/26069 | Loss: 0.9752 | LR: 8.58e-05\n",
            "  Batch 17800/26069 | Loss: 0.9752 | LR: 8.58e-05\n",
            "  Batch 17900/26069 | Loss: 0.9752 | LR: 8.58e-05\n",
            "  Batch 18000/26069 | Loss: 0.9751 | LR: 8.58e-05\n",
            "  Batch 18100/26069 | Loss: 0.9751 | LR: 8.58e-05\n",
            "  Batch 18200/26069 | Loss: 0.9751 | LR: 8.58e-05\n",
            "  Batch 18300/26069 | Loss: 0.9750 | LR: 8.58e-05\n",
            "  Batch 18400/26069 | Loss: 0.9751 | LR: 8.58e-05\n",
            "  Batch 18500/26069 | Loss: 0.9751 | LR: 8.58e-05\n",
            "  Batch 18600/26069 | Loss: 0.9752 | LR: 8.58e-05\n",
            "  Batch 18700/26069 | Loss: 0.9751 | LR: 8.58e-05\n",
            "  Batch 18800/26069 | Loss: 0.9751 | LR: 8.57e-05\n",
            "  Batch 18900/26069 | Loss: 0.9751 | LR: 8.57e-05\n",
            "  Batch 19000/26069 | Loss: 0.9751 | LR: 8.57e-05\n",
            "  Batch 19100/26069 | Loss: 0.9751 | LR: 8.57e-05\n",
            "  Batch 19200/26069 | Loss: 0.9751 | LR: 8.57e-05\n",
            "  Batch 19300/26069 | Loss: 0.9750 | LR: 8.57e-05\n",
            "  Batch 19400/26069 | Loss: 0.9749 | LR: 8.57e-05\n",
            "  Batch 19500/26069 | Loss: 0.9749 | LR: 8.57e-05\n",
            "  Batch 19600/26069 | Loss: 0.9749 | LR: 8.57e-05\n",
            "  Batch 19700/26069 | Loss: 0.9749 | LR: 8.57e-05\n",
            "  Batch 19800/26069 | Loss: 0.9749 | LR: 8.57e-05\n",
            "  Batch 19900/26069 | Loss: 0.9748 | LR: 8.57e-05\n",
            "  Batch 20000/26069 | Loss: 0.9748 | LR: 8.57e-05\n",
            "  Batch 20100/26069 | Loss: 0.9748 | LR: 8.57e-05\n",
            "  Batch 20200/26069 | Loss: 0.9749 | LR: 8.57e-05\n",
            "  Batch 20300/26069 | Loss: 0.9748 | LR: 8.57e-05\n",
            "  Batch 20400/26069 | Loss: 0.9748 | LR: 8.57e-05\n",
            "  Batch 20500/26069 | Loss: 0.9747 | LR: 8.57e-05\n",
            "  Batch 20600/26069 | Loss: 0.9746 | LR: 8.57e-05\n",
            "  Batch 20700/26069 | Loss: 0.9745 | LR: 8.57e-05\n",
            "  Batch 20800/26069 | Loss: 0.9745 | LR: 8.57e-05\n",
            "  Batch 20900/26069 | Loss: 0.9746 | LR: 8.57e-05\n",
            "  Batch 21000/26069 | Loss: 0.9746 | LR: 8.57e-05\n",
            "  Batch 21100/26069 | Loss: 0.9745 | LR: 8.57e-05\n",
            "  Batch 21200/26069 | Loss: 0.9745 | LR: 8.56e-05\n",
            "  Batch 21300/26069 | Loss: 0.9745 | LR: 8.56e-05\n",
            "  Batch 21400/26069 | Loss: 0.9743 | LR: 8.56e-05\n",
            "  Batch 21500/26069 | Loss: 0.9743 | LR: 8.56e-05\n",
            "  Batch 21600/26069 | Loss: 0.9743 | LR: 8.56e-05\n",
            "  Batch 21700/26069 | Loss: 0.9743 | LR: 8.56e-05\n",
            "  Batch 21800/26069 | Loss: 0.9743 | LR: 8.56e-05\n",
            "  Batch 21900/26069 | Loss: 0.9744 | LR: 8.56e-05\n",
            "  Batch 22000/26069 | Loss: 0.9744 | LR: 8.56e-05\n",
            "  Batch 22100/26069 | Loss: 0.9744 | LR: 8.56e-05\n",
            "  Batch 22200/26069 | Loss: 0.9744 | LR: 8.56e-05\n",
            "  Batch 22300/26069 | Loss: 0.9744 | LR: 8.56e-05\n",
            "  Batch 22400/26069 | Loss: 0.9745 | LR: 8.56e-05\n",
            "  Batch 22500/26069 | Loss: 0.9746 | LR: 8.56e-05\n",
            "  Batch 22600/26069 | Loss: 0.9746 | LR: 8.56e-05\n",
            "  Batch 22700/26069 | Loss: 0.9747 | LR: 8.56e-05\n",
            "  Batch 22800/26069 | Loss: 0.9747 | LR: 8.56e-05\n",
            "  Batch 22900/26069 | Loss: 0.9747 | LR: 8.56e-05\n",
            "  Batch 23000/26069 | Loss: 0.9748 | LR: 8.56e-05\n",
            "  Batch 23100/26069 | Loss: 0.9747 | LR: 8.56e-05\n",
            "  Batch 23200/26069 | Loss: 0.9747 | LR: 8.56e-05\n",
            "  Batch 23300/26069 | Loss: 0.9747 | LR: 8.56e-05\n",
            "  Batch 23400/26069 | Loss: 0.9747 | LR: 8.56e-05\n",
            "  Batch 23500/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 23600/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 23700/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 23800/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 23900/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 24000/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 24100/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 24200/26069 | Loss: 0.9748 | LR: 8.55e-05\n",
            "  Batch 24300/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 24400/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 24500/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 24600/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 24700/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 24800/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 24900/26069 | Loss: 0.9748 | LR: 8.55e-05\n",
            "  Batch 25000/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 25100/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 25200/26069 | Loss: 0.9747 | LR: 8.55e-05\n",
            "  Batch 25300/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 25400/26069 | Loss: 0.9745 | LR: 8.55e-05\n",
            "  Batch 25500/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 25600/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 25700/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 25800/26069 | Loss: 0.9746 | LR: 8.55e-05\n",
            "  Batch 25900/26069 | Loss: 0.9746 | LR: 8.54e-05\n",
            "  Batch 26000/26069 | Loss: 0.9745 | LR: 8.54e-05\n",
            "\n",
            "  Train Loss: 0.9745\n",
            "  Val Loss:   0.9543 \n",
            "  Time:       313.0s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_25.pt\n",
            "Epoch 26/100\n",
            "  Batch 100/26069 | Loss: 0.9567 | LR: 8.54e-05\n",
            "  Batch 200/26069 | Loss: 0.9646 | LR: 8.54e-05\n",
            "  Batch 300/26069 | Loss: 0.9746 | LR: 8.54e-05\n",
            "  Batch 400/26069 | Loss: 0.9750 | LR: 8.54e-05\n",
            "  Batch 500/26069 | Loss: 0.9773 | LR: 8.54e-05\n",
            "  Batch 600/26069 | Loss: 0.9747 | LR: 8.54e-05\n",
            "  Batch 700/26069 | Loss: 0.9718 | LR: 8.54e-05\n",
            "  Batch 800/26069 | Loss: 0.9722 | LR: 8.54e-05\n",
            "  Batch 900/26069 | Loss: 0.9724 | LR: 8.54e-05\n",
            "  Batch 1000/26069 | Loss: 0.9703 | LR: 8.54e-05\n",
            "  Batch 1100/26069 | Loss: 0.9701 | LR: 8.54e-05\n",
            "  Batch 1200/26069 | Loss: 0.9713 | LR: 8.54e-05\n",
            "  Batch 1300/26069 | Loss: 0.9712 | LR: 8.54e-05\n",
            "  Batch 1400/26069 | Loss: 0.9695 | LR: 8.54e-05\n",
            "  Batch 1500/26069 | Loss: 0.9693 | LR: 8.54e-05\n",
            "  Batch 1600/26069 | Loss: 0.9686 | LR: 8.54e-05\n",
            "  Batch 1700/26069 | Loss: 0.9687 | LR: 8.54e-05\n",
            "  Batch 1800/26069 | Loss: 0.9688 | LR: 8.54e-05\n",
            "  Batch 1900/26069 | Loss: 0.9695 | LR: 8.54e-05\n",
            "  Batch 2000/26069 | Loss: 0.9703 | LR: 8.54e-05\n",
            "  Batch 2100/26069 | Loss: 0.9711 | LR: 8.54e-05\n",
            "  Batch 2200/26069 | Loss: 0.9710 | LR: 8.53e-05\n",
            "  Batch 2300/26069 | Loss: 0.9705 | LR: 8.53e-05\n",
            "  Batch 2400/26069 | Loss: 0.9700 | LR: 8.53e-05\n",
            "  Batch 2500/26069 | Loss: 0.9692 | LR: 8.53e-05\n",
            "  Batch 2600/26069 | Loss: 0.9698 | LR: 8.53e-05\n",
            "  Batch 2700/26069 | Loss: 0.9697 | LR: 8.53e-05\n",
            "  Batch 2800/26069 | Loss: 0.9707 | LR: 8.53e-05\n",
            "  Batch 2900/26069 | Loss: 0.9718 | LR: 8.53e-05\n",
            "  Batch 3000/26069 | Loss: 0.9724 | LR: 8.53e-05\n",
            "  Batch 3100/26069 | Loss: 0.9721 | LR: 8.53e-05\n",
            "  Batch 3200/26069 | Loss: 0.9725 | LR: 8.53e-05\n",
            "  Batch 3300/26069 | Loss: 0.9728 | LR: 8.53e-05\n",
            "  Batch 3400/26069 | Loss: 0.9722 | LR: 8.53e-05\n",
            "  Batch 3500/26069 | Loss: 0.9724 | LR: 8.53e-05\n",
            "  Batch 3600/26069 | Loss: 0.9721 | LR: 8.53e-05\n",
            "  Batch 3700/26069 | Loss: 0.9725 | LR: 8.53e-05\n",
            "  Batch 3800/26069 | Loss: 0.9725 | LR: 8.53e-05\n",
            "  Batch 3900/26069 | Loss: 0.9720 | LR: 8.53e-05\n",
            "  Batch 4000/26069 | Loss: 0.9721 | LR: 8.53e-05\n",
            "  Batch 4100/26069 | Loss: 0.9721 | LR: 8.53e-05\n",
            "  Batch 4200/26069 | Loss: 0.9720 | LR: 8.53e-05\n",
            "  Batch 4300/26069 | Loss: 0.9722 | LR: 8.53e-05\n",
            "  Batch 4400/26069 | Loss: 0.9722 | LR: 8.53e-05\n",
            "  Batch 4500/26069 | Loss: 0.9722 | LR: 8.52e-05\n",
            "  Batch 4600/26069 | Loss: 0.9723 | LR: 8.52e-05\n",
            "  Batch 4700/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 4800/26069 | Loss: 0.9728 | LR: 8.52e-05\n",
            "  Batch 4900/26069 | Loss: 0.9726 | LR: 8.52e-05\n",
            "  Batch 5000/26069 | Loss: 0.9726 | LR: 8.52e-05\n",
            "  Batch 5100/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 5200/26069 | Loss: 0.9728 | LR: 8.52e-05\n",
            "  Batch 5300/26069 | Loss: 0.9726 | LR: 8.52e-05\n",
            "  Batch 5400/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 5500/26069 | Loss: 0.9726 | LR: 8.52e-05\n",
            "  Batch 5600/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 5700/26069 | Loss: 0.9728 | LR: 8.52e-05\n",
            "  Batch 5800/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 5900/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 6000/26069 | Loss: 0.9728 | LR: 8.52e-05\n",
            "  Batch 6100/26069 | Loss: 0.9726 | LR: 8.52e-05\n",
            "  Batch 6200/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 6300/26069 | Loss: 0.9727 | LR: 8.52e-05\n",
            "  Batch 6400/26069 | Loss: 0.9728 | LR: 8.52e-05\n",
            "  Batch 6500/26069 | Loss: 0.9726 | LR: 8.52e-05\n",
            "  Batch 6600/26069 | Loss: 0.9730 | LR: 8.52e-05\n",
            "  Batch 6700/26069 | Loss: 0.9732 | LR: 8.52e-05\n",
            "  Batch 6800/26069 | Loss: 0.9730 | LR: 8.52e-05\n",
            "  Batch 6900/26069 | Loss: 0.9732 | LR: 8.51e-05\n",
            "  Batch 7000/26069 | Loss: 0.9728 | LR: 8.51e-05\n",
            "  Batch 7100/26069 | Loss: 0.9728 | LR: 8.51e-05\n",
            "  Batch 7200/26069 | Loss: 0.9727 | LR: 8.51e-05\n",
            "  Batch 7300/26069 | Loss: 0.9729 | LR: 8.51e-05\n",
            "  Batch 7400/26069 | Loss: 0.9729 | LR: 8.51e-05\n",
            "  Batch 7500/26069 | Loss: 0.9728 | LR: 8.51e-05\n",
            "  Batch 7600/26069 | Loss: 0.9727 | LR: 8.51e-05\n",
            "  Batch 7700/26069 | Loss: 0.9729 | LR: 8.51e-05\n",
            "  Batch 7800/26069 | Loss: 0.9731 | LR: 8.51e-05\n",
            "  Batch 7900/26069 | Loss: 0.9732 | LR: 8.51e-05\n",
            "  Batch 8000/26069 | Loss: 0.9733 | LR: 8.51e-05\n",
            "  Batch 8100/26069 | Loss: 0.9733 | LR: 8.51e-05\n",
            "  Batch 8200/26069 | Loss: 0.9732 | LR: 8.51e-05\n",
            "  Batch 8300/26069 | Loss: 0.9733 | LR: 8.51e-05\n",
            "  Batch 8400/26069 | Loss: 0.9730 | LR: 8.51e-05\n",
            "  Batch 8500/26069 | Loss: 0.9730 | LR: 8.51e-05\n",
            "  Batch 8600/26069 | Loss: 0.9733 | LR: 8.51e-05\n",
            "  Batch 8700/26069 | Loss: 0.9734 | LR: 8.51e-05\n",
            "  Batch 8800/26069 | Loss: 0.9735 | LR: 8.51e-05\n",
            "  Batch 8900/26069 | Loss: 0.9735 | LR: 8.51e-05\n",
            "  Batch 9000/26069 | Loss: 0.9737 | LR: 8.51e-05\n",
            "  Batch 9100/26069 | Loss: 0.9738 | LR: 8.51e-05\n",
            "  Batch 9200/26069 | Loss: 0.9735 | LR: 8.50e-05\n",
            "  Batch 9300/26069 | Loss: 0.9735 | LR: 8.50e-05\n",
            "  Batch 9400/26069 | Loss: 0.9734 | LR: 8.50e-05\n",
            "  Batch 9500/26069 | Loss: 0.9734 | LR: 8.50e-05\n",
            "  Batch 9600/26069 | Loss: 0.9734 | LR: 8.50e-05\n",
            "  Batch 9700/26069 | Loss: 0.9733 | LR: 8.50e-05\n",
            "  Batch 9800/26069 | Loss: 0.9733 | LR: 8.50e-05\n",
            "  Batch 9900/26069 | Loss: 0.9733 | LR: 8.50e-05\n",
            "  Batch 10000/26069 | Loss: 0.9733 | LR: 8.50e-05\n",
            "  Batch 10100/26069 | Loss: 0.9733 | LR: 8.50e-05\n",
            "  Batch 10200/26069 | Loss: 0.9735 | LR: 8.50e-05\n",
            "  Batch 10300/26069 | Loss: 0.9736 | LR: 8.50e-05\n",
            "  Batch 10400/26069 | Loss: 0.9736 | LR: 8.50e-05\n",
            "  Batch 10500/26069 | Loss: 0.9736 | LR: 8.50e-05\n",
            "  Batch 10600/26069 | Loss: 0.9736 | LR: 8.50e-05\n",
            "  Batch 10700/26069 | Loss: 0.9738 | LR: 8.50e-05\n",
            "  Batch 10800/26069 | Loss: 0.9738 | LR: 8.50e-05\n",
            "  Batch 10900/26069 | Loss: 0.9740 | LR: 8.50e-05\n",
            "  Batch 11000/26069 | Loss: 0.9739 | LR: 8.50e-05\n",
            "  Batch 11100/26069 | Loss: 0.9741 | LR: 8.50e-05\n",
            "  Batch 11200/26069 | Loss: 0.9740 | LR: 8.50e-05\n",
            "  Batch 11300/26069 | Loss: 0.9740 | LR: 8.50e-05\n",
            "  Batch 11400/26069 | Loss: 0.9741 | LR: 8.50e-05\n",
            "  Batch 11500/26069 | Loss: 0.9741 | LR: 8.49e-05\n",
            "  Batch 11600/26069 | Loss: 0.9738 | LR: 8.49e-05\n",
            "  Batch 11700/26069 | Loss: 0.9740 | LR: 8.49e-05\n",
            "  Batch 11800/26069 | Loss: 0.9738 | LR: 8.49e-05\n",
            "  Batch 11900/26069 | Loss: 0.9737 | LR: 8.49e-05\n",
            "  Batch 12000/26069 | Loss: 0.9739 | LR: 8.49e-05\n",
            "  Batch 12100/26069 | Loss: 0.9741 | LR: 8.49e-05\n",
            "  Batch 12200/26069 | Loss: 0.9741 | LR: 8.49e-05\n",
            "  Batch 12300/26069 | Loss: 0.9742 | LR: 8.49e-05\n",
            "  Batch 12400/26069 | Loss: 0.9742 | LR: 8.49e-05\n",
            "  Batch 12500/26069 | Loss: 0.9741 | LR: 8.49e-05\n",
            "  Batch 12600/26069 | Loss: 0.9740 | LR: 8.49e-05\n",
            "  Batch 12700/26069 | Loss: 0.9741 | LR: 8.49e-05\n",
            "  Batch 12800/26069 | Loss: 0.9741 | LR: 8.49e-05\n",
            "  Batch 12900/26069 | Loss: 0.9740 | LR: 8.49e-05\n",
            "  Batch 13000/26069 | Loss: 0.9742 | LR: 8.49e-05\n",
            "  Batch 13100/26069 | Loss: 0.9740 | LR: 8.49e-05\n",
            "  Batch 13200/26069 | Loss: 0.9739 | LR: 8.49e-05\n",
            "  Batch 13300/26069 | Loss: 0.9737 | LR: 8.49e-05\n",
            "  Batch 13400/26069 | Loss: 0.9736 | LR: 8.49e-05\n",
            "  Batch 13500/26069 | Loss: 0.9735 | LR: 8.49e-05\n",
            "  Batch 13600/26069 | Loss: 0.9736 | LR: 8.49e-05\n",
            "  Batch 13700/26069 | Loss: 0.9735 | LR: 8.49e-05\n",
            "  Batch 13800/26069 | Loss: 0.9735 | LR: 8.48e-05\n",
            "  Batch 13900/26069 | Loss: 0.9735 | LR: 8.48e-05\n",
            "  Batch 14000/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 14100/26069 | Loss: 0.9733 | LR: 8.48e-05\n",
            "  Batch 14200/26069 | Loss: 0.9732 | LR: 8.48e-05\n",
            "  Batch 14300/26069 | Loss: 0.9733 | LR: 8.48e-05\n",
            "  Batch 14400/26069 | Loss: 0.9733 | LR: 8.48e-05\n",
            "  Batch 14500/26069 | Loss: 0.9733 | LR: 8.48e-05\n",
            "  Batch 14600/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 14700/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 14800/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 14900/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 15000/26069 | Loss: 0.9735 | LR: 8.48e-05\n",
            "  Batch 15100/26069 | Loss: 0.9735 | LR: 8.48e-05\n",
            "  Batch 15200/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 15300/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 15400/26069 | Loss: 0.9735 | LR: 8.48e-05\n",
            "  Batch 15500/26069 | Loss: 0.9735 | LR: 8.48e-05\n",
            "  Batch 15600/26069 | Loss: 0.9733 | LR: 8.48e-05\n",
            "  Batch 15700/26069 | Loss: 0.9733 | LR: 8.48e-05\n",
            "  Batch 15800/26069 | Loss: 0.9733 | LR: 8.48e-05\n",
            "  Batch 15900/26069 | Loss: 0.9734 | LR: 8.48e-05\n",
            "  Batch 16000/26069 | Loss: 0.9735 | LR: 8.48e-05\n",
            "  Batch 16100/26069 | Loss: 0.9735 | LR: 8.47e-05\n",
            "  Batch 16200/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 16300/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 16400/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 16500/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 16600/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 16700/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 16800/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 16900/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 17000/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 17100/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 17200/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 17300/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 17400/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 17500/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 17600/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 17700/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 17800/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 17900/26069 | Loss: 0.9738 | LR: 8.47e-05\n",
            "  Batch 18000/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 18100/26069 | Loss: 0.9737 | LR: 8.47e-05\n",
            "  Batch 18200/26069 | Loss: 0.9738 | LR: 8.47e-05\n",
            "  Batch 18300/26069 | Loss: 0.9736 | LR: 8.47e-05\n",
            "  Batch 18400/26069 | Loss: 0.9735 | LR: 8.46e-05\n",
            "  Batch 18500/26069 | Loss: 0.9735 | LR: 8.46e-05\n",
            "  Batch 18600/26069 | Loss: 0.9735 | LR: 8.46e-05\n",
            "  Batch 18700/26069 | Loss: 0.9736 | LR: 8.46e-05\n",
            "  Batch 18800/26069 | Loss: 0.9738 | LR: 8.46e-05\n",
            "  Batch 18900/26069 | Loss: 0.9739 | LR: 8.46e-05\n",
            "  Batch 19000/26069 | Loss: 0.9738 | LR: 8.46e-05\n",
            "  Batch 19100/26069 | Loss: 0.9737 | LR: 8.46e-05\n",
            "  Batch 19200/26069 | Loss: 0.9738 | LR: 8.46e-05\n",
            "  Batch 19300/26069 | Loss: 0.9738 | LR: 8.46e-05\n",
            "  Batch 19400/26069 | Loss: 0.9737 | LR: 8.46e-05\n",
            "  Batch 19500/26069 | Loss: 0.9736 | LR: 8.46e-05\n",
            "  Batch 19600/26069 | Loss: 0.9735 | LR: 8.46e-05\n",
            "  Batch 19700/26069 | Loss: 0.9736 | LR: 8.46e-05\n",
            "  Batch 19800/26069 | Loss: 0.9736 | LR: 8.46e-05\n",
            "  Batch 19900/26069 | Loss: 0.9735 | LR: 8.46e-05\n",
            "  Batch 20000/26069 | Loss: 0.9736 | LR: 8.46e-05\n",
            "  Batch 20100/26069 | Loss: 0.9737 | LR: 8.46e-05\n",
            "  Batch 20200/26069 | Loss: 0.9737 | LR: 8.46e-05\n",
            "  Batch 20300/26069 | Loss: 0.9737 | LR: 8.46e-05\n",
            "  Batch 20400/26069 | Loss: 0.9737 | LR: 8.46e-05\n",
            "  Batch 20500/26069 | Loss: 0.9737 | LR: 8.46e-05\n",
            "  Batch 20600/26069 | Loss: 0.9736 | LR: 8.46e-05\n",
            "  Batch 20700/26069 | Loss: 0.9736 | LR: 8.45e-05\n",
            "  Batch 20800/26069 | Loss: 0.9737 | LR: 8.45e-05\n",
            "  Batch 20900/26069 | Loss: 0.9737 | LR: 8.45e-05\n",
            "  Batch 21000/26069 | Loss: 0.9737 | LR: 8.45e-05\n",
            "  Batch 21100/26069 | Loss: 0.9737 | LR: 8.45e-05\n",
            "  Batch 21200/26069 | Loss: 0.9737 | LR: 8.45e-05\n",
            "  Batch 21300/26069 | Loss: 0.9736 | LR: 8.45e-05\n",
            "  Batch 21400/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 21500/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 21600/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 21700/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 21800/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 21900/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 22000/26069 | Loss: 0.9734 | LR: 8.45e-05\n",
            "  Batch 22100/26069 | Loss: 0.9733 | LR: 8.45e-05\n",
            "  Batch 22200/26069 | Loss: 0.9734 | LR: 8.45e-05\n",
            "  Batch 22300/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 22400/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 22500/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 22600/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 22700/26069 | Loss: 0.9735 | LR: 8.45e-05\n",
            "  Batch 22800/26069 | Loss: 0.9736 | LR: 8.45e-05\n",
            "  Batch 22900/26069 | Loss: 0.9736 | LR: 8.45e-05\n",
            "  Batch 23000/26069 | Loss: 0.9736 | LR: 8.44e-05\n",
            "  Batch 23100/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 23200/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 23300/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 23400/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 23500/26069 | Loss: 0.9737 | LR: 8.44e-05\n",
            "  Batch 23600/26069 | Loss: 0.9736 | LR: 8.44e-05\n",
            "  Batch 23700/26069 | Loss: 0.9736 | LR: 8.44e-05\n",
            "  Batch 23800/26069 | Loss: 0.9736 | LR: 8.44e-05\n",
            "  Batch 23900/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 24000/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 24100/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 24200/26069 | Loss: 0.9734 | LR: 8.44e-05\n",
            "  Batch 24300/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 24400/26069 | Loss: 0.9735 | LR: 8.44e-05\n",
            "  Batch 24500/26069 | Loss: 0.9736 | LR: 8.44e-05\n",
            "  Batch 24600/26069 | Loss: 0.9737 | LR: 8.44e-05\n",
            "  Batch 24700/26069 | Loss: 0.9738 | LR: 8.44e-05\n",
            "  Batch 24800/26069 | Loss: 0.9738 | LR: 8.44e-05\n",
            "  Batch 24900/26069 | Loss: 0.9738 | LR: 8.44e-05\n",
            "  Batch 25000/26069 | Loss: 0.9739 | LR: 8.44e-05\n",
            "  Batch 25100/26069 | Loss: 0.9739 | LR: 8.44e-05\n",
            "  Batch 25200/26069 | Loss: 0.9739 | LR: 8.44e-05\n",
            "  Batch 25300/26069 | Loss: 0.9739 | LR: 8.43e-05\n",
            "  Batch 25400/26069 | Loss: 0.9738 | LR: 8.43e-05\n",
            "  Batch 25500/26069 | Loss: 0.9738 | LR: 8.43e-05\n",
            "  Batch 25600/26069 | Loss: 0.9738 | LR: 8.43e-05\n",
            "  Batch 25700/26069 | Loss: 0.9739 | LR: 8.43e-05\n",
            "  Batch 25800/26069 | Loss: 0.9739 | LR: 8.43e-05\n",
            "  Batch 25900/26069 | Loss: 0.9739 | LR: 8.43e-05\n",
            "  Batch 26000/26069 | Loss: 0.9739 | LR: 8.43e-05\n",
            "\n",
            "  Train Loss: 0.9738\n",
            "  Val Loss:   0.9550 \n",
            "  Time:       312.5s\n",
            "\n",
            "Epoch 27/100\n",
            "  Batch 100/26069 | Loss: 0.9754 | LR: 8.43e-05\n",
            "  Batch 200/26069 | Loss: 0.9707 | LR: 8.43e-05\n",
            "  Batch 300/26069 | Loss: 0.9736 | LR: 8.43e-05\n",
            "  Batch 400/26069 | Loss: 0.9781 | LR: 8.43e-05\n",
            "  Batch 500/26069 | Loss: 0.9786 | LR: 8.43e-05\n",
            "  Batch 600/26069 | Loss: 0.9802 | LR: 8.43e-05\n",
            "  Batch 700/26069 | Loss: 0.9779 | LR: 8.43e-05\n",
            "  Batch 800/26069 | Loss: 0.9771 | LR: 8.43e-05\n",
            "  Batch 900/26069 | Loss: 0.9778 | LR: 8.43e-05\n",
            "  Batch 1000/26069 | Loss: 0.9779 | LR: 8.43e-05\n",
            "  Batch 1100/26069 | Loss: 0.9782 | LR: 8.43e-05\n",
            "  Batch 1200/26069 | Loss: 0.9764 | LR: 8.43e-05\n",
            "  Batch 1300/26069 | Loss: 0.9749 | LR: 8.43e-05\n",
            "  Batch 1400/26069 | Loss: 0.9758 | LR: 8.43e-05\n",
            "  Batch 1500/26069 | Loss: 0.9756 | LR: 8.42e-05\n",
            "  Batch 1600/26069 | Loss: 0.9750 | LR: 8.42e-05\n",
            "  Batch 1700/26069 | Loss: 0.9740 | LR: 8.42e-05\n",
            "  Batch 1800/26069 | Loss: 0.9735 | LR: 8.42e-05\n",
            "  Batch 1900/26069 | Loss: 0.9737 | LR: 8.42e-05\n",
            "  Batch 2000/26069 | Loss: 0.9732 | LR: 8.42e-05\n",
            "  Batch 2100/26069 | Loss: 0.9729 | LR: 8.42e-05\n",
            "  Batch 2200/26069 | Loss: 0.9717 | LR: 8.42e-05\n",
            "  Batch 2300/26069 | Loss: 0.9718 | LR: 8.42e-05\n",
            "  Batch 2400/26069 | Loss: 0.9719 | LR: 8.42e-05\n",
            "  Batch 2500/26069 | Loss: 0.9720 | LR: 8.42e-05\n",
            "  Batch 2600/26069 | Loss: 0.9727 | LR: 8.42e-05\n",
            "  Batch 2700/26069 | Loss: 0.9718 | LR: 8.42e-05\n",
            "  Batch 2800/26069 | Loss: 0.9724 | LR: 8.42e-05\n",
            "  Batch 2900/26069 | Loss: 0.9724 | LR: 8.42e-05\n",
            "  Batch 3000/26069 | Loss: 0.9722 | LR: 8.42e-05\n",
            "  Batch 3100/26069 | Loss: 0.9720 | LR: 8.42e-05\n",
            "  Batch 3200/26069 | Loss: 0.9722 | LR: 8.42e-05\n",
            "  Batch 3300/26069 | Loss: 0.9714 | LR: 8.42e-05\n",
            "  Batch 3400/26069 | Loss: 0.9716 | LR: 8.42e-05\n",
            "  Batch 3500/26069 | Loss: 0.9716 | LR: 8.42e-05\n",
            "  Batch 3600/26069 | Loss: 0.9715 | LR: 8.42e-05\n",
            "  Batch 3700/26069 | Loss: 0.9715 | LR: 8.42e-05\n",
            "  Batch 3800/26069 | Loss: 0.9712 | LR: 8.41e-05\n",
            "  Batch 3900/26069 | Loss: 0.9711 | LR: 8.41e-05\n",
            "  Batch 4000/26069 | Loss: 0.9709 | LR: 8.41e-05\n",
            "  Batch 4100/26069 | Loss: 0.9702 | LR: 8.41e-05\n",
            "  Batch 4200/26069 | Loss: 0.9702 | LR: 8.41e-05\n",
            "  Batch 4300/26069 | Loss: 0.9702 | LR: 8.41e-05\n",
            "  Batch 4400/26069 | Loss: 0.9701 | LR: 8.41e-05\n",
            "  Batch 4500/26069 | Loss: 0.9703 | LR: 8.41e-05\n",
            "  Batch 4600/26069 | Loss: 0.9702 | LR: 8.41e-05\n",
            "  Batch 4700/26069 | Loss: 0.9705 | LR: 8.41e-05\n",
            "  Batch 4800/26069 | Loss: 0.9705 | LR: 8.41e-05\n",
            "  Batch 4900/26069 | Loss: 0.9702 | LR: 8.41e-05\n",
            "  Batch 5000/26069 | Loss: 0.9697 | LR: 8.41e-05\n",
            "  Batch 5100/26069 | Loss: 0.9695 | LR: 8.41e-05\n",
            "  Batch 5200/26069 | Loss: 0.9696 | LR: 8.41e-05\n",
            "  Batch 5300/26069 | Loss: 0.9696 | LR: 8.41e-05\n",
            "  Batch 5400/26069 | Loss: 0.9699 | LR: 8.41e-05\n",
            "  Batch 5500/26069 | Loss: 0.9697 | LR: 8.41e-05\n",
            "  Batch 5600/26069 | Loss: 0.9698 | LR: 8.41e-05\n",
            "  Batch 5700/26069 | Loss: 0.9702 | LR: 8.41e-05\n",
            "  Batch 5800/26069 | Loss: 0.9702 | LR: 8.41e-05\n",
            "  Batch 5900/26069 | Loss: 0.9704 | LR: 8.41e-05\n",
            "  Batch 6000/26069 | Loss: 0.9707 | LR: 8.41e-05\n",
            "  Batch 6100/26069 | Loss: 0.9710 | LR: 8.40e-05\n",
            "  Batch 6200/26069 | Loss: 0.9711 | LR: 8.40e-05\n",
            "  Batch 6300/26069 | Loss: 0.9711 | LR: 8.40e-05\n",
            "  Batch 6400/26069 | Loss: 0.9707 | LR: 8.40e-05\n",
            "  Batch 6500/26069 | Loss: 0.9710 | LR: 8.40e-05\n",
            "  Batch 6600/26069 | Loss: 0.9708 | LR: 8.40e-05\n",
            "  Batch 6700/26069 | Loss: 0.9707 | LR: 8.40e-05\n",
            "  Batch 6800/26069 | Loss: 0.9707 | LR: 8.40e-05\n",
            "  Batch 6900/26069 | Loss: 0.9707 | LR: 8.40e-05\n",
            "  Batch 7000/26069 | Loss: 0.9711 | LR: 8.40e-05\n",
            "  Batch 7100/26069 | Loss: 0.9709 | LR: 8.40e-05\n",
            "  Batch 7200/26069 | Loss: 0.9711 | LR: 8.40e-05\n",
            "  Batch 7300/26069 | Loss: 0.9711 | LR: 8.40e-05\n",
            "  Batch 7400/26069 | Loss: 0.9710 | LR: 8.40e-05\n",
            "  Batch 7500/26069 | Loss: 0.9707 | LR: 8.40e-05\n",
            "  Batch 7600/26069 | Loss: 0.9709 | LR: 8.40e-05\n",
            "  Batch 7700/26069 | Loss: 0.9707 | LR: 8.40e-05\n",
            "  Batch 7800/26069 | Loss: 0.9707 | LR: 8.40e-05\n",
            "  Batch 7900/26069 | Loss: 0.9705 | LR: 8.40e-05\n",
            "  Batch 8000/26069 | Loss: 0.9706 | LR: 8.40e-05\n",
            "  Batch 8100/26069 | Loss: 0.9708 | LR: 8.40e-05\n",
            "  Batch 8200/26069 | Loss: 0.9706 | LR: 8.40e-05\n",
            "  Batch 8300/26069 | Loss: 0.9707 | LR: 8.39e-05\n",
            "  Batch 8400/26069 | Loss: 0.9707 | LR: 8.39e-05\n",
            "  Batch 8500/26069 | Loss: 0.9709 | LR: 8.39e-05\n",
            "  Batch 8600/26069 | Loss: 0.9708 | LR: 8.39e-05\n",
            "  Batch 8700/26069 | Loss: 0.9706 | LR: 8.39e-05\n",
            "  Batch 8800/26069 | Loss: 0.9708 | LR: 8.39e-05\n",
            "  Batch 8900/26069 | Loss: 0.9706 | LR: 8.39e-05\n",
            "  Batch 9000/26069 | Loss: 0.9703 | LR: 8.39e-05\n",
            "  Batch 9100/26069 | Loss: 0.9702 | LR: 8.39e-05\n",
            "  Batch 9200/26069 | Loss: 0.9702 | LR: 8.39e-05\n",
            "  Batch 9300/26069 | Loss: 0.9703 | LR: 8.39e-05\n",
            "  Batch 9400/26069 | Loss: 0.9702 | LR: 8.39e-05\n",
            "  Batch 9500/26069 | Loss: 0.9702 | LR: 8.39e-05\n",
            "  Batch 9600/26069 | Loss: 0.9703 | LR: 8.39e-05\n",
            "  Batch 9700/26069 | Loss: 0.9703 | LR: 8.39e-05\n",
            "  Batch 9800/26069 | Loss: 0.9703 | LR: 8.39e-05\n",
            "  Batch 9900/26069 | Loss: 0.9702 | LR: 8.39e-05\n",
            "  Batch 10000/26069 | Loss: 0.9704 | LR: 8.39e-05\n",
            "  Batch 10100/26069 | Loss: 0.9704 | LR: 8.39e-05\n",
            "  Batch 10200/26069 | Loss: 0.9705 | LR: 8.39e-05\n",
            "  Batch 10300/26069 | Loss: 0.9707 | LR: 8.39e-05\n",
            "  Batch 10400/26069 | Loss: 0.9708 | LR: 8.39e-05\n",
            "  Batch 10500/26069 | Loss: 0.9708 | LR: 8.39e-05\n",
            "  Batch 10600/26069 | Loss: 0.9708 | LR: 8.38e-05\n",
            "  Batch 10700/26069 | Loss: 0.9709 | LR: 8.38e-05\n",
            "  Batch 10800/26069 | Loss: 0.9710 | LR: 8.38e-05\n",
            "  Batch 10900/26069 | Loss: 0.9712 | LR: 8.38e-05\n",
            "  Batch 11000/26069 | Loss: 0.9712 | LR: 8.38e-05\n",
            "  Batch 11100/26069 | Loss: 0.9712 | LR: 8.38e-05\n",
            "  Batch 11200/26069 | Loss: 0.9712 | LR: 8.38e-05\n",
            "  Batch 11300/26069 | Loss: 0.9712 | LR: 8.38e-05\n",
            "  Batch 11400/26069 | Loss: 0.9714 | LR: 8.38e-05\n",
            "  Batch 11500/26069 | Loss: 0.9713 | LR: 8.38e-05\n",
            "  Batch 11600/26069 | Loss: 0.9716 | LR: 8.38e-05\n",
            "  Batch 11700/26069 | Loss: 0.9716 | LR: 8.38e-05\n",
            "  Batch 11800/26069 | Loss: 0.9717 | LR: 8.38e-05\n",
            "  Batch 11900/26069 | Loss: 0.9717 | LR: 8.38e-05\n",
            "  Batch 12000/26069 | Loss: 0.9718 | LR: 8.38e-05\n",
            "  Batch 12100/26069 | Loss: 0.9719 | LR: 8.38e-05\n",
            "  Batch 12200/26069 | Loss: 0.9719 | LR: 8.38e-05\n",
            "  Batch 12300/26069 | Loss: 0.9719 | LR: 8.38e-05\n",
            "  Batch 12400/26069 | Loss: 0.9719 | LR: 8.38e-05\n",
            "  Batch 12500/26069 | Loss: 0.9717 | LR: 8.38e-05\n",
            "  Batch 12600/26069 | Loss: 0.9719 | LR: 8.38e-05\n",
            "  Batch 12700/26069 | Loss: 0.9720 | LR: 8.38e-05\n",
            "  Batch 12800/26069 | Loss: 0.9718 | LR: 8.38e-05\n",
            "  Batch 12900/26069 | Loss: 0.9717 | LR: 8.37e-05\n",
            "  Batch 13000/26069 | Loss: 0.9717 | LR: 8.37e-05\n",
            "  Batch 13100/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 13200/26069 | Loss: 0.9721 | LR: 8.37e-05\n",
            "  Batch 13300/26069 | Loss: 0.9722 | LR: 8.37e-05\n",
            "  Batch 13400/26069 | Loss: 0.9721 | LR: 8.37e-05\n",
            "  Batch 13500/26069 | Loss: 0.9719 | LR: 8.37e-05\n",
            "  Batch 13600/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 13700/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 13800/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 13900/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 14000/26069 | Loss: 0.9718 | LR: 8.37e-05\n",
            "  Batch 14100/26069 | Loss: 0.9718 | LR: 8.37e-05\n",
            "  Batch 14200/26069 | Loss: 0.9718 | LR: 8.37e-05\n",
            "  Batch 14300/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 14400/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 14500/26069 | Loss: 0.9720 | LR: 8.37e-05\n",
            "  Batch 14600/26069 | Loss: 0.9721 | LR: 8.37e-05\n",
            "  Batch 14700/26069 | Loss: 0.9722 | LR: 8.37e-05\n",
            "  Batch 14800/26069 | Loss: 0.9724 | LR: 8.37e-05\n",
            "  Batch 14900/26069 | Loss: 0.9724 | LR: 8.37e-05\n",
            "  Batch 15000/26069 | Loss: 0.9722 | LR: 8.37e-05\n",
            "  Batch 15100/26069 | Loss: 0.9722 | LR: 8.36e-05\n",
            "  Batch 15200/26069 | Loss: 0.9722 | LR: 8.36e-05\n",
            "  Batch 15300/26069 | Loss: 0.9724 | LR: 8.36e-05\n",
            "  Batch 15400/26069 | Loss: 0.9724 | LR: 8.36e-05\n",
            "  Batch 15500/26069 | Loss: 0.9723 | LR: 8.36e-05\n",
            "  Batch 15600/26069 | Loss: 0.9723 | LR: 8.36e-05\n",
            "  Batch 15700/26069 | Loss: 0.9723 | LR: 8.36e-05\n",
            "  Batch 15800/26069 | Loss: 0.9723 | LR: 8.36e-05\n",
            "  Batch 15900/26069 | Loss: 0.9724 | LR: 8.36e-05\n",
            "  Batch 16000/26069 | Loss: 0.9725 | LR: 8.36e-05\n",
            "  Batch 16100/26069 | Loss: 0.9725 | LR: 8.36e-05\n",
            "  Batch 16200/26069 | Loss: 0.9724 | LR: 8.36e-05\n",
            "  Batch 16300/26069 | Loss: 0.9725 | LR: 8.36e-05\n",
            "  Batch 16400/26069 | Loss: 0.9726 | LR: 8.36e-05\n",
            "  Batch 16500/26069 | Loss: 0.9726 | LR: 8.36e-05\n",
            "  Batch 16600/26069 | Loss: 0.9726 | LR: 8.36e-05\n",
            "  Batch 16700/26069 | Loss: 0.9727 | LR: 8.36e-05\n",
            "  Batch 16800/26069 | Loss: 0.9727 | LR: 8.36e-05\n",
            "  Batch 16900/26069 | Loss: 0.9726 | LR: 8.36e-05\n",
            "  Batch 17000/26069 | Loss: 0.9726 | LR: 8.36e-05\n",
            "  Batch 17100/26069 | Loss: 0.9726 | LR: 8.36e-05\n",
            "  Batch 17200/26069 | Loss: 0.9726 | LR: 8.36e-05\n",
            "  Batch 17300/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 17400/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 17500/26069 | Loss: 0.9725 | LR: 8.35e-05\n",
            "  Batch 17600/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 17700/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 17800/26069 | Loss: 0.9727 | LR: 8.35e-05\n",
            "  Batch 17900/26069 | Loss: 0.9727 | LR: 8.35e-05\n",
            "  Batch 18000/26069 | Loss: 0.9727 | LR: 8.35e-05\n",
            "  Batch 18100/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 18200/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 18300/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 18400/26069 | Loss: 0.9727 | LR: 8.35e-05\n",
            "  Batch 18500/26069 | Loss: 0.9728 | LR: 8.35e-05\n",
            "  Batch 18600/26069 | Loss: 0.9728 | LR: 8.35e-05\n",
            "  Batch 18700/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 18800/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 18900/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 19000/26069 | Loss: 0.9725 | LR: 8.35e-05\n",
            "  Batch 19100/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 19200/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 19300/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 19400/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 19500/26069 | Loss: 0.9726 | LR: 8.35e-05\n",
            "  Batch 19600/26069 | Loss: 0.9726 | LR: 8.34e-05\n",
            "  Batch 19700/26069 | Loss: 0.9725 | LR: 8.34e-05\n",
            "  Batch 19800/26069 | Loss: 0.9725 | LR: 8.34e-05\n",
            "  Batch 19900/26069 | Loss: 0.9725 | LR: 8.34e-05\n",
            "  Batch 20000/26069 | Loss: 0.9725 | LR: 8.34e-05\n",
            "  Batch 20100/26069 | Loss: 0.9724 | LR: 8.34e-05\n",
            "  Batch 20200/26069 | Loss: 0.9724 | LR: 8.34e-05\n",
            "  Batch 20300/26069 | Loss: 0.9723 | LR: 8.34e-05\n",
            "  Batch 20400/26069 | Loss: 0.9722 | LR: 8.34e-05\n",
            "  Batch 20500/26069 | Loss: 0.9722 | LR: 8.34e-05\n",
            "  Batch 20600/26069 | Loss: 0.9722 | LR: 8.34e-05\n",
            "  Batch 20700/26069 | Loss: 0.9721 | LR: 8.34e-05\n",
            "  Batch 20800/26069 | Loss: 0.9722 | LR: 8.34e-05\n",
            "  Batch 20900/26069 | Loss: 0.9723 | LR: 8.34e-05\n",
            "  Batch 21000/26069 | Loss: 0.9722 | LR: 8.34e-05\n",
            "  Batch 21100/26069 | Loss: 0.9722 | LR: 8.34e-05\n",
            "  Batch 21200/26069 | Loss: 0.9723 | LR: 8.34e-05\n",
            "  Batch 21300/26069 | Loss: 0.9724 | LR: 8.34e-05\n",
            "  Batch 21400/26069 | Loss: 0.9724 | LR: 8.34e-05\n",
            "  Batch 21500/26069 | Loss: 0.9724 | LR: 8.34e-05\n",
            "  Batch 21600/26069 | Loss: 0.9724 | LR: 8.34e-05\n",
            "  Batch 21700/26069 | Loss: 0.9725 | LR: 8.34e-05\n",
            "  Batch 21800/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 21900/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22000/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22100/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22200/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22300/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22400/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22500/26069 | Loss: 0.9726 | LR: 8.33e-05\n",
            "  Batch 22600/26069 | Loss: 0.9726 | LR: 8.33e-05\n",
            "  Batch 22700/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22800/26069 | Loss: 0.9725 | LR: 8.33e-05\n",
            "  Batch 22900/26069 | Loss: 0.9726 | LR: 8.33e-05\n",
            "  Batch 23000/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23100/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23200/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23300/26069 | Loss: 0.9726 | LR: 8.33e-05\n",
            "  Batch 23400/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23500/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23600/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23700/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23800/26069 | Loss: 0.9727 | LR: 8.33e-05\n",
            "  Batch 23900/26069 | Loss: 0.9728 | LR: 8.33e-05\n",
            "  Batch 24000/26069 | Loss: 0.9727 | LR: 8.32e-05\n",
            "  Batch 24100/26069 | Loss: 0.9728 | LR: 8.32e-05\n",
            "  Batch 24200/26069 | Loss: 0.9728 | LR: 8.32e-05\n",
            "  Batch 24300/26069 | Loss: 0.9727 | LR: 8.32e-05\n",
            "  Batch 24400/26069 | Loss: 0.9727 | LR: 8.32e-05\n",
            "  Batch 24500/26069 | Loss: 0.9728 | LR: 8.32e-05\n",
            "  Batch 24600/26069 | Loss: 0.9728 | LR: 8.32e-05\n",
            "  Batch 24700/26069 | Loss: 0.9728 | LR: 8.32e-05\n",
            "  Batch 24800/26069 | Loss: 0.9729 | LR: 8.32e-05\n",
            "  Batch 24900/26069 | Loss: 0.9729 | LR: 8.32e-05\n",
            "  Batch 25000/26069 | Loss: 0.9730 | LR: 8.32e-05\n",
            "  Batch 25100/26069 | Loss: 0.9730 | LR: 8.32e-05\n",
            "  Batch 25200/26069 | Loss: 0.9730 | LR: 8.32e-05\n",
            "  Batch 25300/26069 | Loss: 0.9731 | LR: 8.32e-05\n",
            "  Batch 25400/26069 | Loss: 0.9730 | LR: 8.32e-05\n",
            "  Batch 25500/26069 | Loss: 0.9731 | LR: 8.32e-05\n",
            "  Batch 25600/26069 | Loss: 0.9731 | LR: 8.32e-05\n",
            "  Batch 25700/26069 | Loss: 0.9731 | LR: 8.32e-05\n",
            "  Batch 25800/26069 | Loss: 0.9731 | LR: 8.32e-05\n",
            "  Batch 25900/26069 | Loss: 0.9731 | LR: 8.32e-05\n",
            "  Batch 26000/26069 | Loss: 0.9730 | LR: 8.32e-05\n",
            "\n",
            "  Train Loss: 0.9731\n",
            "  Val Loss:   0.9535 [BEST]\n",
            "  Time:       312.0s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_27.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 28/100\n",
            "  Batch 100/26069 | Loss: 0.9716 | LR: 8.32e-05\n",
            "  Batch 200/26069 | Loss: 0.9678 | LR: 8.31e-05\n",
            "  Batch 300/26069 | Loss: 0.9626 | LR: 8.31e-05\n",
            "  Batch 400/26069 | Loss: 0.9744 | LR: 8.31e-05\n",
            "  Batch 500/26069 | Loss: 0.9772 | LR: 8.31e-05\n",
            "  Batch 600/26069 | Loss: 0.9764 | LR: 8.31e-05\n",
            "  Batch 700/26069 | Loss: 0.9788 | LR: 8.31e-05\n",
            "  Batch 800/26069 | Loss: 0.9770 | LR: 8.31e-05\n",
            "  Batch 900/26069 | Loss: 0.9772 | LR: 8.31e-05\n",
            "  Batch 1000/26069 | Loss: 0.9788 | LR: 8.31e-05\n",
            "  Batch 1100/26069 | Loss: 0.9769 | LR: 8.31e-05\n",
            "  Batch 1200/26069 | Loss: 0.9774 | LR: 8.31e-05\n",
            "  Batch 1300/26069 | Loss: 0.9761 | LR: 8.31e-05\n",
            "  Batch 1400/26069 | Loss: 0.9748 | LR: 8.31e-05\n",
            "  Batch 1500/26069 | Loss: 0.9737 | LR: 8.31e-05\n",
            "  Batch 1600/26069 | Loss: 0.9734 | LR: 8.31e-05\n",
            "  Batch 1700/26069 | Loss: 0.9736 | LR: 8.31e-05\n",
            "  Batch 1800/26069 | Loss: 0.9735 | LR: 8.31e-05\n",
            "  Batch 1900/26069 | Loss: 0.9730 | LR: 8.31e-05\n",
            "  Batch 2000/26069 | Loss: 0.9723 | LR: 8.31e-05\n",
            "  Batch 2100/26069 | Loss: 0.9724 | LR: 8.31e-05\n",
            "  Batch 2200/26069 | Loss: 0.9724 | LR: 8.31e-05\n",
            "  Batch 2300/26069 | Loss: 0.9720 | LR: 8.31e-05\n",
            "  Batch 2400/26069 | Loss: 0.9716 | LR: 8.30e-05\n",
            "  Batch 2500/26069 | Loss: 0.9708 | LR: 8.30e-05\n",
            "  Batch 2600/26069 | Loss: 0.9705 | LR: 8.30e-05\n",
            "  Batch 2700/26069 | Loss: 0.9701 | LR: 8.30e-05\n",
            "  Batch 2800/26069 | Loss: 0.9698 | LR: 8.30e-05\n",
            "  Batch 2900/26069 | Loss: 0.9699 | LR: 8.30e-05\n",
            "  Batch 3000/26069 | Loss: 0.9700 | LR: 8.30e-05\n",
            "  Batch 3100/26069 | Loss: 0.9704 | LR: 8.30e-05\n",
            "  Batch 3200/26069 | Loss: 0.9705 | LR: 8.30e-05\n",
            "  Batch 3300/26069 | Loss: 0.9702 | LR: 8.30e-05\n",
            "  Batch 3400/26069 | Loss: 0.9701 | LR: 8.30e-05\n",
            "  Batch 3500/26069 | Loss: 0.9694 | LR: 8.30e-05\n",
            "  Batch 3600/26069 | Loss: 0.9688 | LR: 8.30e-05\n",
            "  Batch 3700/26069 | Loss: 0.9692 | LR: 8.30e-05\n",
            "  Batch 3800/26069 | Loss: 0.9697 | LR: 8.30e-05\n",
            "  Batch 3900/26069 | Loss: 0.9693 | LR: 8.30e-05\n",
            "  Batch 4000/26069 | Loss: 0.9694 | LR: 8.30e-05\n",
            "  Batch 4100/26069 | Loss: 0.9699 | LR: 8.30e-05\n",
            "  Batch 4200/26069 | Loss: 0.9700 | LR: 8.30e-05\n",
            "  Batch 4300/26069 | Loss: 0.9704 | LR: 8.30e-05\n",
            "  Batch 4400/26069 | Loss: 0.9701 | LR: 8.30e-05\n",
            "  Batch 4500/26069 | Loss: 0.9707 | LR: 8.30e-05\n",
            "  Batch 4600/26069 | Loss: 0.9705 | LR: 8.29e-05\n",
            "  Batch 4700/26069 | Loss: 0.9710 | LR: 8.29e-05\n",
            "  Batch 4800/26069 | Loss: 0.9717 | LR: 8.29e-05\n",
            "  Batch 4900/26069 | Loss: 0.9715 | LR: 8.29e-05\n",
            "  Batch 5000/26069 | Loss: 0.9713 | LR: 8.29e-05\n",
            "  Batch 5100/26069 | Loss: 0.9714 | LR: 8.29e-05\n",
            "  Batch 5200/26069 | Loss: 0.9714 | LR: 8.29e-05\n",
            "  Batch 5300/26069 | Loss: 0.9712 | LR: 8.29e-05\n",
            "  Batch 5400/26069 | Loss: 0.9716 | LR: 8.29e-05\n",
            "  Batch 5500/26069 | Loss: 0.9717 | LR: 8.29e-05\n",
            "  Batch 5600/26069 | Loss: 0.9714 | LR: 8.29e-05\n",
            "  Batch 5700/26069 | Loss: 0.9713 | LR: 8.29e-05\n",
            "  Batch 5800/26069 | Loss: 0.9714 | LR: 8.29e-05\n",
            "  Batch 5900/26069 | Loss: 0.9712 | LR: 8.29e-05\n",
            "  Batch 6000/26069 | Loss: 0.9714 | LR: 8.29e-05\n",
            "  Batch 6100/26069 | Loss: 0.9714 | LR: 8.29e-05\n",
            "  Batch 6200/26069 | Loss: 0.9713 | LR: 8.29e-05\n",
            "  Batch 6300/26069 | Loss: 0.9715 | LR: 8.29e-05\n",
            "  Batch 6400/26069 | Loss: 0.9717 | LR: 8.29e-05\n",
            "  Batch 6500/26069 | Loss: 0.9718 | LR: 8.29e-05\n",
            "  Batch 6600/26069 | Loss: 0.9715 | LR: 8.29e-05\n",
            "  Batch 6700/26069 | Loss: 0.9717 | LR: 8.29e-05\n",
            "  Batch 6800/26069 | Loss: 0.9719 | LR: 8.28e-05\n",
            "  Batch 6900/26069 | Loss: 0.9721 | LR: 8.28e-05\n",
            "  Batch 7000/26069 | Loss: 0.9722 | LR: 8.28e-05\n",
            "  Batch 7100/26069 | Loss: 0.9724 | LR: 8.28e-05\n",
            "  Batch 7200/26069 | Loss: 0.9723 | LR: 8.28e-05\n",
            "  Batch 7300/26069 | Loss: 0.9723 | LR: 8.28e-05\n",
            "  Batch 7400/26069 | Loss: 0.9724 | LR: 8.28e-05\n",
            "  Batch 7500/26069 | Loss: 0.9723 | LR: 8.28e-05\n",
            "  Batch 7600/26069 | Loss: 0.9725 | LR: 8.28e-05\n",
            "  Batch 7700/26069 | Loss: 0.9722 | LR: 8.28e-05\n",
            "  Batch 7800/26069 | Loss: 0.9723 | LR: 8.28e-05\n",
            "  Batch 7900/26069 | Loss: 0.9724 | LR: 8.28e-05\n",
            "  Batch 8000/26069 | Loss: 0.9725 | LR: 8.28e-05\n",
            "  Batch 8100/26069 | Loss: 0.9726 | LR: 8.28e-05\n",
            "  Batch 8200/26069 | Loss: 0.9726 | LR: 8.28e-05\n",
            "  Batch 8300/26069 | Loss: 0.9727 | LR: 8.28e-05\n",
            "  Batch 8400/26069 | Loss: 0.9725 | LR: 8.28e-05\n",
            "  Batch 8500/26069 | Loss: 0.9725 | LR: 8.28e-05\n",
            "  Batch 8600/26069 | Loss: 0.9724 | LR: 8.28e-05\n",
            "  Batch 8700/26069 | Loss: 0.9724 | LR: 8.28e-05\n",
            "  Batch 8800/26069 | Loss: 0.9726 | LR: 8.28e-05\n",
            "  Batch 8900/26069 | Loss: 0.9725 | LR: 8.28e-05\n",
            "  Batch 9000/26069 | Loss: 0.9726 | LR: 8.27e-05\n",
            "  Batch 9100/26069 | Loss: 0.9729 | LR: 8.27e-05\n",
            "  Batch 9200/26069 | Loss: 0.9729 | LR: 8.27e-05\n",
            "  Batch 9300/26069 | Loss: 0.9726 | LR: 8.27e-05\n",
            "  Batch 9400/26069 | Loss: 0.9725 | LR: 8.27e-05\n",
            "  Batch 9500/26069 | Loss: 0.9722 | LR: 8.27e-05\n",
            "  Batch 9600/26069 | Loss: 0.9721 | LR: 8.27e-05\n",
            "  Batch 9700/26069 | Loss: 0.9723 | LR: 8.27e-05\n",
            "  Batch 9800/26069 | Loss: 0.9724 | LR: 8.27e-05\n",
            "  Batch 9900/26069 | Loss: 0.9726 | LR: 8.27e-05\n",
            "  Batch 10000/26069 | Loss: 0.9728 | LR: 8.27e-05\n",
            "  Batch 10100/26069 | Loss: 0.9729 | LR: 8.27e-05\n",
            "  Batch 10200/26069 | Loss: 0.9728 | LR: 8.27e-05\n",
            "  Batch 10300/26069 | Loss: 0.9728 | LR: 8.27e-05\n",
            "  Batch 10400/26069 | Loss: 0.9726 | LR: 8.27e-05\n",
            "  Batch 10500/26069 | Loss: 0.9726 | LR: 8.27e-05\n",
            "  Batch 10600/26069 | Loss: 0.9726 | LR: 8.27e-05\n",
            "  Batch 10700/26069 | Loss: 0.9726 | LR: 8.27e-05\n",
            "  Batch 10800/26069 | Loss: 0.9727 | LR: 8.27e-05\n",
            "  Batch 10900/26069 | Loss: 0.9727 | LR: 8.27e-05\n",
            "  Batch 11000/26069 | Loss: 0.9727 | LR: 8.27e-05\n",
            "  Batch 11100/26069 | Loss: 0.9727 | LR: 8.27e-05\n",
            "  Batch 11200/26069 | Loss: 0.9727 | LR: 8.26e-05\n",
            "  Batch 11300/26069 | Loss: 0.9729 | LR: 8.26e-05\n",
            "  Batch 11400/26069 | Loss: 0.9730 | LR: 8.26e-05\n",
            "  Batch 11500/26069 | Loss: 0.9730 | LR: 8.26e-05\n",
            "  Batch 11600/26069 | Loss: 0.9730 | LR: 8.26e-05\n",
            "  Batch 11700/26069 | Loss: 0.9732 | LR: 8.26e-05\n",
            "  Batch 11800/26069 | Loss: 0.9733 | LR: 8.26e-05\n",
            "  Batch 11900/26069 | Loss: 0.9732 | LR: 8.26e-05\n",
            "  Batch 12000/26069 | Loss: 0.9733 | LR: 8.26e-05\n",
            "  Batch 12100/26069 | Loss: 0.9733 | LR: 8.26e-05\n",
            "  Batch 12200/26069 | Loss: 0.9734 | LR: 8.26e-05\n",
            "  Batch 12300/26069 | Loss: 0.9733 | LR: 8.26e-05\n",
            "  Batch 12400/26069 | Loss: 0.9734 | LR: 8.26e-05\n",
            "  Batch 12500/26069 | Loss: 0.9735 | LR: 8.26e-05\n",
            "  Batch 12600/26069 | Loss: 0.9734 | LR: 8.26e-05\n",
            "  Batch 12700/26069 | Loss: 0.9735 | LR: 8.26e-05\n",
            "  Batch 12800/26069 | Loss: 0.9735 | LR: 8.26e-05\n",
            "  Batch 12900/26069 | Loss: 0.9734 | LR: 8.26e-05\n",
            "  Batch 13000/26069 | Loss: 0.9736 | LR: 8.26e-05\n",
            "  Batch 13100/26069 | Loss: 0.9737 | LR: 8.26e-05\n",
            "  Batch 13200/26069 | Loss: 0.9738 | LR: 8.26e-05\n",
            "  Batch 13300/26069 | Loss: 0.9736 | LR: 8.26e-05\n",
            "  Batch 13400/26069 | Loss: 0.9736 | LR: 8.25e-05\n",
            "  Batch 13500/26069 | Loss: 0.9736 | LR: 8.25e-05\n",
            "  Batch 13600/26069 | Loss: 0.9735 | LR: 8.25e-05\n",
            "  Batch 13700/26069 | Loss: 0.9735 | LR: 8.25e-05\n",
            "  Batch 13800/26069 | Loss: 0.9735 | LR: 8.25e-05\n",
            "  Batch 13900/26069 | Loss: 0.9736 | LR: 8.25e-05\n",
            "  Batch 14000/26069 | Loss: 0.9738 | LR: 8.25e-05\n",
            "  Batch 14100/26069 | Loss: 0.9738 | LR: 8.25e-05\n",
            "  Batch 14200/26069 | Loss: 0.9738 | LR: 8.25e-05\n",
            "  Batch 14300/26069 | Loss: 0.9738 | LR: 8.25e-05\n",
            "  Batch 14400/26069 | Loss: 0.9737 | LR: 8.25e-05\n",
            "  Batch 14500/26069 | Loss: 0.9737 | LR: 8.25e-05\n",
            "  Batch 14600/26069 | Loss: 0.9736 | LR: 8.25e-05\n",
            "  Batch 14700/26069 | Loss: 0.9737 | LR: 8.25e-05\n",
            "  Batch 14800/26069 | Loss: 0.9736 | LR: 8.25e-05\n",
            "  Batch 14900/26069 | Loss: 0.9735 | LR: 8.25e-05\n",
            "  Batch 15000/26069 | Loss: 0.9736 | LR: 8.25e-05\n",
            "  Batch 15100/26069 | Loss: 0.9736 | LR: 8.25e-05\n",
            "  Batch 15200/26069 | Loss: 0.9737 | LR: 8.25e-05\n",
            "  Batch 15300/26069 | Loss: 0.9737 | LR: 8.25e-05\n",
            "  Batch 15400/26069 | Loss: 0.9738 | LR: 8.25e-05\n",
            "  Batch 15500/26069 | Loss: 0.9739 | LR: 8.25e-05\n",
            "  Batch 15600/26069 | Loss: 0.9739 | LR: 8.24e-05\n",
            "  Batch 15700/26069 | Loss: 0.9740 | LR: 8.24e-05\n",
            "  Batch 15800/26069 | Loss: 0.9740 | LR: 8.24e-05\n",
            "  Batch 15900/26069 | Loss: 0.9739 | LR: 8.24e-05\n",
            "  Batch 16000/26069 | Loss: 0.9739 | LR: 8.24e-05\n",
            "  Batch 16100/26069 | Loss: 0.9740 | LR: 8.24e-05\n",
            "  Batch 16200/26069 | Loss: 0.9742 | LR: 8.24e-05\n",
            "  Batch 16300/26069 | Loss: 0.9742 | LR: 8.24e-05\n",
            "  Batch 16400/26069 | Loss: 0.9743 | LR: 8.24e-05\n",
            "  Batch 16500/26069 | Loss: 0.9743 | LR: 8.24e-05\n",
            "  Batch 16600/26069 | Loss: 0.9742 | LR: 8.24e-05\n",
            "  Batch 16700/26069 | Loss: 0.9742 | LR: 8.24e-05\n",
            "  Batch 16800/26069 | Loss: 0.9743 | LR: 8.24e-05\n",
            "  Batch 16900/26069 | Loss: 0.9743 | LR: 8.24e-05\n",
            "  Batch 17000/26069 | Loss: 0.9742 | LR: 8.24e-05\n",
            "  Batch 17100/26069 | Loss: 0.9740 | LR: 8.24e-05\n",
            "  Batch 17200/26069 | Loss: 0.9739 | LR: 8.24e-05\n",
            "  Batch 17300/26069 | Loss: 0.9739 | LR: 8.24e-05\n",
            "  Batch 17400/26069 | Loss: 0.9738 | LR: 8.24e-05\n",
            "  Batch 17500/26069 | Loss: 0.9737 | LR: 8.24e-05\n",
            "  Batch 17600/26069 | Loss: 0.9738 | LR: 8.24e-05\n",
            "  Batch 17700/26069 | Loss: 0.9737 | LR: 8.24e-05\n",
            "  Batch 17800/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 17900/26069 | Loss: 0.9737 | LR: 8.23e-05\n",
            "  Batch 18000/26069 | Loss: 0.9737 | LR: 8.23e-05\n",
            "  Batch 18100/26069 | Loss: 0.9737 | LR: 8.23e-05\n",
            "  Batch 18200/26069 | Loss: 0.9738 | LR: 8.23e-05\n",
            "  Batch 18300/26069 | Loss: 0.9737 | LR: 8.23e-05\n",
            "  Batch 18400/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 18500/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 18600/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 18700/26069 | Loss: 0.9737 | LR: 8.23e-05\n",
            "  Batch 18800/26069 | Loss: 0.9737 | LR: 8.23e-05\n",
            "  Batch 18900/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 19000/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 19100/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 19200/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 19300/26069 | Loss: 0.9736 | LR: 8.23e-05\n",
            "  Batch 19400/26069 | Loss: 0.9735 | LR: 8.23e-05\n",
            "  Batch 19500/26069 | Loss: 0.9735 | LR: 8.23e-05\n",
            "  Batch 19600/26069 | Loss: 0.9735 | LR: 8.23e-05\n",
            "  Batch 19700/26069 | Loss: 0.9735 | LR: 8.23e-05\n",
            "  Batch 19800/26069 | Loss: 0.9733 | LR: 8.23e-05\n",
            "  Batch 19900/26069 | Loss: 0.9734 | LR: 8.22e-05\n",
            "  Batch 20000/26069 | Loss: 0.9734 | LR: 8.22e-05\n",
            "  Batch 20100/26069 | Loss: 0.9735 | LR: 8.22e-05\n",
            "  Batch 20200/26069 | Loss: 0.9735 | LR: 8.22e-05\n",
            "  Batch 20300/26069 | Loss: 0.9736 | LR: 8.22e-05\n",
            "  Batch 20400/26069 | Loss: 0.9736 | LR: 8.22e-05\n",
            "  Batch 20500/26069 | Loss: 0.9735 | LR: 8.22e-05\n",
            "  Batch 20600/26069 | Loss: 0.9735 | LR: 8.22e-05\n",
            "  Batch 20700/26069 | Loss: 0.9733 | LR: 8.22e-05\n",
            "  Batch 20800/26069 | Loss: 0.9734 | LR: 8.22e-05\n",
            "  Batch 20900/26069 | Loss: 0.9733 | LR: 8.22e-05\n",
            "  Batch 21000/26069 | Loss: 0.9734 | LR: 8.22e-05\n",
            "  Batch 21100/26069 | Loss: 0.9733 | LR: 8.22e-05\n",
            "  Batch 21200/26069 | Loss: 0.9733 | LR: 8.22e-05\n",
            "  Batch 21300/26069 | Loss: 0.9733 | LR: 8.22e-05\n",
            "  Batch 21400/26069 | Loss: 0.9733 | LR: 8.22e-05\n",
            "  Batch 21500/26069 | Loss: 0.9733 | LR: 8.22e-05\n",
            "  Batch 21600/26069 | Loss: 0.9732 | LR: 8.22e-05\n",
            "  Batch 21700/26069 | Loss: 0.9732 | LR: 8.22e-05\n",
            "  Batch 21800/26069 | Loss: 0.9731 | LR: 8.22e-05\n",
            "  Batch 21900/26069 | Loss: 0.9730 | LR: 8.22e-05\n",
            "  Batch 22000/26069 | Loss: 0.9729 | LR: 8.22e-05\n",
            "  Batch 22100/26069 | Loss: 0.9729 | LR: 8.21e-05\n",
            "  Batch 22200/26069 | Loss: 0.9729 | LR: 8.21e-05\n",
            "  Batch 22300/26069 | Loss: 0.9729 | LR: 8.21e-05\n",
            "  Batch 22400/26069 | Loss: 0.9728 | LR: 8.21e-05\n",
            "  Batch 22500/26069 | Loss: 0.9727 | LR: 8.21e-05\n",
            "  Batch 22600/26069 | Loss: 0.9727 | LR: 8.21e-05\n",
            "  Batch 22700/26069 | Loss: 0.9727 | LR: 8.21e-05\n",
            "  Batch 22800/26069 | Loss: 0.9726 | LR: 8.21e-05\n",
            "  Batch 22900/26069 | Loss: 0.9727 | LR: 8.21e-05\n",
            "  Batch 23000/26069 | Loss: 0.9728 | LR: 8.21e-05\n",
            "  Batch 23100/26069 | Loss: 0.9727 | LR: 8.21e-05\n",
            "  Batch 23200/26069 | Loss: 0.9727 | LR: 8.21e-05\n",
            "  Batch 23300/26069 | Loss: 0.9726 | LR: 8.21e-05\n",
            "  Batch 23400/26069 | Loss: 0.9725 | LR: 8.21e-05\n",
            "  Batch 23500/26069 | Loss: 0.9725 | LR: 8.21e-05\n",
            "  Batch 23600/26069 | Loss: 0.9725 | LR: 8.21e-05\n",
            "  Batch 23700/26069 | Loss: 0.9724 | LR: 8.21e-05\n",
            "  Batch 23800/26069 | Loss: 0.9723 | LR: 8.21e-05\n",
            "  Batch 23900/26069 | Loss: 0.9723 | LR: 8.21e-05\n",
            "  Batch 24000/26069 | Loss: 0.9722 | LR: 8.21e-05\n",
            "  Batch 24100/26069 | Loss: 0.9722 | LR: 8.21e-05\n",
            "  Batch 24200/26069 | Loss: 0.9722 | LR: 8.21e-05\n",
            "  Batch 24300/26069 | Loss: 0.9723 | LR: 8.20e-05\n",
            "  Batch 24400/26069 | Loss: 0.9722 | LR: 8.20e-05\n",
            "  Batch 24500/26069 | Loss: 0.9724 | LR: 8.20e-05\n",
            "  Batch 24600/26069 | Loss: 0.9724 | LR: 8.20e-05\n",
            "  Batch 24700/26069 | Loss: 0.9723 | LR: 8.20e-05\n",
            "  Batch 24800/26069 | Loss: 0.9722 | LR: 8.20e-05\n",
            "  Batch 24900/26069 | Loss: 0.9723 | LR: 8.20e-05\n",
            "  Batch 25000/26069 | Loss: 0.9724 | LR: 8.20e-05\n",
            "  Batch 25100/26069 | Loss: 0.9723 | LR: 8.20e-05\n",
            "  Batch 25200/26069 | Loss: 0.9723 | LR: 8.20e-05\n",
            "  Batch 25300/26069 | Loss: 0.9723 | LR: 8.20e-05\n",
            "  Batch 25400/26069 | Loss: 0.9723 | LR: 8.20e-05\n",
            "  Batch 25500/26069 | Loss: 0.9724 | LR: 8.20e-05\n",
            "  Batch 25600/26069 | Loss: 0.9724 | LR: 8.20e-05\n",
            "  Batch 25700/26069 | Loss: 0.9724 | LR: 8.20e-05\n",
            "  Batch 25800/26069 | Loss: 0.9725 | LR: 8.20e-05\n",
            "  Batch 25900/26069 | Loss: 0.9725 | LR: 8.20e-05\n",
            "  Batch 26000/26069 | Loss: 0.9725 | LR: 8.20e-05\n",
            "\n",
            "  Train Loss: 0.9725\n",
            "  Val Loss:   0.9524 [BEST]\n",
            "  Time:       313.9s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_28.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 29/100\n",
            "  Batch 100/26069 | Loss: 1.0006 | LR: 8.20e-05\n",
            "  Batch 200/26069 | Loss: 0.9897 | LR: 8.20e-05\n",
            "  Batch 300/26069 | Loss: 0.9845 | LR: 8.20e-05\n",
            "  Batch 400/26069 | Loss: 0.9847 | LR: 8.19e-05\n",
            "  Batch 500/26069 | Loss: 0.9783 | LR: 8.19e-05\n",
            "  Batch 600/26069 | Loss: 0.9748 | LR: 8.19e-05\n",
            "  Batch 700/26069 | Loss: 0.9745 | LR: 8.19e-05\n",
            "  Batch 800/26069 | Loss: 0.9771 | LR: 8.19e-05\n",
            "  Batch 900/26069 | Loss: 0.9774 | LR: 8.19e-05\n",
            "  Batch 1000/26069 | Loss: 0.9780 | LR: 8.19e-05\n",
            "  Batch 1100/26069 | Loss: 0.9785 | LR: 8.19e-05\n",
            "  Batch 1200/26069 | Loss: 0.9770 | LR: 8.19e-05\n",
            "  Batch 1300/26069 | Loss: 0.9757 | LR: 8.19e-05\n",
            "  Batch 1400/26069 | Loss: 0.9749 | LR: 8.19e-05\n",
            "  Batch 1500/26069 | Loss: 0.9752 | LR: 8.19e-05\n",
            "  Batch 1600/26069 | Loss: 0.9753 | LR: 8.19e-05\n",
            "  Batch 1700/26069 | Loss: 0.9757 | LR: 8.19e-05\n",
            "  Batch 1800/26069 | Loss: 0.9751 | LR: 8.19e-05\n",
            "  Batch 1900/26069 | Loss: 0.9746 | LR: 8.19e-05\n",
            "  Batch 2000/26069 | Loss: 0.9753 | LR: 8.19e-05\n",
            "  Batch 2100/26069 | Loss: 0.9748 | LR: 8.19e-05\n",
            "  Batch 2200/26069 | Loss: 0.9750 | LR: 8.19e-05\n",
            "  Batch 2300/26069 | Loss: 0.9747 | LR: 8.19e-05\n",
            "  Batch 2400/26069 | Loss: 0.9742 | LR: 8.19e-05\n",
            "  Batch 2500/26069 | Loss: 0.9741 | LR: 8.18e-05\n",
            "  Batch 2600/26069 | Loss: 0.9741 | LR: 8.18e-05\n",
            "  Batch 2700/26069 | Loss: 0.9742 | LR: 8.18e-05\n",
            "  Batch 2800/26069 | Loss: 0.9738 | LR: 8.18e-05\n",
            "  Batch 2900/26069 | Loss: 0.9741 | LR: 8.18e-05\n",
            "  Batch 3000/26069 | Loss: 0.9739 | LR: 8.18e-05\n",
            "  Batch 3100/26069 | Loss: 0.9738 | LR: 8.18e-05\n",
            "  Batch 3200/26069 | Loss: 0.9731 | LR: 8.18e-05\n",
            "  Batch 3300/26069 | Loss: 0.9729 | LR: 8.18e-05\n",
            "  Batch 3400/26069 | Loss: 0.9731 | LR: 8.18e-05\n",
            "  Batch 3500/26069 | Loss: 0.9723 | LR: 8.18e-05\n",
            "  Batch 3600/26069 | Loss: 0.9722 | LR: 8.18e-05\n",
            "  Batch 3700/26069 | Loss: 0.9721 | LR: 8.18e-05\n",
            "  Batch 3800/26069 | Loss: 0.9724 | LR: 8.18e-05\n",
            "  Batch 3900/26069 | Loss: 0.9719 | LR: 8.18e-05\n",
            "  Batch 4000/26069 | Loss: 0.9714 | LR: 8.18e-05\n",
            "  Batch 4100/26069 | Loss: 0.9714 | LR: 8.18e-05\n",
            "  Batch 4200/26069 | Loss: 0.9710 | LR: 8.18e-05\n",
            "  Batch 4300/26069 | Loss: 0.9712 | LR: 8.18e-05\n",
            "  Batch 4400/26069 | Loss: 0.9708 | LR: 8.18e-05\n",
            "  Batch 4500/26069 | Loss: 0.9703 | LR: 8.18e-05\n",
            "  Batch 4600/26069 | Loss: 0.9703 | LR: 8.18e-05\n",
            "  Batch 4700/26069 | Loss: 0.9706 | LR: 8.17e-05\n",
            "  Batch 4800/26069 | Loss: 0.9706 | LR: 8.17e-05\n",
            "  Batch 4900/26069 | Loss: 0.9710 | LR: 8.17e-05\n",
            "  Batch 5000/26069 | Loss: 0.9711 | LR: 8.17e-05\n",
            "  Batch 5100/26069 | Loss: 0.9706 | LR: 8.17e-05\n",
            "  Batch 5200/26069 | Loss: 0.9708 | LR: 8.17e-05\n",
            "  Batch 5300/26069 | Loss: 0.9709 | LR: 8.17e-05\n",
            "  Batch 5400/26069 | Loss: 0.9711 | LR: 8.17e-05\n",
            "  Batch 5500/26069 | Loss: 0.9714 | LR: 8.17e-05\n",
            "  Batch 5600/26069 | Loss: 0.9712 | LR: 8.17e-05\n",
            "  Batch 5700/26069 | Loss: 0.9711 | LR: 8.17e-05\n",
            "  Batch 5800/26069 | Loss: 0.9713 | LR: 8.17e-05\n",
            "  Batch 5900/26069 | Loss: 0.9714 | LR: 8.17e-05\n",
            "  Batch 6000/26069 | Loss: 0.9711 | LR: 8.17e-05\n",
            "  Batch 6100/26069 | Loss: 0.9713 | LR: 8.17e-05\n",
            "  Batch 6200/26069 | Loss: 0.9717 | LR: 8.17e-05\n",
            "  Batch 6300/26069 | Loss: 0.9716 | LR: 8.17e-05\n",
            "  Batch 6400/26069 | Loss: 0.9717 | LR: 8.17e-05\n",
            "  Batch 6500/26069 | Loss: 0.9717 | LR: 8.17e-05\n",
            "  Batch 6600/26069 | Loss: 0.9715 | LR: 8.17e-05\n",
            "  Batch 6700/26069 | Loss: 0.9716 | LR: 8.17e-05\n",
            "  Batch 6800/26069 | Loss: 0.9717 | LR: 8.16e-05\n",
            "  Batch 6900/26069 | Loss: 0.9718 | LR: 8.16e-05\n",
            "  Batch 7000/26069 | Loss: 0.9719 | LR: 8.16e-05\n",
            "  Batch 7100/26069 | Loss: 0.9722 | LR: 8.16e-05\n",
            "  Batch 7200/26069 | Loss: 0.9722 | LR: 8.16e-05\n",
            "  Batch 7300/26069 | Loss: 0.9726 | LR: 8.16e-05\n",
            "  Batch 7400/26069 | Loss: 0.9725 | LR: 8.16e-05\n",
            "  Batch 7500/26069 | Loss: 0.9726 | LR: 8.16e-05\n",
            "  Batch 7600/26069 | Loss: 0.9729 | LR: 8.16e-05\n",
            "  Batch 7700/26069 | Loss: 0.9728 | LR: 8.16e-05\n",
            "  Batch 7800/26069 | Loss: 0.9725 | LR: 8.16e-05\n",
            "  Batch 7900/26069 | Loss: 0.9725 | LR: 8.16e-05\n",
            "  Batch 8000/26069 | Loss: 0.9726 | LR: 8.16e-05\n",
            "  Batch 8100/26069 | Loss: 0.9725 | LR: 8.16e-05\n",
            "  Batch 8200/26069 | Loss: 0.9723 | LR: 8.16e-05\n",
            "  Batch 8300/26069 | Loss: 0.9722 | LR: 8.16e-05\n",
            "  Batch 8400/26069 | Loss: 0.9720 | LR: 8.16e-05\n",
            "  Batch 8500/26069 | Loss: 0.9719 | LR: 8.16e-05\n",
            "  Batch 8600/26069 | Loss: 0.9719 | LR: 8.16e-05\n",
            "  Batch 8700/26069 | Loss: 0.9718 | LR: 8.16e-05\n",
            "  Batch 8800/26069 | Loss: 0.9718 | LR: 8.16e-05\n",
            "  Batch 8900/26069 | Loss: 0.9717 | LR: 8.15e-05\n",
            "  Batch 9000/26069 | Loss: 0.9719 | LR: 8.15e-05\n",
            "  Batch 9100/26069 | Loss: 0.9720 | LR: 8.15e-05\n",
            "  Batch 9200/26069 | Loss: 0.9719 | LR: 8.15e-05\n",
            "  Batch 9300/26069 | Loss: 0.9720 | LR: 8.15e-05\n",
            "  Batch 9400/26069 | Loss: 0.9720 | LR: 8.15e-05\n",
            "  Batch 9500/26069 | Loss: 0.9723 | LR: 8.15e-05\n",
            "  Batch 9600/26069 | Loss: 0.9723 | LR: 8.15e-05\n",
            "  Batch 9700/26069 | Loss: 0.9724 | LR: 8.15e-05\n",
            "  Batch 9800/26069 | Loss: 0.9721 | LR: 8.15e-05\n",
            "  Batch 9900/26069 | Loss: 0.9723 | LR: 8.15e-05\n",
            "  Batch 10000/26069 | Loss: 0.9723 | LR: 8.15e-05\n",
            "  Batch 10100/26069 | Loss: 0.9723 | LR: 8.15e-05\n",
            "  Batch 10200/26069 | Loss: 0.9722 | LR: 8.15e-05\n",
            "  Batch 10300/26069 | Loss: 0.9723 | LR: 8.15e-05\n",
            "  Batch 10400/26069 | Loss: 0.9724 | LR: 8.15e-05\n",
            "  Batch 10500/26069 | Loss: 0.9722 | LR: 8.15e-05\n",
            "  Batch 10600/26069 | Loss: 0.9724 | LR: 8.15e-05\n",
            "  Batch 10700/26069 | Loss: 0.9724 | LR: 8.15e-05\n",
            "  Batch 10800/26069 | Loss: 0.9722 | LR: 8.15e-05\n",
            "  Batch 10900/26069 | Loss: 0.9720 | LR: 8.15e-05\n",
            "  Batch 11000/26069 | Loss: 0.9720 | LR: 8.15e-05\n",
            "  Batch 11100/26069 | Loss: 0.9721 | LR: 8.14e-05\n",
            "  Batch 11200/26069 | Loss: 0.9718 | LR: 8.14e-05\n",
            "  Batch 11300/26069 | Loss: 0.9718 | LR: 8.14e-05\n",
            "  Batch 11400/26069 | Loss: 0.9718 | LR: 8.14e-05\n",
            "  Batch 11500/26069 | Loss: 0.9717 | LR: 8.14e-05\n",
            "  Batch 11600/26069 | Loss: 0.9716 | LR: 8.14e-05\n",
            "  Batch 11700/26069 | Loss: 0.9716 | LR: 8.14e-05\n",
            "  Batch 11800/26069 | Loss: 0.9715 | LR: 8.14e-05\n",
            "  Batch 11900/26069 | Loss: 0.9716 | LR: 8.14e-05\n",
            "  Batch 12000/26069 | Loss: 0.9715 | LR: 8.14e-05\n",
            "  Batch 12100/26069 | Loss: 0.9715 | LR: 8.14e-05\n",
            "  Batch 12200/26069 | Loss: 0.9716 | LR: 8.14e-05\n",
            "  Batch 12300/26069 | Loss: 0.9716 | LR: 8.14e-05\n",
            "  Batch 12400/26069 | Loss: 0.9716 | LR: 8.14e-05\n",
            "  Batch 12500/26069 | Loss: 0.9717 | LR: 8.14e-05\n",
            "  Batch 12600/26069 | Loss: 0.9716 | LR: 8.14e-05\n",
            "  Batch 12700/26069 | Loss: 0.9717 | LR: 8.14e-05\n",
            "  Batch 12800/26069 | Loss: 0.9719 | LR: 8.14e-05\n",
            "  Batch 12900/26069 | Loss: 0.9718 | LR: 8.14e-05\n",
            "  Batch 13000/26069 | Loss: 0.9717 | LR: 8.14e-05\n",
            "  Batch 13100/26069 | Loss: 0.9718 | LR: 8.14e-05\n",
            "  Batch 13200/26069 | Loss: 0.9717 | LR: 8.13e-05\n",
            "  Batch 13300/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 13400/26069 | Loss: 0.9718 | LR: 8.13e-05\n",
            "  Batch 13500/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 13600/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 13700/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 13800/26069 | Loss: 0.9720 | LR: 8.13e-05\n",
            "  Batch 13900/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 14000/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 14100/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 14200/26069 | Loss: 0.9718 | LR: 8.13e-05\n",
            "  Batch 14300/26069 | Loss: 0.9718 | LR: 8.13e-05\n",
            "  Batch 14400/26069 | Loss: 0.9718 | LR: 8.13e-05\n",
            "  Batch 14500/26069 | Loss: 0.9718 | LR: 8.13e-05\n",
            "  Batch 14600/26069 | Loss: 0.9717 | LR: 8.13e-05\n",
            "  Batch 14700/26069 | Loss: 0.9716 | LR: 8.13e-05\n",
            "  Batch 14800/26069 | Loss: 0.9716 | LR: 8.13e-05\n",
            "  Batch 14900/26069 | Loss: 0.9715 | LR: 8.13e-05\n",
            "  Batch 15000/26069 | Loss: 0.9715 | LR: 8.13e-05\n",
            "  Batch 15100/26069 | Loss: 0.9717 | LR: 8.13e-05\n",
            "  Batch 15200/26069 | Loss: 0.9719 | LR: 8.13e-05\n",
            "  Batch 15300/26069 | Loss: 0.9720 | LR: 8.12e-05\n",
            "  Batch 15400/26069 | Loss: 0.9721 | LR: 8.12e-05\n",
            "  Batch 15500/26069 | Loss: 0.9720 | LR: 8.12e-05\n",
            "  Batch 15600/26069 | Loss: 0.9720 | LR: 8.12e-05\n",
            "  Batch 15700/26069 | Loss: 0.9721 | LR: 8.12e-05\n",
            "  Batch 15800/26069 | Loss: 0.9721 | LR: 8.12e-05\n",
            "  Batch 15900/26069 | Loss: 0.9721 | LR: 8.12e-05\n",
            "  Batch 16000/26069 | Loss: 0.9721 | LR: 8.12e-05\n",
            "  Batch 16100/26069 | Loss: 0.9719 | LR: 8.12e-05\n",
            "  Batch 16200/26069 | Loss: 0.9720 | LR: 8.12e-05\n",
            "  Batch 16300/26069 | Loss: 0.9719 | LR: 8.12e-05\n",
            "  Batch 16400/26069 | Loss: 0.9719 | LR: 8.12e-05\n",
            "  Batch 16500/26069 | Loss: 0.9719 | LR: 8.12e-05\n",
            "  Batch 16600/26069 | Loss: 0.9719 | LR: 8.12e-05\n",
            "  Batch 16700/26069 | Loss: 0.9719 | LR: 8.12e-05\n",
            "  Batch 16800/26069 | Loss: 0.9718 | LR: 8.12e-05\n",
            "  Batch 16900/26069 | Loss: 0.9718 | LR: 8.12e-05\n",
            "  Batch 17000/26069 | Loss: 0.9718 | LR: 8.12e-05\n",
            "  Batch 17100/26069 | Loss: 0.9720 | LR: 8.12e-05\n",
            "  Batch 17200/26069 | Loss: 0.9721 | LR: 8.12e-05\n",
            "  Batch 17300/26069 | Loss: 0.9721 | LR: 8.12e-05\n",
            "  Batch 17400/26069 | Loss: 0.9720 | LR: 8.12e-05\n",
            "  Batch 17500/26069 | Loss: 0.9720 | LR: 8.11e-05\n",
            "  Batch 17600/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 17700/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 17800/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 17900/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 18000/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 18100/26069 | Loss: 0.9718 | LR: 8.11e-05\n",
            "  Batch 18200/26069 | Loss: 0.9717 | LR: 8.11e-05\n",
            "  Batch 18300/26069 | Loss: 0.9718 | LR: 8.11e-05\n",
            "  Batch 18400/26069 | Loss: 0.9717 | LR: 8.11e-05\n",
            "  Batch 18500/26069 | Loss: 0.9717 | LR: 8.11e-05\n",
            "  Batch 18600/26069 | Loss: 0.9718 | LR: 8.11e-05\n",
            "  Batch 18700/26069 | Loss: 0.9717 | LR: 8.11e-05\n",
            "  Batch 18800/26069 | Loss: 0.9718 | LR: 8.11e-05\n",
            "  Batch 18900/26069 | Loss: 0.9718 | LR: 8.11e-05\n",
            "  Batch 19000/26069 | Loss: 0.9717 | LR: 8.11e-05\n",
            "  Batch 19100/26069 | Loss: 0.9718 | LR: 8.11e-05\n",
            "  Batch 19200/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 19300/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 19400/26069 | Loss: 0.9720 | LR: 8.11e-05\n",
            "  Batch 19500/26069 | Loss: 0.9719 | LR: 8.11e-05\n",
            "  Batch 19600/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 19700/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 19800/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 19900/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 20000/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 20100/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 20200/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 20300/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 20400/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 20500/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 20600/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 20700/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 20800/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 20900/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 21000/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 21100/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 21200/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 21300/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 21400/26069 | Loss: 0.9719 | LR: 8.10e-05\n",
            "  Batch 21500/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 21600/26069 | Loss: 0.9720 | LR: 8.10e-05\n",
            "  Batch 21700/26069 | Loss: 0.9719 | LR: 8.09e-05\n",
            "  Batch 21800/26069 | Loss: 0.9719 | LR: 8.09e-05\n",
            "  Batch 21900/26069 | Loss: 0.9718 | LR: 8.09e-05\n",
            "  Batch 22000/26069 | Loss: 0.9717 | LR: 8.09e-05\n",
            "  Batch 22100/26069 | Loss: 0.9717 | LR: 8.09e-05\n",
            "  Batch 22200/26069 | Loss: 0.9716 | LR: 8.09e-05\n",
            "  Batch 22300/26069 | Loss: 0.9715 | LR: 8.09e-05\n",
            "  Batch 22400/26069 | Loss: 0.9715 | LR: 8.09e-05\n",
            "  Batch 22500/26069 | Loss: 0.9716 | LR: 8.09e-05\n",
            "  Batch 22600/26069 | Loss: 0.9716 | LR: 8.09e-05\n",
            "  Batch 22700/26069 | Loss: 0.9716 | LR: 8.09e-05\n",
            "  Batch 22800/26069 | Loss: 0.9716 | LR: 8.09e-05\n",
            "  Batch 22900/26069 | Loss: 0.9715 | LR: 8.09e-05\n",
            "  Batch 23000/26069 | Loss: 0.9714 | LR: 8.09e-05\n",
            "  Batch 23100/26069 | Loss: 0.9714 | LR: 8.09e-05\n",
            "  Batch 23200/26069 | Loss: 0.9715 | LR: 8.09e-05\n",
            "  Batch 23300/26069 | Loss: 0.9715 | LR: 8.09e-05\n",
            "  Batch 23400/26069 | Loss: 0.9716 | LR: 8.09e-05\n",
            "  Batch 23500/26069 | Loss: 0.9717 | LR: 8.09e-05\n",
            "  Batch 23600/26069 | Loss: 0.9717 | LR: 8.09e-05\n",
            "  Batch 23700/26069 | Loss: 0.9716 | LR: 8.09e-05\n",
            "  Batch 23800/26069 | Loss: 0.9716 | LR: 8.08e-05\n",
            "  Batch 23900/26069 | Loss: 0.9717 | LR: 8.08e-05\n",
            "  Batch 24000/26069 | Loss: 0.9717 | LR: 8.08e-05\n",
            "  Batch 24100/26069 | Loss: 0.9716 | LR: 8.08e-05\n",
            "  Batch 24200/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 24300/26069 | Loss: 0.9716 | LR: 8.08e-05\n",
            "  Batch 24400/26069 | Loss: 0.9716 | LR: 8.08e-05\n",
            "  Batch 24500/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 24600/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 24700/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 24800/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 24900/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 25000/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 25100/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 25200/26069 | Loss: 0.9717 | LR: 8.08e-05\n",
            "  Batch 25300/26069 | Loss: 0.9717 | LR: 8.08e-05\n",
            "  Batch 25400/26069 | Loss: 0.9716 | LR: 8.08e-05\n",
            "  Batch 25500/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 25600/26069 | Loss: 0.9715 | LR: 8.08e-05\n",
            "  Batch 25700/26069 | Loss: 0.9717 | LR: 8.08e-05\n",
            "  Batch 25800/26069 | Loss: 0.9717 | LR: 8.08e-05\n",
            "  Batch 25900/26069 | Loss: 0.9718 | LR: 8.07e-05\n",
            "  Batch 26000/26069 | Loss: 0.9718 | LR: 8.07e-05\n",
            "\n",
            "  Train Loss: 0.9718\n",
            "  Val Loss:   0.9516 [BEST]\n",
            "  Time:       313.4s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_29.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 30/100\n",
            "  Batch 100/26069 | Loss: 0.9523 | LR: 8.07e-05\n",
            "  Batch 200/26069 | Loss: 0.9670 | LR: 8.07e-05\n",
            "  Batch 300/26069 | Loss: 0.9717 | LR: 8.07e-05\n",
            "  Batch 400/26069 | Loss: 0.9726 | LR: 8.07e-05\n",
            "  Batch 500/26069 | Loss: 0.9738 | LR: 8.07e-05\n",
            "  Batch 600/26069 | Loss: 0.9734 | LR: 8.07e-05\n",
            "  Batch 700/26069 | Loss: 0.9734 | LR: 8.07e-05\n",
            "  Batch 800/26069 | Loss: 0.9727 | LR: 8.07e-05\n",
            "  Batch 900/26069 | Loss: 0.9729 | LR: 8.07e-05\n",
            "  Batch 1000/26069 | Loss: 0.9734 | LR: 8.07e-05\n",
            "  Batch 1100/26069 | Loss: 0.9741 | LR: 8.07e-05\n",
            "  Batch 1200/26069 | Loss: 0.9741 | LR: 8.07e-05\n",
            "  Batch 1300/26069 | Loss: 0.9746 | LR: 8.07e-05\n",
            "  Batch 1400/26069 | Loss: 0.9754 | LR: 8.07e-05\n",
            "  Batch 1500/26069 | Loss: 0.9755 | LR: 8.07e-05\n",
            "  Batch 1600/26069 | Loss: 0.9758 | LR: 8.07e-05\n",
            "  Batch 1700/26069 | Loss: 0.9759 | LR: 8.07e-05\n",
            "  Batch 1800/26069 | Loss: 0.9749 | LR: 8.07e-05\n",
            "  Batch 1900/26069 | Loss: 0.9755 | LR: 8.07e-05\n",
            "  Batch 2000/26069 | Loss: 0.9760 | LR: 8.06e-05\n",
            "  Batch 2100/26069 | Loss: 0.9748 | LR: 8.06e-05\n",
            "  Batch 2200/26069 | Loss: 0.9740 | LR: 8.06e-05\n",
            "  Batch 2300/26069 | Loss: 0.9748 | LR: 8.06e-05\n",
            "  Batch 2400/26069 | Loss: 0.9750 | LR: 8.06e-05\n",
            "  Batch 2500/26069 | Loss: 0.9753 | LR: 8.06e-05\n",
            "  Batch 2600/26069 | Loss: 0.9749 | LR: 8.06e-05\n",
            "  Batch 2700/26069 | Loss: 0.9756 | LR: 8.06e-05\n",
            "  Batch 2800/26069 | Loss: 0.9756 | LR: 8.06e-05\n",
            "  Batch 2900/26069 | Loss: 0.9752 | LR: 8.06e-05\n",
            "  Batch 3000/26069 | Loss: 0.9753 | LR: 8.06e-05\n",
            "  Batch 3100/26069 | Loss: 0.9753 | LR: 8.06e-05\n",
            "  Batch 3200/26069 | Loss: 0.9752 | LR: 8.06e-05\n",
            "  Batch 3300/26069 | Loss: 0.9757 | LR: 8.06e-05\n",
            "  Batch 3400/26069 | Loss: 0.9753 | LR: 8.06e-05\n",
            "  Batch 3500/26069 | Loss: 0.9746 | LR: 8.06e-05\n",
            "  Batch 3600/26069 | Loss: 0.9754 | LR: 8.06e-05\n",
            "  Batch 3700/26069 | Loss: 0.9752 | LR: 8.06e-05\n",
            "  Batch 3800/26069 | Loss: 0.9749 | LR: 8.06e-05\n",
            "  Batch 3900/26069 | Loss: 0.9747 | LR: 8.06e-05\n",
            "  Batch 4000/26069 | Loss: 0.9745 | LR: 8.06e-05\n",
            "  Batch 4100/26069 | Loss: 0.9739 | LR: 8.05e-05\n",
            "  Batch 4200/26069 | Loss: 0.9738 | LR: 8.05e-05\n",
            "  Batch 4300/26069 | Loss: 0.9740 | LR: 8.05e-05\n",
            "  Batch 4400/26069 | Loss: 0.9741 | LR: 8.05e-05\n",
            "  Batch 4500/26069 | Loss: 0.9739 | LR: 8.05e-05\n",
            "  Batch 4600/26069 | Loss: 0.9741 | LR: 8.05e-05\n",
            "  Batch 4700/26069 | Loss: 0.9742 | LR: 8.05e-05\n",
            "  Batch 4800/26069 | Loss: 0.9740 | LR: 8.05e-05\n",
            "  Batch 4900/26069 | Loss: 0.9740 | LR: 8.05e-05\n",
            "  Batch 5000/26069 | Loss: 0.9742 | LR: 8.05e-05\n",
            "  Batch 5100/26069 | Loss: 0.9745 | LR: 8.05e-05\n",
            "  Batch 5200/26069 | Loss: 0.9745 | LR: 8.05e-05\n",
            "  Batch 5300/26069 | Loss: 0.9747 | LR: 8.05e-05\n",
            "  Batch 5400/26069 | Loss: 0.9749 | LR: 8.05e-05\n",
            "  Batch 5500/26069 | Loss: 0.9749 | LR: 8.05e-05\n",
            "  Batch 5600/26069 | Loss: 0.9748 | LR: 8.05e-05\n",
            "  Batch 5700/26069 | Loss: 0.9750 | LR: 8.05e-05\n",
            "  Batch 5800/26069 | Loss: 0.9747 | LR: 8.05e-05\n",
            "  Batch 5900/26069 | Loss: 0.9746 | LR: 8.05e-05\n",
            "  Batch 6000/26069 | Loss: 0.9746 | LR: 8.05e-05\n",
            "  Batch 6100/26069 | Loss: 0.9745 | LR: 8.04e-05\n",
            "  Batch 6200/26069 | Loss: 0.9746 | LR: 8.04e-05\n",
            "  Batch 6300/26069 | Loss: 0.9746 | LR: 8.04e-05\n",
            "  Batch 6400/26069 | Loss: 0.9745 | LR: 8.04e-05\n",
            "  Batch 6500/26069 | Loss: 0.9747 | LR: 8.04e-05\n",
            "  Batch 6600/26069 | Loss: 0.9747 | LR: 8.04e-05\n",
            "  Batch 6700/26069 | Loss: 0.9743 | LR: 8.04e-05\n",
            "  Batch 6800/26069 | Loss: 0.9744 | LR: 8.04e-05\n",
            "  Batch 6900/26069 | Loss: 0.9741 | LR: 8.04e-05\n",
            "  Batch 7000/26069 | Loss: 0.9738 | LR: 8.04e-05\n",
            "  Batch 7100/26069 | Loss: 0.9738 | LR: 8.04e-05\n",
            "  Batch 7200/26069 | Loss: 0.9738 | LR: 8.04e-05\n",
            "  Batch 7300/26069 | Loss: 0.9738 | LR: 8.04e-05\n",
            "  Batch 7400/26069 | Loss: 0.9738 | LR: 8.04e-05\n",
            "  Batch 7500/26069 | Loss: 0.9736 | LR: 8.04e-05\n",
            "  Batch 7600/26069 | Loss: 0.9735 | LR: 8.04e-05\n",
            "  Batch 7700/26069 | Loss: 0.9736 | LR: 8.04e-05\n",
            "  Batch 7800/26069 | Loss: 0.9735 | LR: 8.04e-05\n",
            "  Batch 7900/26069 | Loss: 0.9736 | LR: 8.04e-05\n",
            "  Batch 8000/26069 | Loss: 0.9736 | LR: 8.04e-05\n",
            "  Batch 8100/26069 | Loss: 0.9734 | LR: 8.04e-05\n",
            "  Batch 8200/26069 | Loss: 0.9734 | LR: 8.03e-05\n",
            "  Batch 8300/26069 | Loss: 0.9733 | LR: 8.03e-05\n",
            "  Batch 8400/26069 | Loss: 0.9731 | LR: 8.03e-05\n",
            "  Batch 8500/26069 | Loss: 0.9732 | LR: 8.03e-05\n",
            "  Batch 8600/26069 | Loss: 0.9733 | LR: 8.03e-05\n",
            "  Batch 8700/26069 | Loss: 0.9733 | LR: 8.03e-05\n",
            "  Batch 8800/26069 | Loss: 0.9731 | LR: 8.03e-05\n",
            "  Batch 8900/26069 | Loss: 0.9732 | LR: 8.03e-05\n",
            "  Batch 9000/26069 | Loss: 0.9729 | LR: 8.03e-05\n",
            "  Batch 9100/26069 | Loss: 0.9730 | LR: 8.03e-05\n",
            "  Batch 9200/26069 | Loss: 0.9729 | LR: 8.03e-05\n",
            "  Batch 9300/26069 | Loss: 0.9730 | LR: 8.03e-05\n",
            "  Batch 9400/26069 | Loss: 0.9727 | LR: 8.03e-05\n",
            "  Batch 9500/26069 | Loss: 0.9727 | LR: 8.03e-05\n",
            "  Batch 9600/26069 | Loss: 0.9727 | LR: 8.03e-05\n",
            "  Batch 9700/26069 | Loss: 0.9725 | LR: 8.03e-05\n",
            "  Batch 9800/26069 | Loss: 0.9726 | LR: 8.03e-05\n",
            "  Batch 9900/26069 | Loss: 0.9725 | LR: 8.03e-05\n",
            "  Batch 10000/26069 | Loss: 0.9725 | LR: 8.03e-05\n",
            "  Batch 10100/26069 | Loss: 0.9724 | LR: 8.03e-05\n",
            "  Batch 10200/26069 | Loss: 0.9723 | LR: 8.03e-05\n",
            "  Batch 10300/26069 | Loss: 0.9723 | LR: 8.02e-05\n",
            "  Batch 10400/26069 | Loss: 0.9723 | LR: 8.02e-05\n",
            "  Batch 10500/26069 | Loss: 0.9723 | LR: 8.02e-05\n",
            "  Batch 10600/26069 | Loss: 0.9723 | LR: 8.02e-05\n",
            "  Batch 10700/26069 | Loss: 0.9724 | LR: 8.02e-05\n",
            "  Batch 10800/26069 | Loss: 0.9724 | LR: 8.02e-05\n",
            "  Batch 10900/26069 | Loss: 0.9725 | LR: 8.02e-05\n",
            "  Batch 11000/26069 | Loss: 0.9726 | LR: 8.02e-05\n",
            "  Batch 11100/26069 | Loss: 0.9725 | LR: 8.02e-05\n",
            "  Batch 11200/26069 | Loss: 0.9725 | LR: 8.02e-05\n",
            "  Batch 11300/26069 | Loss: 0.9726 | LR: 8.02e-05\n",
            "  Batch 11400/26069 | Loss: 0.9727 | LR: 8.02e-05\n",
            "  Batch 11500/26069 | Loss: 0.9727 | LR: 8.02e-05\n",
            "  Batch 11600/26069 | Loss: 0.9729 | LR: 8.02e-05\n",
            "  Batch 11700/26069 | Loss: 0.9727 | LR: 8.02e-05\n",
            "  Batch 11800/26069 | Loss: 0.9726 | LR: 8.02e-05\n",
            "  Batch 11900/26069 | Loss: 0.9725 | LR: 8.02e-05\n",
            "  Batch 12000/26069 | Loss: 0.9725 | LR: 8.02e-05\n",
            "  Batch 12100/26069 | Loss: 0.9726 | LR: 8.02e-05\n",
            "  Batch 12200/26069 | Loss: 0.9725 | LR: 8.02e-05\n",
            "  Batch 12300/26069 | Loss: 0.9723 | LR: 8.02e-05\n",
            "  Batch 12400/26069 | Loss: 0.9723 | LR: 8.01e-05\n",
            "  Batch 12500/26069 | Loss: 0.9724 | LR: 8.01e-05\n",
            "  Batch 12600/26069 | Loss: 0.9725 | LR: 8.01e-05\n",
            "  Batch 12700/26069 | Loss: 0.9725 | LR: 8.01e-05\n",
            "  Batch 12800/26069 | Loss: 0.9725 | LR: 8.01e-05\n",
            "  Batch 12900/26069 | Loss: 0.9725 | LR: 8.01e-05\n",
            "  Batch 13000/26069 | Loss: 0.9724 | LR: 8.01e-05\n",
            "  Batch 13100/26069 | Loss: 0.9722 | LR: 8.01e-05\n",
            "  Batch 13200/26069 | Loss: 0.9722 | LR: 8.01e-05\n",
            "  Batch 13300/26069 | Loss: 0.9721 | LR: 8.01e-05\n",
            "  Batch 13400/26069 | Loss: 0.9721 | LR: 8.01e-05\n",
            "  Batch 13500/26069 | Loss: 0.9721 | LR: 8.01e-05\n",
            "  Batch 13600/26069 | Loss: 0.9721 | LR: 8.01e-05\n",
            "  Batch 13700/26069 | Loss: 0.9721 | LR: 8.01e-05\n",
            "  Batch 13800/26069 | Loss: 0.9721 | LR: 8.01e-05\n",
            "  Batch 13900/26069 | Loss: 0.9720 | LR: 8.01e-05\n",
            "  Batch 14000/26069 | Loss: 0.9720 | LR: 8.01e-05\n",
            "  Batch 14100/26069 | Loss: 0.9720 | LR: 8.01e-05\n",
            "  Batch 14200/26069 | Loss: 0.9719 | LR: 8.01e-05\n",
            "  Batch 14300/26069 | Loss: 0.9719 | LR: 8.01e-05\n",
            "  Batch 14400/26069 | Loss: 0.9719 | LR: 8.01e-05\n",
            "  Batch 14500/26069 | Loss: 0.9720 | LR: 8.00e-05\n",
            "  Batch 14600/26069 | Loss: 0.9717 | LR: 8.00e-05\n",
            "  Batch 14700/26069 | Loss: 0.9717 | LR: 8.00e-05\n",
            "  Batch 14800/26069 | Loss: 0.9717 | LR: 8.00e-05\n",
            "  Batch 14900/26069 | Loss: 0.9718 | LR: 8.00e-05\n",
            "  Batch 15000/26069 | Loss: 0.9718 | LR: 8.00e-05\n",
            "  Batch 15100/26069 | Loss: 0.9717 | LR: 8.00e-05\n",
            "  Batch 15200/26069 | Loss: 0.9716 | LR: 8.00e-05\n",
            "  Batch 15300/26069 | Loss: 0.9715 | LR: 8.00e-05\n",
            "  Batch 15400/26069 | Loss: 0.9715 | LR: 8.00e-05\n",
            "  Batch 15500/26069 | Loss: 0.9714 | LR: 8.00e-05\n",
            "  Batch 15600/26069 | Loss: 0.9715 | LR: 8.00e-05\n",
            "  Batch 15700/26069 | Loss: 0.9714 | LR: 8.00e-05\n",
            "  Batch 15800/26069 | Loss: 0.9714 | LR: 8.00e-05\n",
            "  Batch 15900/26069 | Loss: 0.9715 | LR: 8.00e-05\n",
            "  Batch 16000/26069 | Loss: 0.9716 | LR: 8.00e-05\n",
            "  Batch 16100/26069 | Loss: 0.9715 | LR: 8.00e-05\n",
            "  Batch 16200/26069 | Loss: 0.9715 | LR: 8.00e-05\n",
            "  Batch 16300/26069 | Loss: 0.9714 | LR: 8.00e-05\n",
            "  Batch 16400/26069 | Loss: 0.9714 | LR: 8.00e-05\n",
            "  Batch 16500/26069 | Loss: 0.9715 | LR: 8.00e-05\n",
            "  Batch 16600/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 16700/26069 | Loss: 0.9715 | LR: 7.99e-05\n",
            "  Batch 16800/26069 | Loss: 0.9715 | LR: 7.99e-05\n",
            "  Batch 16900/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 17000/26069 | Loss: 0.9717 | LR: 7.99e-05\n",
            "  Batch 17100/26069 | Loss: 0.9717 | LR: 7.99e-05\n",
            "  Batch 17200/26069 | Loss: 0.9718 | LR: 7.99e-05\n",
            "  Batch 17300/26069 | Loss: 0.9719 | LR: 7.99e-05\n",
            "  Batch 17400/26069 | Loss: 0.9717 | LR: 7.99e-05\n",
            "  Batch 17500/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 17600/26069 | Loss: 0.9717 | LR: 7.99e-05\n",
            "  Batch 17700/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 17800/26069 | Loss: 0.9717 | LR: 7.99e-05\n",
            "  Batch 17900/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 18000/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 18100/26069 | Loss: 0.9715 | LR: 7.99e-05\n",
            "  Batch 18200/26069 | Loss: 0.9715 | LR: 7.99e-05\n",
            "  Batch 18300/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 18400/26069 | Loss: 0.9715 | LR: 7.99e-05\n",
            "  Batch 18500/26069 | Loss: 0.9716 | LR: 7.99e-05\n",
            "  Batch 18600/26069 | Loss: 0.9716 | LR: 7.98e-05\n",
            "  Batch 18700/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 18800/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 18900/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 19000/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 19100/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 19200/26069 | Loss: 0.9714 | LR: 7.98e-05\n",
            "  Batch 19300/26069 | Loss: 0.9714 | LR: 7.98e-05\n",
            "  Batch 19400/26069 | Loss: 0.9714 | LR: 7.98e-05\n",
            "  Batch 19500/26069 | Loss: 0.9713 | LR: 7.98e-05\n",
            "  Batch 19600/26069 | Loss: 0.9712 | LR: 7.98e-05\n",
            "  Batch 19700/26069 | Loss: 0.9713 | LR: 7.98e-05\n",
            "  Batch 19800/26069 | Loss: 0.9713 | LR: 7.98e-05\n",
            "  Batch 19900/26069 | Loss: 0.9714 | LR: 7.98e-05\n",
            "  Batch 20000/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 20100/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 20200/26069 | Loss: 0.9714 | LR: 7.98e-05\n",
            "  Batch 20300/26069 | Loss: 0.9714 | LR: 7.98e-05\n",
            "  Batch 20400/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 20500/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 20600/26069 | Loss: 0.9715 | LR: 7.98e-05\n",
            "  Batch 20700/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 20800/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 20900/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 21000/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 21100/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 21200/26069 | Loss: 0.9713 | LR: 7.97e-05\n",
            "  Batch 21300/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 21400/26069 | Loss: 0.9713 | LR: 7.97e-05\n",
            "  Batch 21500/26069 | Loss: 0.9713 | LR: 7.97e-05\n",
            "  Batch 21600/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 21700/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 21800/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 21900/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 22000/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 22100/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 22200/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 22300/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 22400/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 22500/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 22600/26069 | Loss: 0.9714 | LR: 7.97e-05\n",
            "  Batch 22700/26069 | Loss: 0.9715 | LR: 7.97e-05\n",
            "  Batch 22800/26069 | Loss: 0.9715 | LR: 7.96e-05\n",
            "  Batch 22900/26069 | Loss: 0.9715 | LR: 7.96e-05\n",
            "  Batch 23000/26069 | Loss: 0.9715 | LR: 7.96e-05\n",
            "  Batch 23100/26069 | Loss: 0.9715 | LR: 7.96e-05\n",
            "  Batch 23200/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 23300/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 23400/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 23500/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 23600/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 23700/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 23800/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 23900/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 24000/26069 | Loss: 0.9717 | LR: 7.96e-05\n",
            "  Batch 24100/26069 | Loss: 0.9717 | LR: 7.96e-05\n",
            "  Batch 24200/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 24300/26069 | Loss: 0.9716 | LR: 7.96e-05\n",
            "  Batch 24400/26069 | Loss: 0.9714 | LR: 7.96e-05\n",
            "  Batch 24500/26069 | Loss: 0.9714 | LR: 7.96e-05\n",
            "  Batch 24600/26069 | Loss: 0.9715 | LR: 7.96e-05\n",
            "  Batch 24700/26069 | Loss: 0.9714 | LR: 7.96e-05\n",
            "  Batch 24800/26069 | Loss: 0.9715 | LR: 7.95e-05\n",
            "  Batch 24900/26069 | Loss: 0.9715 | LR: 7.95e-05\n",
            "  Batch 25000/26069 | Loss: 0.9715 | LR: 7.95e-05\n",
            "  Batch 25100/26069 | Loss: 0.9715 | LR: 7.95e-05\n",
            "  Batch 25200/26069 | Loss: 0.9714 | LR: 7.95e-05\n",
            "  Batch 25300/26069 | Loss: 0.9715 | LR: 7.95e-05\n",
            "  Batch 25400/26069 | Loss: 0.9714 | LR: 7.95e-05\n",
            "  Batch 25500/26069 | Loss: 0.9715 | LR: 7.95e-05\n",
            "  Batch 25600/26069 | Loss: 0.9715 | LR: 7.95e-05\n",
            "  Batch 25700/26069 | Loss: 0.9714 | LR: 7.95e-05\n",
            "  Batch 25800/26069 | Loss: 0.9714 | LR: 7.95e-05\n",
            "  Batch 25900/26069 | Loss: 0.9713 | LR: 7.95e-05\n",
            "  Batch 26000/26069 | Loss: 0.9713 | LR: 7.95e-05\n",
            "\n",
            "  Train Loss: 0.9713\n",
            "  Val Loss:   0.9537 \n",
            "  Time:       312.8s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_30.pt\n",
            "Epoch 31/100\n",
            "  Batch 100/26069 | Loss: 0.9421 | LR: 7.95e-05\n",
            "  Batch 200/26069 | Loss: 0.9534 | LR: 7.95e-05\n",
            "  Batch 300/26069 | Loss: 0.9623 | LR: 7.95e-05\n",
            "  Batch 400/26069 | Loss: 0.9610 | LR: 7.95e-05\n",
            "  Batch 500/26069 | Loss: 0.9620 | LR: 7.95e-05\n",
            "  Batch 600/26069 | Loss: 0.9623 | LR: 7.95e-05\n",
            "  Batch 700/26069 | Loss: 0.9624 | LR: 7.95e-05\n",
            "  Batch 800/26069 | Loss: 0.9663 | LR: 7.94e-05\n",
            "  Batch 900/26069 | Loss: 0.9671 | LR: 7.94e-05\n",
            "  Batch 1000/26069 | Loss: 0.9663 | LR: 7.94e-05\n",
            "  Batch 1100/26069 | Loss: 0.9645 | LR: 7.94e-05\n",
            "  Batch 1200/26069 | Loss: 0.9631 | LR: 7.94e-05\n",
            "  Batch 1300/26069 | Loss: 0.9646 | LR: 7.94e-05\n",
            "  Batch 1400/26069 | Loss: 0.9672 | LR: 7.94e-05\n",
            "  Batch 1500/26069 | Loss: 0.9673 | LR: 7.94e-05\n",
            "  Batch 1600/26069 | Loss: 0.9672 | LR: 7.94e-05\n",
            "  Batch 1700/26069 | Loss: 0.9669 | LR: 7.94e-05\n",
            "  Batch 1800/26069 | Loss: 0.9667 | LR: 7.94e-05\n",
            "  Batch 1900/26069 | Loss: 0.9678 | LR: 7.94e-05\n",
            "  Batch 2000/26069 | Loss: 0.9657 | LR: 7.94e-05\n",
            "  Batch 2100/26069 | Loss: 0.9665 | LR: 7.94e-05\n",
            "  Batch 2200/26069 | Loss: 0.9668 | LR: 7.94e-05\n",
            "  Batch 2300/26069 | Loss: 0.9672 | LR: 7.94e-05\n",
            "  Batch 2400/26069 | Loss: 0.9673 | LR: 7.94e-05\n",
            "  Batch 2500/26069 | Loss: 0.9673 | LR: 7.94e-05\n",
            "  Batch 2600/26069 | Loss: 0.9677 | LR: 7.94e-05\n",
            "  Batch 2700/26069 | Loss: 0.9677 | LR: 7.94e-05\n",
            "  Batch 2800/26069 | Loss: 0.9682 | LR: 7.94e-05\n",
            "  Batch 2900/26069 | Loss: 0.9686 | LR: 7.93e-05\n",
            "  Batch 3000/26069 | Loss: 0.9686 | LR: 7.93e-05\n",
            "  Batch 3100/26069 | Loss: 0.9683 | LR: 7.93e-05\n",
            "  Batch 3200/26069 | Loss: 0.9684 | LR: 7.93e-05\n",
            "  Batch 3300/26069 | Loss: 0.9683 | LR: 7.93e-05\n",
            "  Batch 3400/26069 | Loss: 0.9683 | LR: 7.93e-05\n",
            "  Batch 3500/26069 | Loss: 0.9683 | LR: 7.93e-05\n",
            "  Batch 3600/26069 | Loss: 0.9684 | LR: 7.93e-05\n",
            "  Batch 3700/26069 | Loss: 0.9686 | LR: 7.93e-05\n",
            "  Batch 3800/26069 | Loss: 0.9684 | LR: 7.93e-05\n",
            "  Batch 3900/26069 | Loss: 0.9681 | LR: 7.93e-05\n",
            "  Batch 4000/26069 | Loss: 0.9686 | LR: 7.93e-05\n",
            "  Batch 4100/26069 | Loss: 0.9684 | LR: 7.93e-05\n",
            "  Batch 4200/26069 | Loss: 0.9685 | LR: 7.93e-05\n",
            "  Batch 4300/26069 | Loss: 0.9684 | LR: 7.93e-05\n",
            "  Batch 4400/26069 | Loss: 0.9684 | LR: 7.93e-05\n",
            "  Batch 4500/26069 | Loss: 0.9687 | LR: 7.93e-05\n",
            "  Batch 4600/26069 | Loss: 0.9686 | LR: 7.93e-05\n",
            "  Batch 4700/26069 | Loss: 0.9685 | LR: 7.93e-05\n",
            "  Batch 4800/26069 | Loss: 0.9688 | LR: 7.93e-05\n",
            "  Batch 4900/26069 | Loss: 0.9687 | LR: 7.92e-05\n",
            "  Batch 5000/26069 | Loss: 0.9694 | LR: 7.92e-05\n",
            "  Batch 5100/26069 | Loss: 0.9692 | LR: 7.92e-05\n",
            "  Batch 5200/26069 | Loss: 0.9692 | LR: 7.92e-05\n",
            "  Batch 5300/26069 | Loss: 0.9694 | LR: 7.92e-05\n",
            "  Batch 5400/26069 | Loss: 0.9694 | LR: 7.92e-05\n",
            "  Batch 5500/26069 | Loss: 0.9692 | LR: 7.92e-05\n",
            "  Batch 5600/26069 | Loss: 0.9693 | LR: 7.92e-05\n",
            "  Batch 5700/26069 | Loss: 0.9692 | LR: 7.92e-05\n",
            "  Batch 5800/26069 | Loss: 0.9692 | LR: 7.92e-05\n",
            "  Batch 5900/26069 | Loss: 0.9693 | LR: 7.92e-05\n",
            "  Batch 6000/26069 | Loss: 0.9690 | LR: 7.92e-05\n",
            "  Batch 6100/26069 | Loss: 0.9691 | LR: 7.92e-05\n",
            "  Batch 6200/26069 | Loss: 0.9692 | LR: 7.92e-05\n",
            "  Batch 6300/26069 | Loss: 0.9693 | LR: 7.92e-05\n",
            "  Batch 6400/26069 | Loss: 0.9697 | LR: 7.92e-05\n",
            "  Batch 6500/26069 | Loss: 0.9698 | LR: 7.92e-05\n",
            "  Batch 6600/26069 | Loss: 0.9697 | LR: 7.92e-05\n",
            "  Batch 6700/26069 | Loss: 0.9696 | LR: 7.92e-05\n",
            "  Batch 6800/26069 | Loss: 0.9695 | LR: 7.92e-05\n",
            "  Batch 6900/26069 | Loss: 0.9696 | LR: 7.91e-05\n",
            "  Batch 7000/26069 | Loss: 0.9696 | LR: 7.91e-05\n",
            "  Batch 7100/26069 | Loss: 0.9695 | LR: 7.91e-05\n",
            "  Batch 7200/26069 | Loss: 0.9696 | LR: 7.91e-05\n",
            "  Batch 7300/26069 | Loss: 0.9696 | LR: 7.91e-05\n",
            "  Batch 7400/26069 | Loss: 0.9696 | LR: 7.91e-05\n",
            "  Batch 7500/26069 | Loss: 0.9696 | LR: 7.91e-05\n",
            "  Batch 7600/26069 | Loss: 0.9696 | LR: 7.91e-05\n",
            "  Batch 7700/26069 | Loss: 0.9695 | LR: 7.91e-05\n",
            "  Batch 7800/26069 | Loss: 0.9695 | LR: 7.91e-05\n",
            "  Batch 7900/26069 | Loss: 0.9698 | LR: 7.91e-05\n",
            "  Batch 8000/26069 | Loss: 0.9698 | LR: 7.91e-05\n",
            "  Batch 8100/26069 | Loss: 0.9698 | LR: 7.91e-05\n",
            "  Batch 8200/26069 | Loss: 0.9698 | LR: 7.91e-05\n",
            "  Batch 8300/26069 | Loss: 0.9698 | LR: 7.91e-05\n",
            "  Batch 8400/26069 | Loss: 0.9698 | LR: 7.91e-05\n",
            "  Batch 8500/26069 | Loss: 0.9698 | LR: 7.91e-05\n",
            "  Batch 8600/26069 | Loss: 0.9695 | LR: 7.91e-05\n",
            "  Batch 8700/26069 | Loss: 0.9693 | LR: 7.91e-05\n",
            "  Batch 8800/26069 | Loss: 0.9693 | LR: 7.91e-05\n",
            "  Batch 8900/26069 | Loss: 0.9695 | LR: 7.91e-05\n",
            "  Batch 9000/26069 | Loss: 0.9697 | LR: 7.90e-05\n",
            "  Batch 9100/26069 | Loss: 0.9694 | LR: 7.90e-05\n",
            "  Batch 9200/26069 | Loss: 0.9696 | LR: 7.90e-05\n",
            "  Batch 9300/26069 | Loss: 0.9694 | LR: 7.90e-05\n",
            "  Batch 9400/26069 | Loss: 0.9696 | LR: 7.90e-05\n",
            "  Batch 9500/26069 | Loss: 0.9697 | LR: 7.90e-05\n",
            "  Batch 9600/26069 | Loss: 0.9698 | LR: 7.90e-05\n",
            "  Batch 9700/26069 | Loss: 0.9699 | LR: 7.90e-05\n",
            "  Batch 9800/26069 | Loss: 0.9700 | LR: 7.90e-05\n",
            "  Batch 9900/26069 | Loss: 0.9701 | LR: 7.90e-05\n",
            "  Batch 10000/26069 | Loss: 0.9701 | LR: 7.90e-05\n",
            "  Batch 10100/26069 | Loss: 0.9699 | LR: 7.90e-05\n",
            "  Batch 10200/26069 | Loss: 0.9701 | LR: 7.90e-05\n",
            "  Batch 10300/26069 | Loss: 0.9700 | LR: 7.90e-05\n",
            "  Batch 10400/26069 | Loss: 0.9702 | LR: 7.90e-05\n",
            "  Batch 10500/26069 | Loss: 0.9699 | LR: 7.90e-05\n",
            "  Batch 10600/26069 | Loss: 0.9698 | LR: 7.90e-05\n",
            "  Batch 10700/26069 | Loss: 0.9698 | LR: 7.90e-05\n",
            "  Batch 10800/26069 | Loss: 0.9697 | LR: 7.90e-05\n",
            "  Batch 10900/26069 | Loss: 0.9695 | LR: 7.90e-05\n",
            "  Batch 11000/26069 | Loss: 0.9696 | LR: 7.89e-05\n",
            "  Batch 11100/26069 | Loss: 0.9698 | LR: 7.89e-05\n",
            "  Batch 11200/26069 | Loss: 0.9699 | LR: 7.89e-05\n",
            "  Batch 11300/26069 | Loss: 0.9698 | LR: 7.89e-05\n",
            "  Batch 11400/26069 | Loss: 0.9698 | LR: 7.89e-05\n",
            "  Batch 11500/26069 | Loss: 0.9699 | LR: 7.89e-05\n",
            "  Batch 11600/26069 | Loss: 0.9700 | LR: 7.89e-05\n",
            "  Batch 11700/26069 | Loss: 0.9700 | LR: 7.89e-05\n",
            "  Batch 11800/26069 | Loss: 0.9701 | LR: 7.89e-05\n",
            "  Batch 11900/26069 | Loss: 0.9701 | LR: 7.89e-05\n",
            "  Batch 12000/26069 | Loss: 0.9701 | LR: 7.89e-05\n",
            "  Batch 12100/26069 | Loss: 0.9702 | LR: 7.89e-05\n",
            "  Batch 12200/26069 | Loss: 0.9700 | LR: 7.89e-05\n",
            "  Batch 12300/26069 | Loss: 0.9700 | LR: 7.89e-05\n",
            "  Batch 12400/26069 | Loss: 0.9701 | LR: 7.89e-05\n",
            "  Batch 12500/26069 | Loss: 0.9701 | LR: 7.89e-05\n",
            "  Batch 12600/26069 | Loss: 0.9702 | LR: 7.89e-05\n",
            "  Batch 12700/26069 | Loss: 0.9701 | LR: 7.89e-05\n",
            "  Batch 12800/26069 | Loss: 0.9703 | LR: 7.89e-05\n",
            "  Batch 12900/26069 | Loss: 0.9703 | LR: 7.89e-05\n",
            "  Batch 13000/26069 | Loss: 0.9701 | LR: 7.89e-05\n",
            "  Batch 13100/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 13200/26069 | Loss: 0.9705 | LR: 7.88e-05\n",
            "  Batch 13300/26069 | Loss: 0.9705 | LR: 7.88e-05\n",
            "  Batch 13400/26069 | Loss: 0.9705 | LR: 7.88e-05\n",
            "  Batch 13500/26069 | Loss: 0.9704 | LR: 7.88e-05\n",
            "  Batch 13600/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 13700/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 13800/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 13900/26069 | Loss: 0.9704 | LR: 7.88e-05\n",
            "  Batch 14000/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 14100/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 14200/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 14300/26069 | Loss: 0.9703 | LR: 7.88e-05\n",
            "  Batch 14400/26069 | Loss: 0.9704 | LR: 7.88e-05\n",
            "  Batch 14500/26069 | Loss: 0.9704 | LR: 7.88e-05\n",
            "  Batch 14600/26069 | Loss: 0.9705 | LR: 7.88e-05\n",
            "  Batch 14700/26069 | Loss: 0.9706 | LR: 7.88e-05\n",
            "  Batch 14800/26069 | Loss: 0.9705 | LR: 7.88e-05\n",
            "  Batch 14900/26069 | Loss: 0.9704 | LR: 7.88e-05\n",
            "  Batch 15000/26069 | Loss: 0.9704 | LR: 7.88e-05\n",
            "  Batch 15100/26069 | Loss: 0.9704 | LR: 7.87e-05\n",
            "  Batch 15200/26069 | Loss: 0.9703 | LR: 7.87e-05\n",
            "  Batch 15300/26069 | Loss: 0.9703 | LR: 7.87e-05\n",
            "  Batch 15400/26069 | Loss: 0.9703 | LR: 7.87e-05\n",
            "  Batch 15500/26069 | Loss: 0.9703 | LR: 7.87e-05\n",
            "  Batch 15600/26069 | Loss: 0.9704 | LR: 7.87e-05\n",
            "  Batch 15700/26069 | Loss: 0.9703 | LR: 7.87e-05\n",
            "  Batch 15800/26069 | Loss: 0.9703 | LR: 7.87e-05\n",
            "  Batch 15900/26069 | Loss: 0.9704 | LR: 7.87e-05\n",
            "  Batch 16000/26069 | Loss: 0.9704 | LR: 7.87e-05\n",
            "  Batch 16100/26069 | Loss: 0.9705 | LR: 7.87e-05\n",
            "  Batch 16200/26069 | Loss: 0.9704 | LR: 7.87e-05\n",
            "  Batch 16300/26069 | Loss: 0.9705 | LR: 7.87e-05\n",
            "  Batch 16400/26069 | Loss: 0.9705 | LR: 7.87e-05\n",
            "  Batch 16500/26069 | Loss: 0.9706 | LR: 7.87e-05\n",
            "  Batch 16600/26069 | Loss: 0.9707 | LR: 7.87e-05\n",
            "  Batch 16700/26069 | Loss: 0.9707 | LR: 7.87e-05\n",
            "  Batch 16800/26069 | Loss: 0.9706 | LR: 7.87e-05\n",
            "  Batch 16900/26069 | Loss: 0.9705 | LR: 7.87e-05\n",
            "  Batch 17000/26069 | Loss: 0.9707 | LR: 7.87e-05\n",
            "  Batch 17100/26069 | Loss: 0.9707 | LR: 7.86e-05\n",
            "  Batch 17200/26069 | Loss: 0.9707 | LR: 7.86e-05\n",
            "  Batch 17300/26069 | Loss: 0.9707 | LR: 7.86e-05\n",
            "  Batch 17400/26069 | Loss: 0.9706 | LR: 7.86e-05\n",
            "  Batch 17500/26069 | Loss: 0.9706 | LR: 7.86e-05\n",
            "  Batch 17600/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 17700/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 17800/26069 | Loss: 0.9704 | LR: 7.86e-05\n",
            "  Batch 17900/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 18000/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 18100/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 18200/26069 | Loss: 0.9704 | LR: 7.86e-05\n",
            "  Batch 18300/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 18400/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 18500/26069 | Loss: 0.9705 | LR: 7.86e-05\n",
            "  Batch 18600/26069 | Loss: 0.9706 | LR: 7.86e-05\n",
            "  Batch 18700/26069 | Loss: 0.9707 | LR: 7.86e-05\n",
            "  Batch 18800/26069 | Loss: 0.9707 | LR: 7.86e-05\n",
            "  Batch 18900/26069 | Loss: 0.9707 | LR: 7.86e-05\n",
            "  Batch 19000/26069 | Loss: 0.9706 | LR: 7.86e-05\n",
            "  Batch 19100/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 19200/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 19300/26069 | Loss: 0.9705 | LR: 7.85e-05\n",
            "  Batch 19400/26069 | Loss: 0.9705 | LR: 7.85e-05\n",
            "  Batch 19500/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 19600/26069 | Loss: 0.9707 | LR: 7.85e-05\n",
            "  Batch 19700/26069 | Loss: 0.9707 | LR: 7.85e-05\n",
            "  Batch 19800/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 19900/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 20000/26069 | Loss: 0.9707 | LR: 7.85e-05\n",
            "  Batch 20100/26069 | Loss: 0.9708 | LR: 7.85e-05\n",
            "  Batch 20200/26069 | Loss: 0.9708 | LR: 7.85e-05\n",
            "  Batch 20300/26069 | Loss: 0.9708 | LR: 7.85e-05\n",
            "  Batch 20400/26069 | Loss: 0.9709 | LR: 7.85e-05\n",
            "  Batch 20500/26069 | Loss: 0.9709 | LR: 7.85e-05\n",
            "  Batch 20600/26069 | Loss: 0.9709 | LR: 7.85e-05\n",
            "  Batch 20700/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 20800/26069 | Loss: 0.9707 | LR: 7.85e-05\n",
            "  Batch 20900/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 21000/26069 | Loss: 0.9706 | LR: 7.85e-05\n",
            "  Batch 21100/26069 | Loss: 0.9707 | LR: 7.85e-05\n",
            "  Batch 21200/26069 | Loss: 0.9707 | LR: 7.84e-05\n",
            "  Batch 21300/26069 | Loss: 0.9708 | LR: 7.84e-05\n",
            "  Batch 21400/26069 | Loss: 0.9707 | LR: 7.84e-05\n",
            "  Batch 21500/26069 | Loss: 0.9707 | LR: 7.84e-05\n",
            "  Batch 21600/26069 | Loss: 0.9707 | LR: 7.84e-05\n",
            "  Batch 21700/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 21800/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 21900/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22000/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22100/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22200/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22300/26069 | Loss: 0.9707 | LR: 7.84e-05\n",
            "  Batch 22400/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22500/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22600/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22700/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22800/26069 | Loss: 0.9706 | LR: 7.84e-05\n",
            "  Batch 22900/26069 | Loss: 0.9705 | LR: 7.84e-05\n",
            "  Batch 23000/26069 | Loss: 0.9704 | LR: 7.84e-05\n",
            "  Batch 23100/26069 | Loss: 0.9704 | LR: 7.84e-05\n",
            "  Batch 23200/26069 | Loss: 0.9704 | LR: 7.83e-05\n",
            "  Batch 23300/26069 | Loss: 0.9704 | LR: 7.83e-05\n",
            "  Batch 23400/26069 | Loss: 0.9704 | LR: 7.83e-05\n",
            "  Batch 23500/26069 | Loss: 0.9704 | LR: 7.83e-05\n",
            "  Batch 23600/26069 | Loss: 0.9705 | LR: 7.83e-05\n",
            "  Batch 23700/26069 | Loss: 0.9705 | LR: 7.83e-05\n",
            "  Batch 23800/26069 | Loss: 0.9705 | LR: 7.83e-05\n",
            "  Batch 23900/26069 | Loss: 0.9705 | LR: 7.83e-05\n",
            "  Batch 24000/26069 | Loss: 0.9706 | LR: 7.83e-05\n",
            "  Batch 24100/26069 | Loss: 0.9706 | LR: 7.83e-05\n",
            "  Batch 24200/26069 | Loss: 0.9706 | LR: 7.83e-05\n",
            "  Batch 24300/26069 | Loss: 0.9706 | LR: 7.83e-05\n",
            "  Batch 24400/26069 | Loss: 0.9707 | LR: 7.83e-05\n",
            "  Batch 24500/26069 | Loss: 0.9706 | LR: 7.83e-05\n",
            "  Batch 24600/26069 | Loss: 0.9707 | LR: 7.83e-05\n",
            "  Batch 24700/26069 | Loss: 0.9707 | LR: 7.83e-05\n",
            "  Batch 24800/26069 | Loss: 0.9707 | LR: 7.83e-05\n",
            "  Batch 24900/26069 | Loss: 0.9707 | LR: 7.83e-05\n",
            "  Batch 25000/26069 | Loss: 0.9706 | LR: 7.83e-05\n",
            "  Batch 25100/26069 | Loss: 0.9706 | LR: 7.83e-05\n",
            "  Batch 25200/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "  Batch 25300/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "  Batch 25400/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "  Batch 25500/26069 | Loss: 0.9707 | LR: 7.82e-05\n",
            "  Batch 25600/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "  Batch 25700/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "  Batch 25800/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "  Batch 25900/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "  Batch 26000/26069 | Loss: 0.9706 | LR: 7.82e-05\n",
            "\n",
            "  Train Loss: 0.9707\n",
            "  Val Loss:   0.9522 \n",
            "  Time:       313.0s\n",
            "\n",
            "Epoch 32/100\n",
            "  Batch 100/26069 | Loss: 0.9533 | LR: 7.82e-05\n",
            "  Batch 200/26069 | Loss: 0.9646 | LR: 7.82e-05\n",
            "  Batch 300/26069 | Loss: 0.9633 | LR: 7.82e-05\n",
            "  Batch 400/26069 | Loss: 0.9675 | LR: 7.82e-05\n",
            "  Batch 500/26069 | Loss: 0.9677 | LR: 7.82e-05\n",
            "  Batch 600/26069 | Loss: 0.9640 | LR: 7.82e-05\n",
            "  Batch 700/26069 | Loss: 0.9625 | LR: 7.82e-05\n",
            "  Batch 800/26069 | Loss: 0.9636 | LR: 7.82e-05\n",
            "  Batch 900/26069 | Loss: 0.9628 | LR: 7.82e-05\n",
            "  Batch 1000/26069 | Loss: 0.9630 | LR: 7.82e-05\n",
            "  Batch 1100/26069 | Loss: 0.9647 | LR: 7.81e-05\n",
            "  Batch 1200/26069 | Loss: 0.9653 | LR: 7.81e-05\n",
            "  Batch 1300/26069 | Loss: 0.9649 | LR: 7.81e-05\n",
            "  Batch 1400/26069 | Loss: 0.9642 | LR: 7.81e-05\n",
            "  Batch 1500/26069 | Loss: 0.9639 | LR: 7.81e-05\n",
            "  Batch 1600/26069 | Loss: 0.9640 | LR: 7.81e-05\n",
            "  Batch 1700/26069 | Loss: 0.9645 | LR: 7.81e-05\n",
            "  Batch 1800/26069 | Loss: 0.9650 | LR: 7.81e-05\n",
            "  Batch 1900/26069 | Loss: 0.9655 | LR: 7.81e-05\n",
            "  Batch 2000/26069 | Loss: 0.9661 | LR: 7.81e-05\n",
            "  Batch 2100/26069 | Loss: 0.9657 | LR: 7.81e-05\n",
            "  Batch 2200/26069 | Loss: 0.9646 | LR: 7.81e-05\n",
            "  Batch 2300/26069 | Loss: 0.9650 | LR: 7.81e-05\n",
            "  Batch 2400/26069 | Loss: 0.9648 | LR: 7.81e-05\n",
            "  Batch 2500/26069 | Loss: 0.9658 | LR: 7.81e-05\n",
            "  Batch 2600/26069 | Loss: 0.9658 | LR: 7.81e-05\n",
            "  Batch 2700/26069 | Loss: 0.9657 | LR: 7.81e-05\n",
            "  Batch 2800/26069 | Loss: 0.9648 | LR: 7.81e-05\n",
            "  Batch 2900/26069 | Loss: 0.9656 | LR: 7.81e-05\n",
            "  Batch 3000/26069 | Loss: 0.9659 | LR: 7.81e-05\n",
            "  Batch 3100/26069 | Loss: 0.9669 | LR: 7.80e-05\n",
            "  Batch 3200/26069 | Loss: 0.9663 | LR: 7.80e-05\n",
            "  Batch 3300/26069 | Loss: 0.9660 | LR: 7.80e-05\n",
            "  Batch 3400/26069 | Loss: 0.9658 | LR: 7.80e-05\n",
            "  Batch 3500/26069 | Loss: 0.9659 | LR: 7.80e-05\n",
            "  Batch 3600/26069 | Loss: 0.9654 | LR: 7.80e-05\n",
            "  Batch 3700/26069 | Loss: 0.9657 | LR: 7.80e-05\n",
            "  Batch 3800/26069 | Loss: 0.9661 | LR: 7.80e-05\n",
            "  Batch 3900/26069 | Loss: 0.9664 | LR: 7.80e-05\n",
            "  Batch 4000/26069 | Loss: 0.9666 | LR: 7.80e-05\n",
            "  Batch 4100/26069 | Loss: 0.9668 | LR: 7.80e-05\n",
            "  Batch 4200/26069 | Loss: 0.9668 | LR: 7.80e-05\n",
            "  Batch 4300/26069 | Loss: 0.9671 | LR: 7.80e-05\n",
            "  Batch 4400/26069 | Loss: 0.9669 | LR: 7.80e-05\n",
            "  Batch 4500/26069 | Loss: 0.9667 | LR: 7.80e-05\n",
            "  Batch 4600/26069 | Loss: 0.9662 | LR: 7.80e-05\n",
            "  Batch 4700/26069 | Loss: 0.9668 | LR: 7.80e-05\n",
            "  Batch 4800/26069 | Loss: 0.9671 | LR: 7.80e-05\n",
            "  Batch 4900/26069 | Loss: 0.9670 | LR: 7.80e-05\n",
            "  Batch 5000/26069 | Loss: 0.9670 | LR: 7.80e-05\n",
            "  Batch 5100/26069 | Loss: 0.9675 | LR: 7.79e-05\n",
            "  Batch 5200/26069 | Loss: 0.9674 | LR: 7.79e-05\n",
            "  Batch 5300/26069 | Loss: 0.9672 | LR: 7.79e-05\n",
            "  Batch 5400/26069 | Loss: 0.9674 | LR: 7.79e-05\n",
            "  Batch 5500/26069 | Loss: 0.9675 | LR: 7.79e-05\n",
            "  Batch 5600/26069 | Loss: 0.9675 | LR: 7.79e-05\n",
            "  Batch 5700/26069 | Loss: 0.9679 | LR: 7.79e-05\n",
            "  Batch 5800/26069 | Loss: 0.9677 | LR: 7.79e-05\n",
            "  Batch 5900/26069 | Loss: 0.9677 | LR: 7.79e-05\n",
            "  Batch 6000/26069 | Loss: 0.9674 | LR: 7.79e-05\n",
            "  Batch 6100/26069 | Loss: 0.9677 | LR: 7.79e-05\n",
            "  Batch 6200/26069 | Loss: 0.9674 | LR: 7.79e-05\n",
            "  Batch 6300/26069 | Loss: 0.9671 | LR: 7.79e-05\n",
            "  Batch 6400/26069 | Loss: 0.9671 | LR: 7.79e-05\n",
            "  Batch 6500/26069 | Loss: 0.9672 | LR: 7.79e-05\n",
            "  Batch 6600/26069 | Loss: 0.9672 | LR: 7.79e-05\n",
            "  Batch 6700/26069 | Loss: 0.9673 | LR: 7.79e-05\n",
            "  Batch 6800/26069 | Loss: 0.9670 | LR: 7.79e-05\n",
            "  Batch 6900/26069 | Loss: 0.9671 | LR: 7.79e-05\n",
            "  Batch 7000/26069 | Loss: 0.9673 | LR: 7.79e-05\n",
            "  Batch 7100/26069 | Loss: 0.9673 | LR: 7.78e-05\n",
            "  Batch 7200/26069 | Loss: 0.9675 | LR: 7.78e-05\n",
            "  Batch 7300/26069 | Loss: 0.9676 | LR: 7.78e-05\n",
            "  Batch 7400/26069 | Loss: 0.9677 | LR: 7.78e-05\n",
            "  Batch 7500/26069 | Loss: 0.9677 | LR: 7.78e-05\n",
            "  Batch 7600/26069 | Loss: 0.9679 | LR: 7.78e-05\n",
            "  Batch 7700/26069 | Loss: 0.9679 | LR: 7.78e-05\n",
            "  Batch 7800/26069 | Loss: 0.9678 | LR: 7.78e-05\n",
            "  Batch 7900/26069 | Loss: 0.9681 | LR: 7.78e-05\n",
            "  Batch 8000/26069 | Loss: 0.9684 | LR: 7.78e-05\n",
            "  Batch 8100/26069 | Loss: 0.9682 | LR: 7.78e-05\n",
            "  Batch 8200/26069 | Loss: 0.9682 | LR: 7.78e-05\n",
            "  Batch 8300/26069 | Loss: 0.9681 | LR: 7.78e-05\n",
            "  Batch 8400/26069 | Loss: 0.9684 | LR: 7.78e-05\n",
            "  Batch 8500/26069 | Loss: 0.9685 | LR: 7.78e-05\n",
            "  Batch 8600/26069 | Loss: 0.9687 | LR: 7.78e-05\n",
            "  Batch 8700/26069 | Loss: 0.9687 | LR: 7.78e-05\n",
            "  Batch 8800/26069 | Loss: 0.9688 | LR: 7.78e-05\n",
            "  Batch 8900/26069 | Loss: 0.9689 | LR: 7.78e-05\n",
            "  Batch 9000/26069 | Loss: 0.9691 | LR: 7.78e-05\n",
            "  Batch 9100/26069 | Loss: 0.9690 | LR: 7.77e-05\n",
            "  Batch 9200/26069 | Loss: 0.9692 | LR: 7.77e-05\n",
            "  Batch 9300/26069 | Loss: 0.9691 | LR: 7.77e-05\n",
            "  Batch 9400/26069 | Loss: 0.9690 | LR: 7.77e-05\n",
            "  Batch 9500/26069 | Loss: 0.9691 | LR: 7.77e-05\n",
            "  Batch 9600/26069 | Loss: 0.9691 | LR: 7.77e-05\n",
            "  Batch 9700/26069 | Loss: 0.9689 | LR: 7.77e-05\n",
            "  Batch 9800/26069 | Loss: 0.9690 | LR: 7.77e-05\n",
            "  Batch 9900/26069 | Loss: 0.9692 | LR: 7.77e-05\n",
            "  Batch 10000/26069 | Loss: 0.9692 | LR: 7.77e-05\n",
            "  Batch 10100/26069 | Loss: 0.9691 | LR: 7.77e-05\n",
            "  Batch 10200/26069 | Loss: 0.9691 | LR: 7.77e-05\n",
            "  Batch 10300/26069 | Loss: 0.9688 | LR: 7.77e-05\n",
            "  Batch 10400/26069 | Loss: 0.9688 | LR: 7.77e-05\n",
            "  Batch 10500/26069 | Loss: 0.9689 | LR: 7.77e-05\n",
            "  Batch 10600/26069 | Loss: 0.9688 | LR: 7.77e-05\n",
            "  Batch 10700/26069 | Loss: 0.9686 | LR: 7.77e-05\n",
            "  Batch 10800/26069 | Loss: 0.9686 | LR: 7.77e-05\n",
            "  Batch 10900/26069 | Loss: 0.9686 | LR: 7.77e-05\n",
            "  Batch 11000/26069 | Loss: 0.9687 | LR: 7.77e-05\n",
            "  Batch 11100/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 11200/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 11300/26069 | Loss: 0.9686 | LR: 7.76e-05\n",
            "  Batch 11400/26069 | Loss: 0.9686 | LR: 7.76e-05\n",
            "  Batch 11500/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 11600/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 11700/26069 | Loss: 0.9686 | LR: 7.76e-05\n",
            "  Batch 11800/26069 | Loss: 0.9685 | LR: 7.76e-05\n",
            "  Batch 11900/26069 | Loss: 0.9685 | LR: 7.76e-05\n",
            "  Batch 12000/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 12100/26069 | Loss: 0.9686 | LR: 7.76e-05\n",
            "  Batch 12200/26069 | Loss: 0.9686 | LR: 7.76e-05\n",
            "  Batch 12300/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 12400/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 12500/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 12600/26069 | Loss: 0.9688 | LR: 7.76e-05\n",
            "  Batch 12700/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 12800/26069 | Loss: 0.9687 | LR: 7.76e-05\n",
            "  Batch 12900/26069 | Loss: 0.9686 | LR: 7.76e-05\n",
            "  Batch 13000/26069 | Loss: 0.9686 | LR: 7.76e-05\n",
            "  Batch 13100/26069 | Loss: 0.9686 | LR: 7.75e-05\n",
            "  Batch 13200/26069 | Loss: 0.9685 | LR: 7.75e-05\n",
            "  Batch 13300/26069 | Loss: 0.9686 | LR: 7.75e-05\n",
            "  Batch 13400/26069 | Loss: 0.9686 | LR: 7.75e-05\n",
            "  Batch 13500/26069 | Loss: 0.9687 | LR: 7.75e-05\n",
            "  Batch 13600/26069 | Loss: 0.9687 | LR: 7.75e-05\n",
            "  Batch 13700/26069 | Loss: 0.9687 | LR: 7.75e-05\n",
            "  Batch 13800/26069 | Loss: 0.9686 | LR: 7.75e-05\n",
            "  Batch 13900/26069 | Loss: 0.9685 | LR: 7.75e-05\n",
            "  Batch 14000/26069 | Loss: 0.9684 | LR: 7.75e-05\n",
            "  Batch 14100/26069 | Loss: 0.9684 | LR: 7.75e-05\n",
            "  Batch 14200/26069 | Loss: 0.9683 | LR: 7.75e-05\n",
            "  Batch 14300/26069 | Loss: 0.9683 | LR: 7.75e-05\n",
            "  Batch 14400/26069 | Loss: 0.9683 | LR: 7.75e-05\n",
            "  Batch 14500/26069 | Loss: 0.9684 | LR: 7.75e-05\n",
            "  Batch 14600/26069 | Loss: 0.9684 | LR: 7.75e-05\n",
            "  Batch 14700/26069 | Loss: 0.9683 | LR: 7.75e-05\n",
            "  Batch 14800/26069 | Loss: 0.9683 | LR: 7.75e-05\n",
            "  Batch 14900/26069 | Loss: 0.9683 | LR: 7.75e-05\n",
            "  Batch 15000/26069 | Loss: 0.9685 | LR: 7.75e-05\n",
            "  Batch 15100/26069 | Loss: 0.9685 | LR: 7.74e-05\n",
            "  Batch 15200/26069 | Loss: 0.9685 | LR: 7.74e-05\n",
            "  Batch 15300/26069 | Loss: 0.9686 | LR: 7.74e-05\n",
            "  Batch 15400/26069 | Loss: 0.9686 | LR: 7.74e-05\n",
            "  Batch 15500/26069 | Loss: 0.9686 | LR: 7.74e-05\n",
            "  Batch 15600/26069 | Loss: 0.9687 | LR: 7.74e-05\n",
            "  Batch 15700/26069 | Loss: 0.9688 | LR: 7.74e-05\n",
            "  Batch 15800/26069 | Loss: 0.9688 | LR: 7.74e-05\n",
            "  Batch 15900/26069 | Loss: 0.9688 | LR: 7.74e-05\n",
            "  Batch 16000/26069 | Loss: 0.9688 | LR: 7.74e-05\n",
            "  Batch 16100/26069 | Loss: 0.9688 | LR: 7.74e-05\n",
            "  Batch 16200/26069 | Loss: 0.9688 | LR: 7.74e-05\n",
            "  Batch 16300/26069 | Loss: 0.9689 | LR: 7.74e-05\n",
            "  Batch 16400/26069 | Loss: 0.9688 | LR: 7.74e-05\n",
            "  Batch 16500/26069 | Loss: 0.9689 | LR: 7.74e-05\n",
            "  Batch 16600/26069 | Loss: 0.9690 | LR: 7.74e-05\n",
            "  Batch 16700/26069 | Loss: 0.9691 | LR: 7.74e-05\n",
            "  Batch 16800/26069 | Loss: 0.9692 | LR: 7.74e-05\n",
            "  Batch 16900/26069 | Loss: 0.9693 | LR: 7.74e-05\n",
            "  Batch 17000/26069 | Loss: 0.9693 | LR: 7.74e-05\n",
            "  Batch 17100/26069 | Loss: 0.9693 | LR: 7.73e-05\n",
            "  Batch 17200/26069 | Loss: 0.9694 | LR: 7.73e-05\n",
            "  Batch 17300/26069 | Loss: 0.9694 | LR: 7.73e-05\n",
            "  Batch 17400/26069 | Loss: 0.9694 | LR: 7.73e-05\n",
            "  Batch 17500/26069 | Loss: 0.9694 | LR: 7.73e-05\n",
            "  Batch 17600/26069 | Loss: 0.9693 | LR: 7.73e-05\n",
            "  Batch 17700/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 17800/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 17900/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 18000/26069 | Loss: 0.9693 | LR: 7.73e-05\n",
            "  Batch 18100/26069 | Loss: 0.9693 | LR: 7.73e-05\n",
            "  Batch 18200/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 18300/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 18400/26069 | Loss: 0.9693 | LR: 7.73e-05\n",
            "  Batch 18500/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 18600/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 18700/26069 | Loss: 0.9693 | LR: 7.73e-05\n",
            "  Batch 18800/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 18900/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 19000/26069 | Loss: 0.9692 | LR: 7.73e-05\n",
            "  Batch 19100/26069 | Loss: 0.9692 | LR: 7.72e-05\n",
            "  Batch 19200/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 19300/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 19400/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 19500/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 19600/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 19700/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 19800/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 19900/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 20000/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 20100/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 20200/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 20300/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 20400/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 20500/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 20600/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 20700/26069 | Loss: 0.9693 | LR: 7.72e-05\n",
            "  Batch 20800/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 20900/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 21000/26069 | Loss: 0.9694 | LR: 7.72e-05\n",
            "  Batch 21100/26069 | Loss: 0.9694 | LR: 7.71e-05\n",
            "  Batch 21200/26069 | Loss: 0.9694 | LR: 7.71e-05\n",
            "  Batch 21300/26069 | Loss: 0.9694 | LR: 7.71e-05\n",
            "  Batch 21400/26069 | Loss: 0.9694 | LR: 7.71e-05\n",
            "  Batch 21500/26069 | Loss: 0.9695 | LR: 7.71e-05\n",
            "  Batch 21600/26069 | Loss: 0.9695 | LR: 7.71e-05\n",
            "  Batch 21700/26069 | Loss: 0.9694 | LR: 7.71e-05\n",
            "  Batch 21800/26069 | Loss: 0.9694 | LR: 7.71e-05\n",
            "  Batch 21900/26069 | Loss: 0.9695 | LR: 7.71e-05\n",
            "  Batch 22000/26069 | Loss: 0.9695 | LR: 7.71e-05\n",
            "  Batch 22100/26069 | Loss: 0.9696 | LR: 7.71e-05\n",
            "  Batch 22200/26069 | Loss: 0.9696 | LR: 7.71e-05\n",
            "  Batch 22300/26069 | Loss: 0.9696 | LR: 7.71e-05\n",
            "  Batch 22400/26069 | Loss: 0.9697 | LR: 7.71e-05\n",
            "  Batch 22500/26069 | Loss: 0.9696 | LR: 7.71e-05\n",
            "  Batch 22600/26069 | Loss: 0.9696 | LR: 7.71e-05\n",
            "  Batch 22700/26069 | Loss: 0.9697 | LR: 7.71e-05\n",
            "  Batch 22800/26069 | Loss: 0.9697 | LR: 7.71e-05\n",
            "  Batch 22900/26069 | Loss: 0.9696 | LR: 7.71e-05\n",
            "  Batch 23000/26069 | Loss: 0.9696 | LR: 7.70e-05\n",
            "  Batch 23100/26069 | Loss: 0.9697 | LR: 7.70e-05\n",
            "  Batch 23200/26069 | Loss: 0.9696 | LR: 7.70e-05\n",
            "  Batch 23300/26069 | Loss: 0.9697 | LR: 7.70e-05\n",
            "  Batch 23400/26069 | Loss: 0.9696 | LR: 7.70e-05\n",
            "  Batch 23500/26069 | Loss: 0.9697 | LR: 7.70e-05\n",
            "  Batch 23600/26069 | Loss: 0.9698 | LR: 7.70e-05\n",
            "  Batch 23700/26069 | Loss: 0.9698 | LR: 7.70e-05\n",
            "  Batch 23800/26069 | Loss: 0.9698 | LR: 7.70e-05\n",
            "  Batch 23900/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 24000/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 24100/26069 | Loss: 0.9698 | LR: 7.70e-05\n",
            "  Batch 24200/26069 | Loss: 0.9698 | LR: 7.70e-05\n",
            "  Batch 24300/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 24400/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 24500/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 24600/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 24700/26069 | Loss: 0.9698 | LR: 7.70e-05\n",
            "  Batch 24800/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 24900/26069 | Loss: 0.9699 | LR: 7.70e-05\n",
            "  Batch 25000/26069 | Loss: 0.9699 | LR: 7.69e-05\n",
            "  Batch 25100/26069 | Loss: 0.9699 | LR: 7.69e-05\n",
            "  Batch 25200/26069 | Loss: 0.9700 | LR: 7.69e-05\n",
            "  Batch 25300/26069 | Loss: 0.9700 | LR: 7.69e-05\n",
            "  Batch 25400/26069 | Loss: 0.9700 | LR: 7.69e-05\n",
            "  Batch 25500/26069 | Loss: 0.9699 | LR: 7.69e-05\n",
            "  Batch 25600/26069 | Loss: 0.9699 | LR: 7.69e-05\n",
            "  Batch 25700/26069 | Loss: 0.9699 | LR: 7.69e-05\n",
            "  Batch 25800/26069 | Loss: 0.9699 | LR: 7.69e-05\n",
            "  Batch 25900/26069 | Loss: 0.9699 | LR: 7.69e-05\n",
            "  Batch 26000/26069 | Loss: 0.9700 | LR: 7.69e-05\n",
            "\n",
            "  Train Loss: 0.9700\n",
            "  Val Loss:   0.9511 [BEST]\n",
            "  Time:       312.7s\n",
            "\n",
            "  ‚úì Saved checkpoint: checkpoints/checkpoint_epoch_32.pt\n",
            "  ‚úì Saved best model: checkpoints/best_model.pt\n",
            "Epoch 33/100\n",
            "  Batch 100/26069 | Loss: 0.9748 | LR: 7.69e-05\n",
            "  Batch 200/26069 | Loss: 0.9708 | LR: 7.69e-05\n",
            "  Batch 300/26069 | Loss: 0.9694 | LR: 7.69e-05\n",
            "  Batch 400/26069 | Loss: 0.9713 | LR: 7.69e-05\n",
            "  Batch 500/26069 | Loss: 0.9737 | LR: 7.69e-05\n",
            "  Batch 600/26069 | Loss: 0.9731 | LR: 7.69e-05\n",
            "  Batch 700/26069 | Loss: 0.9716 | LR: 7.69e-05\n",
            "  Batch 800/26069 | Loss: 0.9705 | LR: 7.69e-05\n",
            "  Batch 900/26069 | Loss: 0.9682 | LR: 7.68e-05\n",
            "  Batch 1000/26069 | Loss: 0.9680 | LR: 7.68e-05\n",
            "  Batch 1100/26069 | Loss: 0.9680 | LR: 7.68e-05\n",
            "  Batch 1200/26069 | Loss: 0.9665 | LR: 7.68e-05\n",
            "  Batch 1300/26069 | Loss: 0.9655 | LR: 7.68e-05\n",
            "  Batch 1400/26069 | Loss: 0.9667 | LR: 7.68e-05\n",
            "  Batch 1500/26069 | Loss: 0.9674 | LR: 7.68e-05\n",
            "  Batch 1600/26069 | Loss: 0.9668 | LR: 7.68e-05\n",
            "  Batch 1700/26069 | Loss: 0.9662 | LR: 7.68e-05\n",
            "  Batch 1800/26069 | Loss: 0.9661 | LR: 7.68e-05\n",
            "  Batch 1900/26069 | Loss: 0.9663 | LR: 7.68e-05\n",
            "  Batch 2000/26069 | Loss: 0.9670 | LR: 7.68e-05\n",
            "  Batch 2100/26069 | Loss: 0.9665 | LR: 7.68e-05\n",
            "  Batch 2200/26069 | Loss: 0.9665 | LR: 7.68e-05\n",
            "  Batch 2300/26069 | Loss: 0.9662 | LR: 7.68e-05\n",
            "  Batch 2400/26069 | Loss: 0.9664 | LR: 7.68e-05\n",
            "  Batch 2500/26069 | Loss: 0.9666 | LR: 7.68e-05\n",
            "  Batch 2600/26069 | Loss: 0.9664 | LR: 7.68e-05\n",
            "  Batch 2700/26069 | Loss: 0.9669 | LR: 7.68e-05\n",
            "  Batch 2800/26069 | Loss: 0.9672 | LR: 7.68e-05\n",
            "  Batch 2900/26069 | Loss: 0.9673 | LR: 7.67e-05\n",
            "  Batch 3000/26069 | Loss: 0.9670 | LR: 7.67e-05\n",
            "  Batch 3100/26069 | Loss: 0.9672 | LR: 7.67e-05\n",
            "  Batch 3200/26069 | Loss: 0.9674 | LR: 7.67e-05\n",
            "  Batch 3300/26069 | Loss: 0.9672 | LR: 7.67e-05\n",
            "  Batch 3400/26069 | Loss: 0.9675 | LR: 7.67e-05\n",
            "  Batch 3500/26069 | Loss: 0.9677 | LR: 7.67e-05\n",
            "  Batch 3600/26069 | Loss: 0.9669 | LR: 7.67e-05\n",
            "  Batch 3700/26069 | Loss: 0.9668 | LR: 7.67e-05\n",
            "  Batch 3800/26069 | Loss: 0.9669 | LR: 7.67e-05\n",
            "  Batch 3900/26069 | Loss: 0.9670 | LR: 7.67e-05\n",
            "  Batch 4000/26069 | Loss: 0.9670 | LR: 7.67e-05\n",
            "  Batch 4100/26069 | Loss: 0.9671 | LR: 7.67e-05\n",
            "  Batch 4200/26069 | Loss: 0.9670 | LR: 7.67e-05\n",
            "  Batch 4300/26069 | Loss: 0.9668 | LR: 7.67e-05\n",
            "  Batch 4400/26069 | Loss: 0.9668 | LR: 7.67e-05\n",
            "  Batch 4500/26069 | Loss: 0.9675 | LR: 7.67e-05\n",
            "  Batch 4600/26069 | Loss: 0.9674 | LR: 7.67e-05\n",
            "  Batch 4700/26069 | Loss: 0.9676 | LR: 7.67e-05\n",
            "  Batch 4800/26069 | Loss: 0.9675 | LR: 7.66e-05\n",
            "  Batch 4900/26069 | Loss: 0.9675 | LR: 7.66e-05\n",
            "  Batch 5000/26069 | Loss: 0.9675 | LR: 7.66e-05\n",
            "  Batch 5100/26069 | Loss: 0.9679 | LR: 7.66e-05\n",
            "  Batch 5200/26069 | Loss: 0.9680 | LR: 7.66e-05\n",
            "  Batch 5300/26069 | Loss: 0.9678 | LR: 7.66e-05\n",
            "  Batch 5400/26069 | Loss: 0.9675 | LR: 7.66e-05\n",
            "  Batch 5500/26069 | Loss: 0.9678 | LR: 7.66e-05\n",
            "  Batch 5600/26069 | Loss: 0.9683 | LR: 7.66e-05\n",
            "  Batch 5700/26069 | Loss: 0.9685 | LR: 7.66e-05\n",
            "  Batch 5800/26069 | Loss: 0.9689 | LR: 7.66e-05\n",
            "  Batch 5900/26069 | Loss: 0.9691 | LR: 7.66e-05\n",
            "  Batch 6000/26069 | Loss: 0.9687 | LR: 7.66e-05\n",
            "  Batch 6100/26069 | Loss: 0.9683 | LR: 7.66e-05\n",
            "  Batch 6200/26069 | Loss: 0.9678 | LR: 7.66e-05\n",
            "  Batch 6300/26069 | Loss: 0.9682 | LR: 7.66e-05\n",
            "  Batch 6400/26069 | Loss: 0.9682 | LR: 7.66e-05\n",
            "  Batch 6500/26069 | Loss: 0.9683 | LR: 7.66e-05\n",
            "  Batch 6600/26069 | Loss: 0.9686 | LR: 7.66e-05\n",
            "  Batch 6700/26069 | Loss: 0.9686 | LR: 7.66e-05\n",
            "  Batch 6800/26069 | Loss: 0.9687 | LR: 7.65e-05\n",
            "  Batch 6900/26069 | Loss: 0.9685 | LR: 7.65e-05\n",
            "  Batch 7000/26069 | Loss: 0.9686 | LR: 7.65e-05\n",
            "  Batch 7100/26069 | Loss: 0.9690 | LR: 7.65e-05\n",
            "  Batch 7200/26069 | Loss: 0.9688 | LR: 7.65e-05\n",
            "  Batch 7300/26069 | Loss: 0.9685 | LR: 7.65e-05\n",
            "  Batch 7400/26069 | Loss: 0.9686 | LR: 7.65e-05\n",
            "  Batch 7500/26069 | Loss: 0.9684 | LR: 7.65e-05\n",
            "  Batch 7600/26069 | Loss: 0.9683 | LR: 7.65e-05\n",
            "  Batch 7700/26069 | Loss: 0.9686 | LR: 7.65e-05\n",
            "  Batch 7800/26069 | Loss: 0.9688 | LR: 7.65e-05\n",
            "  Batch 7900/26069 | Loss: 0.9687 | LR: 7.65e-05\n",
            "  Batch 8000/26069 | Loss: 0.9687 | LR: 7.65e-05\n",
            "  Batch 8100/26069 | Loss: 0.9686 | LR: 7.65e-05\n",
            "  Batch 8200/26069 | Loss: 0.9687 | LR: 7.65e-05\n",
            "  Batch 8300/26069 | Loss: 0.9687 | LR: 7.65e-05\n",
            "  Batch 8400/26069 | Loss: 0.9690 | LR: 7.65e-05\n",
            "  Batch 8500/26069 | Loss: 0.9691 | LR: 7.65e-05\n",
            "  Batch 8600/26069 | Loss: 0.9693 | LR: 7.65e-05\n",
            "  Batch 8700/26069 | Loss: 0.9694 | LR: 7.64e-05\n",
            "  Batch 8800/26069 | Loss: 0.9695 | LR: 7.64e-05\n",
            "  Batch 8900/26069 | Loss: 0.9697 | LR: 7.64e-05\n",
            "  Batch 9000/26069 | Loss: 0.9697 | LR: 7.64e-05\n",
            "  Batch 9100/26069 | Loss: 0.9695 | LR: 7.64e-05\n",
            "  Batch 9200/26069 | Loss: 0.9693 | LR: 7.64e-05\n",
            "  Batch 9300/26069 | Loss: 0.9693 | LR: 7.64e-05\n",
            "  Batch 9400/26069 | Loss: 0.9693 | LR: 7.64e-05\n",
            "  Batch 9500/26069 | Loss: 0.9691 | LR: 7.64e-05\n",
            "  Batch 9600/26069 | Loss: 0.9693 | LR: 7.64e-05\n",
            "  Batch 9700/26069 | Loss: 0.9693 | LR: 7.64e-05\n",
            "  Batch 9800/26069 | Loss: 0.9694 | LR: 7.64e-05\n",
            "  Batch 9900/26069 | Loss: 0.9695 | LR: 7.64e-05\n",
            "  Batch 10000/26069 | Loss: 0.9695 | LR: 7.64e-05\n",
            "  Batch 10100/26069 | Loss: 0.9697 | LR: 7.64e-05\n",
            "  Batch 10200/26069 | Loss: 0.9696 | LR: 7.64e-05\n",
            "  Batch 10300/26069 | Loss: 0.9698 | LR: 7.64e-05\n",
            "  Batch 10400/26069 | Loss: 0.9699 | LR: 7.64e-05\n",
            "  Batch 10500/26069 | Loss: 0.9699 | LR: 7.64e-05\n",
            "  Batch 10600/26069 | Loss: 0.9698 | LR: 7.64e-05\n",
            "  Batch 10700/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 10800/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 10900/26069 | Loss: 0.9698 | LR: 7.63e-05\n",
            "  Batch 11000/26069 | Loss: 0.9699 | LR: 7.63e-05\n",
            "  Batch 11100/26069 | Loss: 0.9699 | LR: 7.63e-05\n",
            "  Batch 11200/26069 | Loss: 0.9700 | LR: 7.63e-05\n",
            "  Batch 11300/26069 | Loss: 0.9699 | LR: 7.63e-05\n",
            "  Batch 11400/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 11500/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 11600/26069 | Loss: 0.9696 | LR: 7.63e-05\n",
            "  Batch 11700/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 11800/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 11900/26069 | Loss: 0.9696 | LR: 7.63e-05\n",
            "  Batch 12000/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 12100/26069 | Loss: 0.9696 | LR: 7.63e-05\n",
            "  Batch 12200/26069 | Loss: 0.9696 | LR: 7.63e-05\n",
            "  Batch 12300/26069 | Loss: 0.9698 | LR: 7.63e-05\n",
            "  Batch 12400/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 12500/26069 | Loss: 0.9697 | LR: 7.63e-05\n",
            "  Batch 12600/26069 | Loss: 0.9699 | LR: 7.62e-05\n",
            "  Batch 12700/26069 | Loss: 0.9698 | LR: 7.62e-05\n",
            "  Batch 12800/26069 | Loss: 0.9697 | LR: 7.62e-05\n",
            "  Batch 12900/26069 | Loss: 0.9698 | LR: 7.62e-05\n",
            "  Batch 13000/26069 | Loss: 0.9699 | LR: 7.62e-05\n",
            "  Batch 13100/26069 | Loss: 0.9697 | LR: 7.62e-05\n",
            "  Batch 13200/26069 | Loss: 0.9694 | LR: 7.62e-05\n",
            "  Batch 13300/26069 | Loss: 0.9695 | LR: 7.62e-05\n",
            "  Batch 13400/26069 | Loss: 0.9695 | LR: 7.62e-05\n",
            "  Batch 13500/26069 | Loss: 0.9692 | LR: 7.62e-05\n",
            "  Batch 13600/26069 | Loss: 0.9692 | LR: 7.62e-05\n",
            "  Batch 13700/26069 | Loss: 0.9692 | LR: 7.62e-05\n",
            "  Batch 13800/26069 | Loss: 0.9693 | LR: 7.62e-05\n",
            "  Batch 13900/26069 | Loss: 0.9695 | LR: 7.62e-05\n",
            "  Batch 14000/26069 | Loss: 0.9696 | LR: 7.62e-05\n",
            "  Batch 14100/26069 | Loss: 0.9695 | LR: 7.62e-05\n",
            "  Batch 14200/26069 | Loss: 0.9697 | LR: 7.62e-05\n",
            "  Batch 14300/26069 | Loss: 0.9697 | LR: 7.62e-05\n",
            "  Batch 14400/26069 | Loss: 0.9696 | LR: 7.62e-05\n",
            "  Batch 14500/26069 | Loss: 0.9696 | LR: 7.62e-05\n",
            "  Batch 14600/26069 | Loss: 0.9696 | LR: 7.61e-05\n",
            "  Batch 14700/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 14800/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 14900/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 15000/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 15100/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 15200/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 15300/26069 | Loss: 0.9696 | LR: 7.61e-05\n",
            "  Batch 15400/26069 | Loss: 0.9695 | LR: 7.61e-05\n",
            "  Batch 15500/26069 | Loss: 0.9695 | LR: 7.61e-05\n",
            "  Batch 15600/26069 | Loss: 0.9695 | LR: 7.61e-05\n",
            "  Batch 15700/26069 | Loss: 0.9695 | LR: 7.61e-05\n",
            "  Batch 15800/26069 | Loss: 0.9696 | LR: 7.61e-05\n",
            "  Batch 15900/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 16000/26069 | Loss: 0.9696 | LR: 7.61e-05\n",
            "  Batch 16100/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 16200/26069 | Loss: 0.9697 | LR: 7.61e-05\n",
            "  Batch 16300/26069 | Loss: 0.9696 | LR: 7.61e-05\n",
            "  Batch 16400/26069 | Loss: 0.9696 | LR: 7.61e-05\n",
            "  Batch 16500/26069 | Loss: 0.9696 | LR: 7.60e-05\n",
            "  Batch 16600/26069 | Loss: 0.9695 | LR: 7.60e-05\n",
            "  Batch 16700/26069 | Loss: 0.9695 | LR: 7.60e-05\n",
            "  Batch 16800/26069 | Loss: 0.9693 | LR: 7.60e-05\n",
            "  Batch 16900/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 17000/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 17100/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 17200/26069 | Loss: 0.9695 | LR: 7.60e-05\n",
            "  Batch 17300/26069 | Loss: 0.9696 | LR: 7.60e-05\n",
            "  Batch 17400/26069 | Loss: 0.9696 | LR: 7.60e-05\n",
            "  Batch 17500/26069 | Loss: 0.9695 | LR: 7.60e-05\n",
            "  Batch 17600/26069 | Loss: 0.9693 | LR: 7.60e-05\n",
            "  Batch 17700/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 17800/26069 | Loss: 0.9693 | LR: 7.60e-05\n",
            "  Batch 17900/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 18000/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 18100/26069 | Loss: 0.9695 | LR: 7.60e-05\n",
            "  Batch 18200/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 18300/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 18400/26069 | Loss: 0.9694 | LR: 7.60e-05\n",
            "  Batch 18500/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 18600/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 18700/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 18800/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 18900/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 19000/26069 | Loss: 0.9693 | LR: 7.59e-05\n",
            "  Batch 19100/26069 | Loss: 0.9693 | LR: 7.59e-05\n",
            "  Batch 19200/26069 | Loss: 0.9693 | LR: 7.59e-05\n",
            "  Batch 19300/26069 | Loss: 0.9693 | LR: 7.59e-05\n",
            "  Batch 19400/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 19500/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 19600/26069 | Loss: 0.9693 | LR: 7.59e-05\n",
            "  Batch 19700/26069 | Loss: 0.9693 | LR: 7.59e-05\n",
            "  Batch 19800/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 19900/26069 | Loss: 0.9694 | LR: 7.59e-05\n",
            "  Batch 20000/26069 | Loss: 0.9695 | LR: 7.59e-05\n",
            "  Batch 20100/26069 | Loss: 0.9696 | LR: 7.59e-05\n",
            "  Batch 20200/26069 | Loss: 0.9696 | LR: 7.59e-05\n",
            "  Batch 20300/26069 | Loss: 0.9695 | LR: 7.59e-05\n",
            "  Batch 20400/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 20500/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 20600/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 20700/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 20800/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 20900/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 21000/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21100/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21200/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21300/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21400/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21500/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21600/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21700/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21800/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 21900/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 22000/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 22100/26069 | Loss: 0.9696 | LR: 7.58e-05\n",
            "  Batch 22200/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 22300/26069 | Loss: 0.9695 | LR: 7.58e-05\n",
            "  Batch 22400/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 22500/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 22600/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 22700/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 22800/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 22900/26069 | Loss: 0.9696 | LR: 7.57e-05\n",
            "  Batch 23000/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 23100/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 23200/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 23300/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 23400/26069 | Loss: 0.9696 | LR: 7.57e-05\n",
            "  Batch 23500/26069 | Loss: 0.9696 | LR: 7.57e-05\n",
            "  Batch 23600/26069 | Loss: 0.9696 | LR: 7.57e-05\n",
            "  Batch 23700/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 23800/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 23900/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 24000/26069 | Loss: 0.9695 | LR: 7.57e-05\n",
            "  Batch 24100/26069 | Loss: 0.9696 | LR: 7.57e-05\n",
            "  Batch 24200/26069 | Loss: 0.9696 | LR: 7.57e-05\n",
            "  Batch 24300/26069 | Loss: 0.9697 | LR: 7.56e-05\n",
            "  Batch 24400/26069 | Loss: 0.9697 | LR: 7.56e-05\n",
            "  Batch 24500/26069 | Loss: 0.9697 | LR: 7.56e-05\n",
            "  Batch 24600/26069 | Loss: 0.9697 | LR: 7.56e-05\n",
            "  Batch 24700/26069 | Loss: 0.9697 | LR: 7.56e-05\n",
            "  Batch 24800/26069 | Loss: 0.9697 | LR: 7.56e-05\n",
            "  Batch 24900/26069 | Loss: 0.9697 | LR: 7.56e-05\n",
            "  Batch 25000/26069 | Loss: 0.9698 | LR: 7.56e-05\n",
            "  Batch 25100/26069 | Loss: 0.9698 | LR: 7.56e-05\n",
            "  Batch 25200/26069 | Loss: 0.9698 | LR: 7.56e-05\n",
            "  Batch 25300/26069 | Loss: 0.9698 | LR: 7.56e-05\n",
            "  Batch 25400/26069 | Loss: 0.9699 | LR: 7.56e-05\n",
            "  Batch 25500/26069 | Loss: 0.9700 | LR: 7.56e-05\n",
            "  Batch 25600/26069 | Loss: 0.9700 | LR: 7.56e-05\n",
            "  Batch 25700/26069 | Loss: 0.9700 | LR: 7.56e-05\n",
            "  Batch 25800/26069 | Loss: 0.9699 | LR: 7.56e-05\n",
            "  Batch 25900/26069 | Loss: 0.9699 | LR: 7.56e-05\n"
          ]
        }
      ],
      "source": [
        "#@title 4. Train Model { display-mode: \"form\" }\n",
        "#@markdown Training will begin with the configuration from Step 2.\n",
        "#@markdown\n",
        "#@markdown **What to watch for:**\n",
        "#@markdown - Loss should decrease over epochs\n",
        "#@markdown - Val loss < 1.0 is good progress\n",
        "#@markdown - Val loss < 0.85 is excellent\n",
        "#@markdown - `[BEST]` indicates a new best checkpoint was saved\n",
        "#@markdown - **Stop if val loss starts rising** (overfitting)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {config['model_size']} | Epochs: {config['num_epochs']}\")\n",
        "if config['resume']:\n",
        "    print(\"Resuming from last checkpoint...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Build training command\n",
        "# Note: batch-size defaults to 64 in train.py\n",
        "cmd = f\"python training/train.py --model-size {config['model_size']} --num-epochs {config['num_epochs']}\"\n",
        "if config['resume']:\n",
        "    cmd += \" --resume\"\n",
        "\n",
        "# Run training\n",
        "!{cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nüìÅ Saved checkpoints:\")\n",
        "!ls -lh checkpoints/*.pt 2>/dev/null | tail -5\n",
        "print(\"\\nBest model saved to: checkpoints/best_model.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c5d8599"
      },
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "repo_path = '/content/Symbolic-Transformers'\n",
        "output_zip = '/content/Symbolic-Transformers_patched.zip'\n",
        "\n",
        "print(f\"üì¶ Creating zip archive of '{repo_path}'...\")\n",
        "!zip -r -q {output_zip} {repo_path}\n",
        "\n",
        "if os.path.exists(output_zip):\n",
        "    print(f\"‚úÖ Archive created: {output_zip}\")\n",
        "    files.download(output_zip)\n",
        "else:\n",
        "    print(f\"‚ùå Failed to create archive at {output_zip}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK5pG240TFHK"
      },
      "source": [
        "## 5Ô∏è‚É£ Evaluate Model (Optional)\n",
        "Run evaluation on the test set to see accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqNCfzUXTFHK"
      },
      "outputs": [],
      "source": [
        "#@title 5. Evaluate Model { display-mode: \"form\" }\n",
        "#@markdown Run evaluation on the test set.\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"üìä Evaluating model on test set...\\n\")\n",
        "\n",
        "!python evaluate_model.py \\\n",
        "    --checkpoint checkpoints/best_model.pt \\\n",
        "    --test-data datasets/fol_next_symbol/test.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7uXvIWTFHK"
      },
      "source": [
        "## 6Ô∏è‚É£ Download Trained Model (Optional)\n",
        "Download the trained model checkpoint to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uImTbmmlTFHK"
      },
      "outputs": [],
      "source": [
        "#@title 6. Download Model { display-mode: \"form\" }\n",
        "#@markdown Download the best model checkpoint.\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "checkpoint_path = '/content/Symbolic-Transformers/checkpoints/best_model.pt'\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"üì¶ Preparing download...\")\n",
        "    file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
        "    print(f\"   File: best_model.pt ({file_size:.1f} MB)\")\n",
        "    print(f\"   Model: {config['model_size']}\")\n",
        "    print()\n",
        "    files.download(checkpoint_path)\n",
        "else:\n",
        "    print(\"‚ùå No checkpoint found. Run training first (Step 4).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2xHc6xzTFHK"
      },
      "source": [
        "## 7Ô∏è‚É£ Interactive Demo (Optional)\n",
        "Try the model interactively - type tokens and see predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ocH3f-STFHK"
      },
      "outputs": [],
      "source": [
        "#@title 7. Quick Demo { display-mode: \"form\" }\n",
        "#@markdown See the model's predictions for a sample input.\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "sys.path.insert(0, '/content/Symbolic-Transformers')\n",
        "\n",
        "from utils.vocabulary import Vocabulary\n",
        "from models.transformer import SymbolicTransformer, get_model_config\n",
        "\n",
        "# Load model\n",
        "print(\"üîÑ Loading model...\")\n",
        "vocab = Vocabulary('unified_vocabulary.json')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "checkpoint = torch.load('checkpoints/best_model.pt', map_location=device, weights_only=False)\n",
        "model_config = get_model_config(checkpoint['config']['model_size'], vocab.vocab_size)\n",
        "model = SymbolicTransformer(**model_config).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úì Loaded {checkpoint['config']['model_size']} model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Val loss: {checkpoint['best_val_loss']:.4f}\\n\")\n",
        "\n",
        "# Demo predictions\n",
        "def predict_next(tokens, top_k=5):\n",
        "    \"\"\"Predict next token given a sequence.\"\"\"\n",
        "    token_ids = [vocab.encode_label(t) if t in vocab.label_to_id else int(t) for t in tokens]\n",
        "    x = torch.tensor([token_ids], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = torch.softmax(logits[0, -1], dim=-1)\n",
        "        top_probs, top_ids = torch.topk(probs, top_k)\n",
        "\n",
        "    print(f\"Input: {' '.join(tokens)}\")\n",
        "    print(f\"\\nTop {top_k} predictions:\")\n",
        "    for i, (prob, tid) in enumerate(zip(top_probs, top_ids)):\n",
        "        label = vocab.decode_id(tid.item())\n",
        "        bar = '‚ñà' * int(prob * 30)\n",
        "        print(f\"  {i+1}. {label:12s} {prob*100:5.1f}% {bar}\")\n",
        "    print()\n",
        "\n",
        "# Show example predictions\n",
        "print(\"=\"*50)\n",
        "print(\"üìä EXAMPLE PREDICTIONS\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# After FORALL, should predict VAR with high confidence\n",
        "predict_next(['FORALL'])\n",
        "\n",
        "# After FORALL VAR, should predict a numeral\n",
        "predict_next(['FORALL', 'VAR'])\n",
        "\n",
        "# After a complete variable binding\n",
        "predict_next(['FORALL', 'VAR', '1'])\n",
        "\n",
        "# After predicate (should predict LPAREN)\n",
        "predict_next(['PRED', '3'])\n",
        "\n",
        "print(\"\\nüí° The model learned FOL syntax rules!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N6cg1OvTFHL"
      },
      "source": [
        "---\n",
        "\n",
        "## üìñ Quick Reference\n",
        "\n",
        "### Model Sizes\n",
        "| Size | Parameters | File Size | Time/Epoch (A100) | Capacity |\n",
        "|------|-----------|-----------|-------------------|----------|\n",
        "| tiny | 566K | ~2.2MB | ~30s | Good for <10K formulas |\n",
        "| small | 3.5M | ~14MB | ~90s | Good for 10-50K formulas |\n",
        "| base | 19.6M | ~78MB | ~180s | Good for 50K+ formulas |\n",
        "\n",
        "### Data Generator Comparison\n",
        "| Feature | Basic | Advanced |\n",
        "|---------|-------|----------|\n",
        "| Predicates `P(x, y)` | ‚úì | ‚úì |\n",
        "| Functions `f(x)` | ‚úó | ‚úì |\n",
        "| Nested terms `P(f(g(x)))` | ‚úó | ‚úì |\n",
        "| Fixed arities | ‚úó | ‚úì |\n",
        "| Horn clauses | ‚úó | ‚úì |\n",
        "| Vacuous quantification | ‚úó | ‚úì |\n",
        "\n",
        "### Recommended Configurations\n",
        "\n",
        "**Quick Test (5-10 min):**\n",
        "- Model: tiny\n",
        "- Data: 1000 formulas (basic)\n",
        "- Epochs: 20\n",
        "\n",
        "**Standard Training (30-60 min):**\n",
        "- Model: small\n",
        "- Data: 10000 formulas (advanced)\n",
        "- Epochs: 50-100\n",
        "\n",
        "**Best Results (2+ hours):**\n",
        "- Model: base\n",
        "- Data: 30000+ formulas (advanced)\n",
        "- Epochs: 100-200 (watch val loss!)\n",
        "\n",
        "### Interpreting Results\n",
        "- **Val Loss > 1.5**: Model is still learning basic patterns\n",
        "- **Val Loss 1.0-1.5**: Good progress, learning syntax rules\n",
        "- **Val Loss 0.85-1.0**: Excellent, model understands FOL structure\n",
        "- **Val Loss < 0.85**: Very good, approaching optimal\n",
        "\n",
        "### ‚ö†Ô∏è Overfitting Warning Signs\n",
        "- Train loss keeps dropping but val loss stops improving\n",
        "- Val loss starts **increasing** while train loss decreases\n",
        "- Gap between train and val loss > 0.1\n",
        "\n",
        "**If overfitting:**\n",
        "1. Stop training and use the checkpoint with lowest val loss\n",
        "2. Generate more training data\n",
        "3. Enable advanced generator for richer patterns\n",
        "\n",
        "### Files Created\n",
        "- `checkpoints/best_model.pt` - Best performing model\n",
        "- `checkpoints/checkpoint_epoch_N.pt` - Periodic checkpoints\n",
        "- `datasets/fol_next_symbol/` - Training data"
      ]
    }
  ]
}