{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripptytrip/Symbolic-Transformers/blob/main/train_symbolic_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGtE5_xgTFHG"
      },
      "source": [
        "# ðŸ§  Symbolic Transformer Training\n",
        "\n",
        "Train a tiny transformer to predict next symbols in First-Order Logic formulas.\n",
        "\n",
        "**What this does:**\n",
        "- Generates synthetic FOL training data\n",
        "- Trains a small transformer model (566K - 14M parameters)\n",
        "- Learns syntax rules like: `âˆ€` â†’ must be followed by `VAR`\n",
        "\n",
        "**Quick Start:** Run cells 1-4 in order. Training takes ~3 min/epoch on GPU.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF64tUdlTFHI"
      },
      "source": [
        "## 1ï¸âƒ£ Setup Environment\n",
        "Clone the repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzVNpxbRTFHI",
        "outputId": "0cc04620-38f7-449e-cd29-ecee12b5a9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Checking GPU availability...\n",
            "/bin/bash: -c: line 1: syntax error near unexpected token `\"âš ï¸ No GPU detected - training will be slower\"'\n",
            "/bin/bash: -c: line 1: `nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || print(\"âš ï¸ No GPU detected - training will be slower\")'\n",
            "\n",
            "ðŸ“¦ Cloning Symbolic-Transformers repository...\n",
            "/content\n",
            "âœ“ Repository cloned\n",
            "/content/Symbolic-Transformers\n",
            "\n",
            "ðŸ“š Installing dependencies...\n",
            "âœ“ Dependencies installed\n",
            "\n",
            "ðŸ”¤ Verifying vocabulary...\n",
            "âœ“ Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "âœ“ Vocabulary loaded: 662 tokens\n",
            "\n",
            "==================================================\n",
            "âœ… Setup complete! Proceed to Step 2.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 1. Setup Environment { display-mode: \"form\" }\n",
        "#@markdown Run this cell first to set up the environment.\n",
        "\n",
        "import os\n",
        "\n",
        "# Check GPU\n",
        "print(\"ðŸ” Checking GPU availability...\")\n",
        "!nvidia-smi --query-gpu=name,memory.total --format=csv,noheader 2>/dev/null || print(\"âš ï¸ No GPU detected - training will be slower\")\n",
        "\n",
        "# Clone repository\n",
        "print(\"\\nðŸ“¦ Cloning Symbolic-Transformers repository...\")\n",
        "%cd /content\n",
        "if not os.path.exists('Symbolic-Transformers'):\n",
        "    !git clone -q https://github.com/tripptytrip/Symbolic-Transformers.git\n",
        "    print(\"âœ“ Repository cloned\")\n",
        "else:\n",
        "    !cd Symbolic-Transformers && git pull -q\n",
        "    print(\"âœ“ Repository updated\")\n",
        "\n",
        "%cd /content/Symbolic-Transformers\n",
        "\n",
        "# Install dependencies\n",
        "print(\"\\nðŸ“š Installing dependencies...\")\n",
        "!pip -q install numpy scipy pandas tqdm rich tensorboard\n",
        "print(\"âœ“ Dependencies installed\")\n",
        "\n",
        "# Verify vocabulary\n",
        "print(\"\\nðŸ”¤ Verifying vocabulary...\")\n",
        "!python -c \"from utils.vocabulary import Vocabulary; v = Vocabulary('unified_vocabulary.json'); print(f'âœ“ Vocabulary loaded: {v.vocab_size} tokens')\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Setup complete! Proceed to Step 2.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPKqzjMATFHI"
      },
      "source": [
        "## 2ï¸âƒ£ Configure Training\n",
        "Adjust the settings below, then run the cell to apply them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7xB8AeHTFHJ",
        "outputId": "6ce6103c-ce89-4afc-ab7d-bdb60abb8568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "ðŸ“‹ TRAINING CONFIGURATION\n",
            "==================================================\n",
            "\n",
            "ðŸ—ï¸  Model:     base (14M parameters, ~56MB)\n",
            "ðŸ“Š Dataset:   10000 train / 1000 val formulas\n",
            "âš™ï¸  Training:  200 epochs\n",
            "ðŸ”„ Resume:    No (fresh start)\n",
            "\n",
            "â±ï¸  Estimated training time: ~364.5 minutes on GPU\n",
            "   (109.4s per epoch, 2187 batches)\n",
            "\n",
            "==================================================\n",
            "âœ… Configuration saved! Proceed to Step 3.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 2. Training Configuration { display-mode: \"form\" }\n",
        "\n",
        "#@markdown ### ðŸ—ï¸ Model Size\n",
        "model_size = \"base\" #@param [\"tiny\", \"small\", \"base\"]\n",
        "#@markdown - **tiny**: 566K params (~2.2MB) - Fast training, good for experiments\n",
        "#@markdown - **small**: 3.5M params (~14MB) - Better accuracy, moderate training time\n",
        "#@markdown - **base**: 14M params (~56MB) - Best accuracy, longest training\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸ“Š Dataset Size\n",
        "num_train_formulas = 10000 #@param {type:\"slider\", min:100, max:10000, step:100}\n",
        "#@markdown Number of unique FOL formulas to generate for training.\n",
        "#@markdown - 100-500: Quick test runs\n",
        "#@markdown - 1000-3000: Standard training\n",
        "#@markdown - 5000-10000: Large-scale training (recommended for best results)\n",
        "\n",
        "num_val_formulas = 1000 #@param {type:\"slider\", min:50, max:1000, step:50}\n",
        "#@markdown Number of formulas for validation (typically 10-20% of training).\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### âš™ï¸ Training Parameters\n",
        "num_epochs = 200 #@param {type:\"slider\", min:10, max:1000, step:10}\n",
        "#@markdown Number of training epochs.\n",
        "#@markdown - 10-50: Quick experiments\n",
        "#@markdown - 100-300: Standard training\n",
        "#@markdown - 500-1000: Train to convergence (watch for overfitting!)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ðŸ’¾ Resume from Checkpoint\n",
        "resume_training = False #@param {type:\"boolean\"}\n",
        "#@markdown Resume from the last saved checkpoint.\n",
        "\n",
        "# Store configuration\n",
        "config = {\n",
        "    'model_size': model_size,\n",
        "    'num_train_formulas': num_train_formulas,\n",
        "    'num_val_formulas': num_val_formulas,\n",
        "    'num_test_formulas': max(50, num_val_formulas // 2),\n",
        "    'num_epochs': num_epochs,\n",
        "    'resume': resume_training\n",
        "}\n",
        "\n",
        "# Display configuration summary\n",
        "print(\"=\"*50)\n",
        "print(\"ðŸ“‹ TRAINING CONFIGURATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "model_params = {'tiny': '566K', 'small': '3.5M', 'base': '14M'}\n",
        "model_size_mb = {'tiny': '~2.2MB', 'small': '~14MB', 'base': '~56MB'}\n",
        "\n",
        "print(f\"\\nðŸ—ï¸  Model:     {model_size} ({model_params[model_size]} parameters, {model_size_mb[model_size]})\")\n",
        "print(f\"ðŸ“Š Dataset:   {num_train_formulas} train / {num_val_formulas} val formulas\")\n",
        "print(f\"âš™ï¸  Training:  {num_epochs} epochs\")\n",
        "print(f\"ðŸ”„ Resume:    {'Yes' if resume_training else 'No (fresh start)'}\")\n",
        "\n",
        "# Estimate training time\n",
        "samples_per_formula = 14  # Average tokens per formula\n",
        "total_samples = num_train_formulas * samples_per_formula\n",
        "batch_size = 64  # Default in train.py\n",
        "batches_per_epoch = total_samples // batch_size\n",
        "time_per_batch = {'tiny': 0.012, 'small': 0.025, 'base': 0.05}  # seconds on T4\n",
        "est_epoch_time = batches_per_epoch * time_per_batch[model_size]\n",
        "est_total_time = est_epoch_time * num_epochs / 60\n",
        "\n",
        "print(f\"\\nâ±ï¸  Estimated training time: ~{est_total_time:.1f} minutes on GPU\")\n",
        "print(f\"   ({est_epoch_time:.1f}s per epoch, {batches_per_epoch} batches)\")\n",
        "\n",
        "# Overfitting warning\n",
        "if num_epochs > 200 and num_train_formulas < 3000:\n",
        "    print(f\"\\nâš ï¸  WARNING: High epochs ({num_epochs}) with small dataset ({num_train_formulas})\")\n",
        "    print(f\"   Risk of overfitting! Consider:\")\n",
        "    print(f\"   - Reducing epochs to 100-200\")\n",
        "    print(f\"   - Or increasing training formulas to 5000+\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Configuration saved! Proceed to Step 3.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VJA9pkTFHJ"
      },
      "source": [
        "## 3ï¸âƒ£ Generate Training Data\n",
        "Create synthetic First-Order Logic formulas for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a25zb42qTFHJ",
        "outputId": "b996cff2-8d62-4843-87b0-d038c2256b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Generating training data...\n",
            "   Train: 10000 formulas\n",
            "   Val:   1000 formulas\n",
            "   Test:  500 formulas\n",
            "\n",
            "============================================================\n",
            "FOL DATASET GENERATION\n",
            "============================================================\n",
            "âœ“ Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "\n",
            "Generating train set (10000 formulas)...\n",
            "  Complexity 1: 2000 formulas\n",
            "  Complexity 2: 4000 formulas\n",
            "  Complexity 3: 3000 formulas\n",
            "  Complexity 4: 1000 formulas\n",
            "âœ“ Saved 144686 samples to datasets/fol_next_symbol/train.json\n",
            "\n",
            "Generating val set (1000 formulas)...\n",
            "  Complexity 1: 200 formulas\n",
            "  Complexity 2: 400 formulas\n",
            "  Complexity 3: 300 formulas\n",
            "  Complexity 4: 100 formulas\n",
            "âœ“ Saved 14614 samples to datasets/fol_next_symbol/val.json\n",
            "\n",
            "Generating test set (500 formulas)...\n",
            "  Complexity 1: 100 formulas\n",
            "  Complexity 2: 200 formulas\n",
            "  Complexity 3: 150 formulas\n",
            "  Complexity 4: 50 formulas\n",
            "âœ“ Saved 6895 samples to datasets/fol_next_symbol/test.json\n",
            "\n",
            "============================================================\n",
            "âœ“ Dataset generation complete!\n",
            "âœ“ Output directory: datasets/fol_next_symbol\n",
            "âœ“ Train samples: 144686\n",
            "âœ“ Val samples: 14614\n",
            "âœ“ Test samples: 6895\n",
            "============================================================\n",
            "\n",
            "ðŸ“ Dataset files:\n",
            "-rw-r--r-- 1 root root  399 Dec 20 10:28 datasets/fol_next_symbol/metadata.json\n",
            "-rw-r--r-- 1 root root 1.5M Dec 20 10:28 datasets/fol_next_symbol/test.json\n",
            "-rw-r--r-- 1 root root  32M Dec 20 10:28 datasets/fol_next_symbol/train.json\n",
            "-rw-r--r-- 1 root root 3.3M Dec 20 10:28 datasets/fol_next_symbol/val.json\n",
            "\n",
            "==================================================\n",
            "âœ… Data generated! Proceed to Step 4.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "#@title 3. Generate Training Data { display-mode: \"form\" }\n",
        "#@markdown This generates synthetic FOL formulas like:\n",
        "#@markdown - `âˆ€xâ‚ (Pâ‚…(xâ‚) â†’ Qâ‚‚(xâ‚))`\n",
        "#@markdown - `âˆƒxâ‚€ âˆƒxâ‚ (Râ‚ƒ(xâ‚€, xâ‚) âˆ§ Pâ‚(xâ‚€))`\n",
        "\n",
        "print(\"ðŸ”„ Generating training data...\")\n",
        "print(f\"   Train: {config['num_train_formulas']} formulas\")\n",
        "print(f\"   Val:   {config['num_val_formulas']} formulas\")\n",
        "print(f\"   Test:  {config['num_test_formulas']} formulas\")\n",
        "print()\n",
        "\n",
        "# Generate data using the dataset generator\n",
        "import sys\n",
        "sys.path.insert(0, '/content/Symbolic-Transformers')\n",
        "\n",
        "from data.dataset_generator import generate_training_data\n",
        "\n",
        "generate_training_data(\n",
        "    vocab_path=\"unified_vocabulary.json\",\n",
        "    output_dir=\"datasets/fol_next_symbol\",\n",
        "    n_train=config['num_train_formulas'],\n",
        "    n_val=config['num_val_formulas'],\n",
        "    n_test=config['num_test_formulas'],\n",
        ")\n",
        "\n",
        "# Show dataset stats\n",
        "print(\"\\nðŸ“ Dataset files:\")\n",
        "!ls -lh datasets/fol_next_symbol/*.json\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Data generated! Proceed to Step 4.\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI2H8LpcTFHJ"
      },
      "source": [
        "## 4ï¸âƒ£ Train the Model\n",
        "Start training! Watch the loss decrease over epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBTsqEasTFHJ",
        "outputId": "0e0a8ea4-291f-4d27-9270-48670ec47082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ðŸš€ STARTING TRAINING\n",
            "============================================================\n",
            "Model: base | Epochs: 200\n",
            "============================================================\n",
            "\n",
            "âœ“ Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "\n",
            "âœ“ Loaded vocabulary: 662 tokens\n",
            "\n",
            "Loading datasets...\n",
            "âœ“ Loaded 144686 samples from datasets/fol_next_symbol/train.json\n",
            "âœ“ Loaded 14614 samples from datasets/fol_next_symbol/val.json\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "âœ“ Train batches: 9043\n",
            "âœ“ Val batches: 914\n",
            "\n",
            "Creating model...\n",
            "âœ“ Created base model with 19,593,878 parameters\n",
            "âœ“ Using device: cuda\n",
            "âœ“ GPU: Tesla T4\n",
            "âœ“ VRAM: 15.8 GB\n",
            "\n",
            "============================================================\n",
            "TRAINING START\n",
            "============================================================\n",
            "Model: base\n",
            "Vocab size: 662\n",
            "Batch size: 16\n",
            "Epochs: 200\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "Epoch 1/200\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  Batch 100/9043 | Loss: 5.7431 | LR: 5.00e-06\n",
            "  Batch 200/9043 | Loss: 4.7619 | LR: 1.00e-05\n",
            "  Batch 300/9043 | Loss: 4.2362 | LR: 1.50e-05\n",
            "  Batch 400/9043 | Loss: 3.8724 | LR: 2.00e-05\n",
            "  Batch 500/9043 | Loss: 3.6071 | LR: 2.50e-05\n",
            "  Batch 600/9043 | Loss: 3.4038 | LR: 3.00e-05\n",
            "  Batch 700/9043 | Loss: 3.2243 | LR: 3.50e-05\n",
            "  Batch 800/9043 | Loss: 3.0599 | LR: 4.00e-05\n",
            "  Batch 900/9043 | Loss: 2.9179 | LR: 4.50e-05\n",
            "  Batch 1000/9043 | Loss: 2.7833 | LR: 5.00e-05\n",
            "  Batch 1100/9043 | Loss: 2.6614 | LR: 5.50e-05\n",
            "  Batch 1200/9043 | Loss: 2.5517 | LR: 6.00e-05\n",
            "  Batch 1300/9043 | Loss: 2.4558 | LR: 6.50e-05\n",
            "  Batch 1400/9043 | Loss: 2.3707 | LR: 7.00e-05\n",
            "  Batch 1500/9043 | Loss: 2.2936 | LR: 7.50e-05\n",
            "  Batch 1600/9043 | Loss: 2.2255 | LR: 8.00e-05\n",
            "  Batch 1700/9043 | Loss: 2.1642 | LR: 8.50e-05\n",
            "  Batch 1800/9043 | Loss: 2.1099 | LR: 9.00e-05\n"
          ]
        }
      ],
      "source": [
        "#@title 4. Train Model { display-mode: \"form\" }\n",
        "#@markdown Training will begin with the configuration from Step 2.\n",
        "#@markdown\n",
        "#@markdown **What to watch for:**\n",
        "#@markdown - Loss should decrease over epochs\n",
        "#@markdown - Val loss < 1.0 is good progress\n",
        "#@markdown - Val loss < 0.9 is excellent\n",
        "#@markdown - `[BEST]` indicates a new best checkpoint was saved\n",
        "#@markdown - **Stop if val loss starts rising** (overfitting)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"ðŸš€ STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {config['model_size']} | Epochs: {config['num_epochs']}\")\n",
        "if config['resume']:\n",
        "    print(\"Resuming from last checkpoint...\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Build training command with only supported arguments\n",
        "cmd = f\"python training/train.py --model-size {config['model_size']} --num-epochs {config['num_epochs']}\"\n",
        "if config['resume']:\n",
        "    cmd += \" --resume\"\n",
        "\n",
        "# Run training\n",
        "!{cmd}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nðŸ“ Saved checkpoints:\")\n",
        "!ls -lh checkpoints/*.pt 2>/dev/null | tail -5\n",
        "print(\"\\nBest model saved to: checkpoints/best_model.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK5pG240TFHK"
      },
      "source": [
        "## 5ï¸âƒ£ Evaluate Model (Optional)\n",
        "Run evaluation on the test set to see accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqNCfzUXTFHK"
      },
      "outputs": [],
      "source": [
        "#@title 5. Evaluate Model { display-mode: \"form\" }\n",
        "#@markdown Run evaluation on the test set.\n",
        "\n",
        "import os\n",
        "os.chdir('/content/Symbolic-Transformers')\n",
        "\n",
        "print(\"ðŸ“Š Evaluating model on test set...\\n\")\n",
        "\n",
        "!python evaluate_model.py \\\n",
        "    --checkpoint checkpoints/best_model.pt \\\n",
        "    --test-data datasets/fol_next_symbol/test.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7uXvIWTFHK"
      },
      "source": [
        "## 6ï¸âƒ£ Download Trained Model (Optional)\n",
        "Download the trained model checkpoint to your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uImTbmmlTFHK"
      },
      "outputs": [],
      "source": [
        "#@title 6. Download Model { display-mode: \"form\" }\n",
        "#@markdown Download the best model checkpoint.\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "checkpoint_path = '/content/Symbolic-Transformers/checkpoints/best_model.pt'\n",
        "\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"ðŸ“¦ Preparing download...\")\n",
        "    file_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
        "    print(f\"   File: best_model.pt ({file_size:.1f} MB)\")\n",
        "    print(f\"   Model: {config['model_size']}\")\n",
        "    print()\n",
        "    files.download(checkpoint_path)\n",
        "else:\n",
        "    print(\"âŒ No checkpoint found. Run training first (Step 4).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2xHc6xzTFHK"
      },
      "source": [
        "## 7ï¸âƒ£ Interactive Demo (Optional)\n",
        "Try the model interactively - type tokens and see predictions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ocH3f-STFHK"
      },
      "outputs": [],
      "source": [
        "#@title 7. Quick Demo { display-mode: \"form\" }\n",
        "#@markdown See the model's predictions for a sample input.\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "sys.path.insert(0, '/content/Symbolic-Transformers')\n",
        "\n",
        "from utils.vocabulary import Vocabulary\n",
        "from models.transformer import SymbolicTransformer, get_model_config\n",
        "\n",
        "# Load model\n",
        "print(\"ðŸ”„ Loading model...\")\n",
        "vocab = Vocabulary('unified_vocabulary.json')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "checkpoint = torch.load('checkpoints/best_model.pt', map_location=device, weights_only=False)\n",
        "model_config = get_model_config(checkpoint['config']['model_size'], vocab.vocab_size)\n",
        "model = SymbolicTransformer(**model_config).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"âœ“ Loaded {checkpoint['config']['model_size']} model from epoch {checkpoint['epoch']}\")\n",
        "print(f\"  Val loss: {checkpoint['best_val_loss']:.4f}\\n\")\n",
        "\n",
        "# Demo predictions\n",
        "def predict_next(tokens, top_k=5):\n",
        "    \"\"\"Predict next token given a sequence.\"\"\"\n",
        "    token_ids = [vocab.encode_label(t) if t in vocab.label_to_id else int(t) for t in tokens]\n",
        "    x = torch.tensor([token_ids], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(x)\n",
        "        probs = torch.softmax(logits[0, -1], dim=-1)\n",
        "        top_probs, top_ids = torch.topk(probs, top_k)\n",
        "\n",
        "    print(f\"Input: {' '.join(tokens)}\")\n",
        "    print(f\"\\nTop {top_k} predictions:\")\n",
        "    for i, (prob, tid) in enumerate(zip(top_probs, top_ids)):\n",
        "        label = vocab.decode_id(tid.item())\n",
        "        bar = 'â–ˆ' * int(prob * 30)\n",
        "        print(f\"  {i+1}. {label:12s} {prob*100:5.1f}% {bar}\")\n",
        "    print()\n",
        "\n",
        "# Show example predictions\n",
        "print(\"=\"*50)\n",
        "print(\"ðŸ“Š EXAMPLE PREDICTIONS\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# After FORALL, should predict VAR with high confidence\n",
        "predict_next(['FORALL'])\n",
        "\n",
        "# After FORALL VAR, should predict a numeral\n",
        "predict_next(['FORALL', 'VAR'])\n",
        "\n",
        "# After a complete variable binding\n",
        "predict_next(['FORALL', 'VAR', '1'])\n",
        "\n",
        "print(\"\\nðŸ’¡ The model learned that FORALL must be followed by VAR!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N6cg1OvTFHL"
      },
      "source": [
        "---\n",
        "\n",
        "## ðŸ“– Quick Reference\n",
        "\n",
        "### Model Sizes\n",
        "| Size | Parameters | File Size | Training Speed | Accuracy |\n",
        "|------|-----------|-----------|----------------|----------|\n",
        "| tiny | 566K | ~2.2MB | ~3s/epoch | Good |\n",
        "| small | 3.5M | ~14MB | ~8s/epoch | Better |\n",
        "| base | 14M | ~56MB | ~20s/epoch | Best |\n",
        "\n",
        "### Recommended Configurations\n",
        "\n",
        "**Quick Test (5 min):**\n",
        "- Model: tiny\n",
        "- Data: 500 formulas\n",
        "- Epochs: 20\n",
        "\n",
        "**Standard Training (30 min):**\n",
        "- Model: tiny\n",
        "- Data: 1000 formulas  \n",
        "- Epochs: 100\n",
        "\n",
        "**Best Results (2+ hours):**\n",
        "- Model: small\n",
        "- Data: 5000+ formulas\n",
        "- Epochs: 200-300 (watch val loss!)\n",
        "\n",
        "### Interpreting Results\n",
        "- **Val Loss > 2.0**: Model is still learning basic patterns\n",
        "- **Val Loss 1.0-2.0**: Good progress, learning syntax rules\n",
        "- **Val Loss < 1.0**: Excellent, model understands FOL structure\n",
        "- **Val Loss < 0.9**: Very good, approaching capacity limits\n",
        "\n",
        "### âš ï¸ Overfitting Warning Signs\n",
        "- Train loss keeps dropping but val loss stops improving\n",
        "- Val loss starts **increasing** while train loss decreases\n",
        "- Gap between train and val loss > 0.2\n",
        "\n",
        "**If overfitting:** Stop training and use the checkpoint with lowest val loss.\n",
        "\n",
        "### Files Created\n",
        "- `checkpoints/best_model.pt` - Best performing model\n",
        "- `checkpoints/checkpoint_epoch_N.pt` - Periodic checkpoints\n",
        "- `datasets/fol_next_symbol/` - Training data"
      ]
    }
  ]
}