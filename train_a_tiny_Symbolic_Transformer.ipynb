{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPH/CnrVMSGoRPboKIDfoAy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tripptytrip/Symbolic-Transformers/blob/main/train_a_tiny_Symbolic_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv8qnGepHdG4",
        "outputId": "0ce18c7e-1420-4ec6-b8c5-bd1ee8e189a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 20 09:36:42 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/tripptytrip/Symbolic-Transformers.git\n",
        "%cd /content/Symbolic-Transformers\n",
        "!ls -lah\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFI8ry8fH-rZ",
        "outputId": "5d153c30-ada7-48cc-b42c-879f2f76a14e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Symbolic-Transformers'...\n",
            "remote: Enumerating objects: 795, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 795 (delta 3), reused 6 (delta 3), pack-reused 786 (from 2)\u001b[K\n",
            "Receiving objects: 100% (795/795), 344.93 MiB | 36.72 MiB/s, done.\n",
            "Resolving deltas: 100% (630/630), done.\n",
            "Updating files: 100% (738/738), done.\n",
            "/content/Symbolic-Transformers\n",
            "total 320K\n",
            "drwxr-xr-x 11 root root 4.0K Dec 20 09:38 .\n",
            "drwxr-xr-x  1 root root 4.0K Dec 20 09:38 ..\n",
            "drwxr-xr-x  2 root root 4.0K Dec 20 09:38 checkpoints\n",
            "drwxr-xr-x  3 root root 4.0K Dec 20 09:38 data\n",
            "drwxr-xr-x  3 root root 4.0K Dec 20 09:38 datasets\n",
            "-rw-r--r--  1 root root 7.8K Dec 20 09:38 evaluate_model.py\n",
            "drwxr-xr-x  2 root root 4.0K Dec 20 09:38 evaluation\n",
            "-rw-r--r--  1 root root 6.1K Dec 20 09:38 GETTING_STARTED.md\n",
            "drwxr-xr-x  8 root root 4.0K Dec 20 09:38 .git\n",
            "-rw-r--r--  1 root root 1.1K Dec 20 09:38 LICENSE\n",
            "drwxr-xr-x  3 root root 4.0K Dec 20 09:38 models\n",
            "drwxr-xr-x  4 root root 4.0K Dec 20 09:38 out\n",
            "-rw-r--r--  1 root root 6.3K Dec 20 09:38 quickstart.py\n",
            "-rw-r--r--  1 root root  15K Dec 20 09:38 README.md\n",
            "-rw-r--r--  1 root root  300 Dec 20 09:38 requirements.txt\n",
            "-rw-r--r--  1 root root  16K Dec 20 09:38 TECHNICAL_ARCHITECTURE.md\n",
            "drwxr-xr-x  3 root root 4.0K Dec 20 09:38 training\n",
            "-rw-r--r--  1 root root 211K Dec 20 09:38 unified_vocabulary.json\n",
            "drwxr-xr-x  3 root root 4.0K Dec 20 09:38 utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install numpy scipy pandas tqdm rich tensorboard"
      ],
      "metadata": {
        "id": "GBFpNQxvIiqr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Symbolic-Transformers\n",
        "!python - <<'PY'\n",
        "from utils.vocabulary import Vocabulary\n",
        "v = Vocabulary(\"unified_vocabulary.json\")\n",
        "print(\"vocab_size =\", v.vocab_size)\n",
        "print(\"FORALL id =\", v.encode_label(\"FORALL\"))\n",
        "print(\"numeral 24 id =\", v.encode_numeral(24))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x088EcbbIn1L",
        "outputId": "5bb1d0e3-819b-4232-e70a-1d4b8711f81a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Symbolic-Transformers\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "✓ Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "vocab_size = 662\n",
            "FORALL id = 641\n",
            "numeral 24 id = 24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Symbolic-Transformers\n",
        "!python - <<'PY'\n",
        "from data.dataset_generator import generate_training_data\n",
        "\n",
        "generate_training_data(\n",
        "    vocab_path=\"unified_vocabulary.json\",\n",
        "    output_dir=\"datasets/fol_next_symbol\",\n",
        "    n_train=1000,\n",
        "    n_val=200,\n",
        "    n_test=200,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on0I8mF1I_aE",
        "outputId": "158cb438-3233-4fbc-8beb-7cbd35ecfbdf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Symbolic-Transformers\n",
            "/bin/bash: line 1: warning: here-document at line 1 delimited by end-of-file (wanted `PY')\n",
            "============================================================\n",
            "FOL DATASET GENERATION\n",
            "============================================================\n",
            "✓ Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "\n",
            "Generating train set (1000 formulas)...\n",
            "  Complexity 1: 200 formulas\n",
            "  Complexity 2: 400 formulas\n",
            "  Complexity 3: 300 formulas\n",
            "  Complexity 4: 100 formulas\n",
            "✓ Saved 14322 samples to datasets/fol_next_symbol/train.json\n",
            "\n",
            "Generating val set (200 formulas)...\n",
            "  Complexity 1: 40 formulas\n",
            "  Complexity 2: 80 formulas\n",
            "  Complexity 3: 60 formulas\n",
            "  Complexity 4: 20 formulas\n",
            "✓ Saved 2948 samples to datasets/fol_next_symbol/val.json\n",
            "\n",
            "Generating test set (200 formulas)...\n",
            "  Complexity 1: 40 formulas\n",
            "  Complexity 2: 80 formulas\n",
            "  Complexity 3: 60 formulas\n",
            "  Complexity 4: 20 formulas\n",
            "✓ Saved 2899 samples to datasets/fol_next_symbol/test.json\n",
            "\n",
            "============================================================\n",
            "✓ Dataset generation complete!\n",
            "✓ Output directory: datasets/fol_next_symbol\n",
            "✓ Train samples: 14322\n",
            "✓ Val samples: 2948\n",
            "✓ Test samples: 2899\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lah datasets/fol_next_symbol | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-skg0ftLJLdA",
        "outputId": "de8d4747-ab3d-4071-e5e9-4cfc978edd18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.4M\n",
            "drwxr-xr-x 2 root root 4.0K Dec 20 09:38 .\n",
            "drwxr-xr-x 3 root root 4.0K Dec 20 09:38 ..\n",
            "-rw-r--r-- 1 root root  395 Dec 20 09:43 metadata.json\n",
            "-rw-r--r-- 1 root root 632K Dec 20 09:43 test.json\n",
            "-rw-r--r-- 1 root root 3.2M Dec 20 09:43 train.json\n",
            "-rw-r--r-- 1 root root 663K Dec 20 09:43 val.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Symbolic-Transformers\n",
        "!python training/train.py --model-size tiny --num-epochs 1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2wvuSWKJOee",
        "outputId": "a9ce220a-beb5-4a15-a553-971b6a85d4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Symbolic-Transformers\n",
            "✓ Vocabulary loaded: 662 tokens\n",
            "  - Numerals: 0-624\n",
            "  - Symbols: 625-661\n",
            "  - Compositional: ['VAR', 'CONST', 'PRED', 'FUNC', 'SORT']\n",
            "\n",
            "✓ Loaded vocabulary: 662 tokens\n",
            "\n",
            "Loading datasets...\n",
            "✓ Loaded 14322 samples from datasets/fol_next_symbol/train.json\n",
            "✓ Loaded 2948 samples from datasets/fol_next_symbol/val.json\n",
            "✓ Train batches: 224\n",
            "✓ Val batches: 47\n",
            "\n",
            "Creating model...\n",
            "✓ Created tiny model with 566,934 parameters\n",
            "✓ Using device: cuda\n",
            "✓ GPU: NVIDIA A100-SXM4-40GB\n",
            "✓ VRAM: 42.5 GB\n",
            "\n",
            "============================================================\n",
            "TRAINING START\n",
            "============================================================\n",
            "Model: tiny\n",
            "Vocab size: 662\n",
            "Batch size: 64\n",
            "Epochs: 1000\n",
            "Device: cuda\n",
            "============================================================\n",
            "\n",
            "Epoch 1/1000\n",
            "  Batch 100/224 | Loss: 6.4805 | LR: 5.00e-06\n",
            "  Batch 200/224 | Loss: 6.3615 | LR: 1.00e-05\n",
            "\n",
            "  Train Loss: 6.3231\n",
            "  Val Loss:   5.9105 [BEST]\n",
            "  Time:       2.5s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_1.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 2/1000\n",
            "  Batch 100/224 | Loss: 5.7052 | LR: 1.62e-05\n",
            "  Batch 200/224 | Loss: 5.4706 | LR: 2.12e-05\n",
            "\n",
            "  Train Loss: 5.4168\n",
            "  Val Loss:   4.8483 [BEST]\n",
            "  Time:       2.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_2.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 3/1000\n",
            "  Batch 100/224 | Loss: 4.6546 | LR: 2.74e-05\n",
            "  Batch 200/224 | Loss: 4.3775 | LR: 3.24e-05\n",
            "\n",
            "  Train Loss: 4.3090\n",
            "  Val Loss:   3.6351 [BEST]\n",
            "  Time:       2.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_3.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 4/1000\n",
            "  Batch 100/224 | Loss: 3.4211 | LR: 3.86e-05\n",
            "  Batch 200/224 | Loss: 3.1766 | LR: 4.36e-05\n",
            "\n",
            "  Train Loss: 3.1305\n",
            "  Val Loss:   2.6972 [BEST]\n",
            "  Time:       2.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_4.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 5/1000\n",
            "  Batch 100/224 | Loss: 2.6285 | LR: 4.98e-05\n",
            "  Batch 200/224 | Loss: 2.5522 | LR: 5.48e-05\n",
            "\n",
            "  Train Loss: 2.5396\n",
            "  Val Loss:   2.4295 [BEST]\n",
            "  Time:       2.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_5.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 6/1000\n",
            "  Batch 100/224 | Loss: 2.3984 | LR: 6.10e-05\n",
            "  Batch 200/224 | Loss: 2.3778 | LR: 6.60e-05\n",
            "\n",
            "  Train Loss: 2.3685\n",
            "  Val Loss:   2.3005 [BEST]\n",
            "  Time:       2.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_6.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 7/1000\n",
            "  Batch 100/224 | Loss: 2.2646 | LR: 7.22e-05\n",
            "  Batch 200/224 | Loss: 2.2206 | LR: 7.72e-05\n",
            "\n",
            "  Train Loss: 2.2073\n",
            "  Val Loss:   2.0248 [BEST]\n",
            "  Time:       2.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_7.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 8/1000\n",
            "  Batch 100/224 | Loss: 2.0485 | LR: 8.34e-05\n",
            "  Batch 200/224 | Loss: 1.9872 | LR: 8.84e-05\n",
            "\n",
            "  Train Loss: 1.9787\n",
            "  Val Loss:   1.7835 [BEST]\n",
            "  Time:       2.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_8.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 9/1000\n",
            "  Batch 100/224 | Loss: 1.8549 | LR: 9.46e-05\n",
            "  Batch 200/224 | Loss: 1.8091 | LR: 9.96e-05\n",
            "\n",
            "  Train Loss: 1.7995\n",
            "  Val Loss:   1.6616 [BEST]\n",
            "  Time:       2.2s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_9.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 10/1000\n",
            "  Batch 100/224 | Loss: 1.7018 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.6884 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.6813\n",
            "  Val Loss:   1.5447 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_10.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 11/1000\n",
            "  Batch 100/224 | Loss: 1.6234 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.6183 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.6157\n",
            "  Val Loss:   1.4902 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_11.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 12/1000\n",
            "  Batch 100/224 | Loss: 1.5550 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.5663 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.5663\n",
            "  Val Loss:   1.4593 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_12.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 13/1000\n",
            "  Batch 100/224 | Loss: 1.5324 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.5211 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.5141\n",
            "  Val Loss:   1.3732 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_13.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 14/1000\n",
            "  Batch 100/224 | Loss: 1.4582 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.4255 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.4214\n",
            "  Val Loss:   1.1653 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_14.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 15/1000\n",
            "  Batch 100/224 | Loss: 1.3072 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.2980 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.2919\n",
            "  Val Loss:   1.1010 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_15.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 16/1000\n",
            "  Batch 100/224 | Loss: 1.2424 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.2377 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.2355\n",
            "  Val Loss:   1.0580 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_16.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 17/1000\n",
            "  Batch 100/224 | Loss: 1.2032 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.1983 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.1936\n",
            "  Val Loss:   1.0456 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_17.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 18/1000\n",
            "  Batch 100/224 | Loss: 1.1788 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.1620 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.1634\n",
            "  Val Loss:   1.0218 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_18.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 19/1000\n",
            "  Batch 100/224 | Loss: 1.1523 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.1433 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.1451\n",
            "  Val Loss:   1.0073 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_19.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 20/1000\n",
            "  Batch 100/224 | Loss: 1.1399 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.1285 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.1320\n",
            "  Val Loss:   1.0032 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_20.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 21/1000\n",
            "  Batch 100/224 | Loss: 1.1247 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.1182 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.1172\n",
            "  Val Loss:   0.9971 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_21.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 22/1000\n",
            "  Batch 100/224 | Loss: 1.0890 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.1010 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.0980\n",
            "  Val Loss:   0.9902 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_22.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 23/1000\n",
            "  Batch 100/224 | Loss: 1.0986 | LR: 1.00e-04\n",
            "  Batch 200/224 | Loss: 1.0886 | LR: 1.00e-04\n",
            "\n",
            "  Train Loss: 1.0877\n",
            "  Val Loss:   0.9852 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_23.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 24/1000\n",
            "  Batch 100/224 | Loss: 1.0824 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0795 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0795\n",
            "  Val Loss:   0.9831 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_24.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 25/1000\n",
            "  Batch 100/224 | Loss: 1.0909 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0708 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0774\n",
            "  Val Loss:   0.9779 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_25.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 26/1000\n",
            "  Batch 100/224 | Loss: 1.0514 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0664 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0656\n",
            "  Val Loss:   0.9811 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 27/1000\n",
            "  Batch 100/224 | Loss: 1.0668 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0530 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0511\n",
            "  Val Loss:   0.9739 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_27.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 28/1000\n",
            "  Batch 100/224 | Loss: 1.0492 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0469 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0481\n",
            "  Val Loss:   0.9750 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 29/1000\n",
            "  Batch 100/224 | Loss: 1.0487 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0449 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0435\n",
            "  Val Loss:   0.9657 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_29.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 30/1000\n",
            "  Batch 100/224 | Loss: 1.0434 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0434 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0441\n",
            "  Val Loss:   0.9644 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_30.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 31/1000\n",
            "  Batch 100/224 | Loss: 1.0496 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0374 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0360\n",
            "  Val Loss:   0.9636 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_31.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 32/1000\n",
            "  Batch 100/224 | Loss: 1.0371 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0374 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0363\n",
            "  Val Loss:   0.9567 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_32.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 33/1000\n",
            "  Batch 100/224 | Loss: 1.0385 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0319 | LR: 9.99e-05\n",
            "\n",
            "  Train Loss: 1.0350\n",
            "  Val Loss:   0.9596 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 34/1000\n",
            "  Batch 100/224 | Loss: 1.0297 | LR: 9.99e-05\n",
            "  Batch 200/224 | Loss: 1.0271 | LR: 9.98e-05\n",
            "\n",
            "  Train Loss: 1.0268\n",
            "  Val Loss:   0.9606 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 35/1000\n",
            "  Batch 100/224 | Loss: 1.0201 | LR: 9.98e-05\n",
            "  Batch 200/224 | Loss: 1.0220 | LR: 9.98e-05\n",
            "\n",
            "  Train Loss: 1.0159\n",
            "  Val Loss:   0.9584 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_35.pt\n",
            "Epoch 36/1000\n",
            "  Batch 100/224 | Loss: 1.0189 | LR: 9.98e-05\n",
            "  Batch 200/224 | Loss: 1.0124 | LR: 9.98e-05\n",
            "\n",
            "  Train Loss: 1.0169\n",
            "  Val Loss:   0.9581 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 37/1000\n",
            "  Batch 100/224 | Loss: 1.0042 | LR: 9.98e-05\n",
            "  Batch 200/224 | Loss: 1.0114 | LR: 9.98e-05\n",
            "\n",
            "  Train Loss: 1.0136\n",
            "  Val Loss:   0.9559 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_37.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 38/1000\n",
            "  Batch 100/224 | Loss: 1.0137 | LR: 9.98e-05\n",
            "  Batch 200/224 | Loss: 1.0117 | LR: 9.98e-05\n",
            "\n",
            "  Train Loss: 1.0127\n",
            "  Val Loss:   0.9573 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 39/1000\n",
            "  Batch 100/224 | Loss: 1.0131 | LR: 9.98e-05\n",
            "  Batch 200/224 | Loss: 1.0101 | LR: 9.98e-05\n",
            "\n",
            "  Train Loss: 1.0066\n",
            "  Val Loss:   0.9549 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_39.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 40/1000\n",
            "  Batch 100/224 | Loss: 0.9952 | LR: 9.98e-05\n",
            "  Batch 200/224 | Loss: 1.0040 | LR: 9.98e-05\n",
            "\n",
            "  Train Loss: 1.0063\n",
            "  Val Loss:   0.9542 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_40.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 41/1000\n",
            "  Batch 100/224 | Loss: 1.0030 | LR: 9.98e-05\n",
            "  Batch 200/224 | Loss: 0.9986 | LR: 9.97e-05\n",
            "\n",
            "  Train Loss: 1.0027\n",
            "  Val Loss:   0.9597 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 42/1000\n",
            "  Batch 100/224 | Loss: 0.9967 | LR: 9.97e-05\n",
            "  Batch 200/224 | Loss: 1.0050 | LR: 9.97e-05\n",
            "\n",
            "  Train Loss: 1.0050\n",
            "  Val Loss:   0.9482 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_42.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 43/1000\n",
            "  Batch 100/224 | Loss: 0.9935 | LR: 9.97e-05\n",
            "  Batch 200/224 | Loss: 1.0074 | LR: 9.97e-05\n",
            "\n",
            "  Train Loss: 1.0044\n",
            "  Val Loss:   0.9537 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 44/1000\n",
            "  Batch 100/224 | Loss: 1.0034 | LR: 9.97e-05\n",
            "  Batch 200/224 | Loss: 1.0000 | LR: 9.97e-05\n",
            "\n",
            "  Train Loss: 0.9980\n",
            "  Val Loss:   0.9450 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_44.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 45/1000\n",
            "  Batch 100/224 | Loss: 0.9773 | LR: 9.97e-05\n",
            "  Batch 200/224 | Loss: 0.9980 | LR: 9.97e-05\n",
            "\n",
            "  Train Loss: 0.9955\n",
            "  Val Loss:   0.9467 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_45.pt\n",
            "Epoch 46/1000\n",
            "  Batch 100/224 | Loss: 1.0031 | LR: 9.97e-05\n",
            "  Batch 200/224 | Loss: 1.0004 | LR: 9.97e-05\n",
            "\n",
            "  Train Loss: 0.9990\n",
            "  Val Loss:   0.9445 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_46.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 47/1000\n",
            "  Batch 100/224 | Loss: 0.9829 | LR: 9.97e-05\n",
            "  Batch 200/224 | Loss: 0.9892 | LR: 9.96e-05\n",
            "\n",
            "  Train Loss: 0.9897\n",
            "  Val Loss:   0.9424 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_47.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 48/1000\n",
            "  Batch 100/224 | Loss: 0.9810 | LR: 9.96e-05\n",
            "  Batch 200/224 | Loss: 0.9957 | LR: 9.96e-05\n",
            "\n",
            "  Train Loss: 0.9939\n",
            "  Val Loss:   0.9475 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 49/1000\n",
            "  Batch 100/224 | Loss: 0.9997 | LR: 9.96e-05\n",
            "  Batch 200/224 | Loss: 0.9892 | LR: 9.96e-05\n",
            "\n",
            "  Train Loss: 0.9882\n",
            "  Val Loss:   0.9463 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 50/1000\n",
            "  Batch 100/224 | Loss: 0.9875 | LR: 9.96e-05\n",
            "  Batch 200/224 | Loss: 0.9882 | LR: 9.96e-05\n",
            "\n",
            "  Train Loss: 0.9860\n",
            "  Val Loss:   0.9469 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_50.pt\n",
            "Epoch 51/1000\n",
            "  Batch 100/224 | Loss: 0.9939 | LR: 9.96e-05\n",
            "  Batch 200/224 | Loss: 0.9842 | LR: 9.96e-05\n",
            "\n",
            "  Train Loss: 0.9820\n",
            "  Val Loss:   0.9417 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_51.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 52/1000\n",
            "  Batch 100/224 | Loss: 0.9780 | LR: 9.96e-05\n",
            "  Batch 200/224 | Loss: 0.9891 | LR: 9.95e-05\n",
            "\n",
            "  Train Loss: 0.9840\n",
            "  Val Loss:   0.9403 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_52.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 53/1000\n",
            "  Batch 100/224 | Loss: 0.9894 | LR: 9.95e-05\n",
            "  Batch 200/224 | Loss: 0.9831 | LR: 9.95e-05\n",
            "\n",
            "  Train Loss: 0.9822\n",
            "  Val Loss:   0.9467 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 54/1000\n",
            "  Batch 100/224 | Loss: 0.9772 | LR: 9.95e-05\n",
            "  Batch 200/224 | Loss: 0.9801 | LR: 9.95e-05\n",
            "\n",
            "  Train Loss: 0.9812\n",
            "  Val Loss:   0.9365 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_54.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 55/1000\n",
            "  Batch 100/224 | Loss: 0.9785 | LR: 9.95e-05\n",
            "  Batch 200/224 | Loss: 0.9799 | LR: 9.95e-05\n",
            "\n",
            "  Train Loss: 0.9798\n",
            "  Val Loss:   0.9395 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_55.pt\n",
            "Epoch 56/1000\n",
            "  Batch 100/224 | Loss: 0.9867 | LR: 9.95e-05\n",
            "  Batch 200/224 | Loss: 0.9781 | LR: 9.95e-05\n",
            "\n",
            "  Train Loss: 0.9795\n",
            "  Val Loss:   0.9339 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_56.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 57/1000\n",
            "  Batch 100/224 | Loss: 0.9789 | LR: 9.94e-05\n",
            "  Batch 200/224 | Loss: 0.9736 | LR: 9.94e-05\n",
            "\n",
            "  Train Loss: 0.9749\n",
            "  Val Loss:   0.9358 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 58/1000\n",
            "  Batch 100/224 | Loss: 0.9784 | LR: 9.94e-05\n",
            "  Batch 200/224 | Loss: 0.9701 | LR: 9.94e-05\n",
            "\n",
            "  Train Loss: 0.9731\n",
            "  Val Loss:   0.9368 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 59/1000\n",
            "  Batch 100/224 | Loss: 0.9679 | LR: 9.94e-05\n",
            "  Batch 200/224 | Loss: 0.9783 | LR: 9.94e-05\n",
            "\n",
            "  Train Loss: 0.9749\n",
            "  Val Loss:   0.9377 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 60/1000\n",
            "  Batch 100/224 | Loss: 0.9722 | LR: 9.94e-05\n",
            "  Batch 200/224 | Loss: 0.9724 | LR: 9.94e-05\n",
            "\n",
            "  Train Loss: 0.9738\n",
            "  Val Loss:   0.9342 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_60.pt\n",
            "Epoch 61/1000\n",
            "  Batch 100/224 | Loss: 0.9761 | LR: 9.93e-05\n",
            "  Batch 200/224 | Loss: 0.9787 | LR: 9.93e-05\n",
            "\n",
            "  Train Loss: 0.9723\n",
            "  Val Loss:   0.9337 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_61.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 62/1000\n",
            "  Batch 100/224 | Loss: 0.9555 | LR: 9.93e-05\n",
            "  Batch 200/224 | Loss: 0.9679 | LR: 9.93e-05\n",
            "\n",
            "  Train Loss: 0.9659\n",
            "  Val Loss:   0.9347 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 63/1000\n",
            "  Batch 100/224 | Loss: 0.9762 | LR: 9.93e-05\n",
            "  Batch 200/224 | Loss: 0.9710 | LR: 9.93e-05\n",
            "\n",
            "  Train Loss: 0.9738\n",
            "  Val Loss:   0.9338 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 64/1000\n",
            "  Batch 100/224 | Loss: 0.9796 | LR: 9.93e-05\n",
            "  Batch 200/224 | Loss: 0.9759 | LR: 9.93e-05\n",
            "\n",
            "  Train Loss: 0.9760\n",
            "  Val Loss:   0.9308 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_64.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 65/1000\n",
            "  Batch 100/224 | Loss: 0.9576 | LR: 9.92e-05\n",
            "  Batch 200/224 | Loss: 0.9666 | LR: 9.92e-05\n",
            "\n",
            "  Train Loss: 0.9652\n",
            "  Val Loss:   0.9304 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_65.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 66/1000\n",
            "  Batch 100/224 | Loss: 0.9575 | LR: 9.92e-05\n",
            "  Batch 200/224 | Loss: 0.9670 | LR: 9.92e-05\n",
            "\n",
            "  Train Loss: 0.9641\n",
            "  Val Loss:   0.9272 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_66.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 67/1000\n",
            "  Batch 100/224 | Loss: 0.9566 | LR: 9.92e-05\n",
            "  Batch 200/224 | Loss: 0.9634 | LR: 9.92e-05\n",
            "\n",
            "  Train Loss: 0.9627\n",
            "  Val Loss:   0.9337 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 68/1000\n",
            "  Batch 100/224 | Loss: 0.9624 | LR: 9.92e-05\n",
            "  Batch 200/224 | Loss: 0.9650 | LR: 9.91e-05\n",
            "\n",
            "  Train Loss: 0.9622\n",
            "  Val Loss:   0.9283 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 69/1000\n",
            "  Batch 100/224 | Loss: 0.9542 | LR: 9.91e-05\n",
            "  Batch 200/224 | Loss: 0.9628 | LR: 9.91e-05\n",
            "\n",
            "  Train Loss: 0.9627\n",
            "  Val Loss:   0.9296 \n",
            "  Time:       2.8s\n",
            "\n",
            "Epoch 70/1000\n",
            "  Batch 100/224 | Loss: 0.9664 | LR: 9.91e-05\n",
            "  Batch 200/224 | Loss: 0.9647 | LR: 9.91e-05\n",
            "\n",
            "  Train Loss: 0.9631\n",
            "  Val Loss:   0.9237 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_70.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 71/1000\n",
            "  Batch 100/224 | Loss: 0.9577 | LR: 9.91e-05\n",
            "  Batch 200/224 | Loss: 0.9591 | LR: 9.91e-05\n",
            "\n",
            "  Train Loss: 0.9594\n",
            "  Val Loss:   0.9252 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 72/1000\n",
            "  Batch 100/224 | Loss: 0.9469 | LR: 9.90e-05\n",
            "  Batch 200/224 | Loss: 0.9610 | LR: 9.90e-05\n",
            "\n",
            "  Train Loss: 0.9593\n",
            "  Val Loss:   0.9315 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 73/1000\n",
            "  Batch 100/224 | Loss: 0.9600 | LR: 9.90e-05\n",
            "  Batch 200/224 | Loss: 0.9559 | LR: 9.90e-05\n",
            "\n",
            "  Train Loss: 0.9535\n",
            "  Val Loss:   0.9406 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 74/1000\n",
            "  Batch 100/224 | Loss: 0.9673 | LR: 9.90e-05\n",
            "  Batch 200/224 | Loss: 0.9676 | LR: 9.90e-05\n",
            "\n",
            "  Train Loss: 0.9616\n",
            "  Val Loss:   0.9258 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 75/1000\n",
            "  Batch 100/224 | Loss: 0.9440 | LR: 9.89e-05\n",
            "  Batch 200/224 | Loss: 0.9579 | LR: 9.89e-05\n",
            "\n",
            "  Train Loss: 0.9580\n",
            "  Val Loss:   0.9290 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_75.pt\n",
            "Epoch 76/1000\n",
            "  Batch 100/224 | Loss: 0.9583 | LR: 9.89e-05\n",
            "  Batch 200/224 | Loss: 0.9582 | LR: 9.89e-05\n",
            "\n",
            "  Train Loss: 0.9558\n",
            "  Val Loss:   0.9210 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_76.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 77/1000\n",
            "  Batch 100/224 | Loss: 0.9589 | LR: 9.89e-05\n",
            "  Batch 200/224 | Loss: 0.9511 | LR: 9.89e-05\n",
            "\n",
            "  Train Loss: 0.9554\n",
            "  Val Loss:   0.9292 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 78/1000\n",
            "  Batch 100/224 | Loss: 0.9480 | LR: 9.88e-05\n",
            "  Batch 200/224 | Loss: 0.9532 | LR: 9.88e-05\n",
            "\n",
            "  Train Loss: 0.9575\n",
            "  Val Loss:   0.9246 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 79/1000\n",
            "  Batch 100/224 | Loss: 0.9474 | LR: 9.88e-05\n",
            "  Batch 200/224 | Loss: 0.9557 | LR: 9.88e-05\n",
            "\n",
            "  Train Loss: 0.9558\n",
            "  Val Loss:   0.9216 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 80/1000\n",
            "  Batch 100/224 | Loss: 0.9487 | LR: 9.88e-05\n",
            "  Batch 200/224 | Loss: 0.9528 | LR: 9.88e-05\n",
            "\n",
            "  Train Loss: 0.9549\n",
            "  Val Loss:   0.9183 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_80.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 81/1000\n",
            "  Batch 100/224 | Loss: 0.9499 | LR: 9.87e-05\n",
            "  Batch 200/224 | Loss: 0.9514 | LR: 9.87e-05\n",
            "\n",
            "  Train Loss: 0.9549\n",
            "  Val Loss:   0.9212 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 82/1000\n",
            "  Batch 100/224 | Loss: 0.9520 | LR: 9.87e-05\n",
            "  Batch 200/224 | Loss: 0.9576 | LR: 9.87e-05\n",
            "\n",
            "  Train Loss: 0.9559\n",
            "  Val Loss:   0.9176 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_82.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 83/1000\n",
            "  Batch 100/224 | Loss: 0.9442 | LR: 9.87e-05\n",
            "  Batch 200/224 | Loss: 0.9442 | LR: 9.87e-05\n",
            "\n",
            "  Train Loss: 0.9481\n",
            "  Val Loss:   0.9210 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 84/1000\n",
            "  Batch 100/224 | Loss: 0.9503 | LR: 9.86e-05\n",
            "  Batch 200/224 | Loss: 0.9426 | LR: 9.86e-05\n",
            "\n",
            "  Train Loss: 0.9440\n",
            "  Val Loss:   0.9196 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 85/1000\n",
            "  Batch 100/224 | Loss: 0.9487 | LR: 9.86e-05\n",
            "  Batch 200/224 | Loss: 0.9447 | LR: 9.86e-05\n",
            "\n",
            "  Train Loss: 0.9475\n",
            "  Val Loss:   0.9219 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_85.pt\n",
            "Epoch 86/1000\n",
            "  Batch 100/224 | Loss: 0.9465 | LR: 9.86e-05\n",
            "  Batch 200/224 | Loss: 0.9538 | LR: 9.85e-05\n",
            "\n",
            "  Train Loss: 0.9494\n",
            "  Val Loss:   0.9193 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 87/1000\n",
            "  Batch 100/224 | Loss: 0.9448 | LR: 9.85e-05\n",
            "  Batch 200/224 | Loss: 0.9422 | LR: 9.85e-05\n",
            "\n",
            "  Train Loss: 0.9443\n",
            "  Val Loss:   0.9227 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 88/1000\n",
            "  Batch 100/224 | Loss: 0.9669 | LR: 9.85e-05\n",
            "  Batch 200/224 | Loss: 0.9482 | LR: 9.85e-05\n",
            "\n",
            "  Train Loss: 0.9509\n",
            "  Val Loss:   0.9199 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 89/1000\n",
            "  Batch 100/224 | Loss: 0.9744 | LR: 9.84e-05\n",
            "  Batch 200/224 | Loss: 0.9531 | LR: 9.84e-05\n",
            "\n",
            "  Train Loss: 0.9529\n",
            "  Val Loss:   0.9227 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 90/1000\n",
            "  Batch 100/224 | Loss: 0.9322 | LR: 9.84e-05\n",
            "  Batch 200/224 | Loss: 0.9344 | LR: 9.84e-05\n",
            "\n",
            "  Train Loss: 0.9425\n",
            "  Val Loss:   0.9198 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_90.pt\n",
            "Epoch 91/1000\n",
            "  Batch 100/224 | Loss: 0.9293 | LR: 9.84e-05\n",
            "  Batch 200/224 | Loss: 0.9400 | LR: 9.84e-05\n",
            "\n",
            "  Train Loss: 0.9423\n",
            "  Val Loss:   0.9235 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 92/1000\n",
            "  Batch 100/224 | Loss: 0.9521 | LR: 9.83e-05\n",
            "  Batch 200/224 | Loss: 0.9465 | LR: 9.83e-05\n",
            "\n",
            "  Train Loss: 0.9461\n",
            "  Val Loss:   0.9231 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 93/1000\n",
            "  Batch 100/224 | Loss: 0.9378 | LR: 9.83e-05\n",
            "  Batch 200/224 | Loss: 0.9460 | LR: 9.83e-05\n",
            "\n",
            "  Train Loss: 0.9434\n",
            "  Val Loss:   0.9217 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 94/1000\n",
            "  Batch 100/224 | Loss: 0.9190 | LR: 9.82e-05\n",
            "  Batch 200/224 | Loss: 0.9414 | LR: 9.82e-05\n",
            "\n",
            "  Train Loss: 0.9405\n",
            "  Val Loss:   0.9196 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 95/1000\n",
            "  Batch 100/224 | Loss: 0.9386 | LR: 9.82e-05\n",
            "  Batch 200/224 | Loss: 0.9463 | LR: 9.82e-05\n",
            "\n",
            "  Train Loss: 0.9431\n",
            "  Val Loss:   0.9145 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_95.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 96/1000\n",
            "  Batch 100/224 | Loss: 0.9339 | LR: 9.82e-05\n",
            "  Batch 200/224 | Loss: 0.9425 | LR: 9.81e-05\n",
            "\n",
            "  Train Loss: 0.9413\n",
            "  Val Loss:   0.9172 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 97/1000\n",
            "  Batch 100/224 | Loss: 0.9467 | LR: 9.81e-05\n",
            "  Batch 200/224 | Loss: 0.9433 | LR: 9.81e-05\n",
            "\n",
            "  Train Loss: 0.9440\n",
            "  Val Loss:   0.9186 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 98/1000\n",
            "  Batch 100/224 | Loss: 0.9358 | LR: 9.81e-05\n",
            "  Batch 200/224 | Loss: 0.9369 | LR: 9.81e-05\n",
            "\n",
            "  Train Loss: 0.9397\n",
            "  Val Loss:   0.9252 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 99/1000\n",
            "  Batch 100/224 | Loss: 0.9278 | LR: 9.80e-05\n",
            "  Batch 200/224 | Loss: 0.9303 | LR: 9.80e-05\n",
            "\n",
            "  Train Loss: 0.9350\n",
            "  Val Loss:   0.9227 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 100/1000\n",
            "  Batch 100/224 | Loss: 0.9377 | LR: 9.80e-05\n",
            "  Batch 200/224 | Loss: 0.9413 | LR: 9.80e-05\n",
            "\n",
            "  Train Loss: 0.9385\n",
            "  Val Loss:   0.9177 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_100.pt\n",
            "Epoch 101/1000\n",
            "  Batch 100/224 | Loss: 0.9229 | LR: 9.79e-05\n",
            "  Batch 200/224 | Loss: 0.9376 | LR: 9.79e-05\n",
            "\n",
            "  Train Loss: 0.9360\n",
            "  Val Loss:   0.9220 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 102/1000\n",
            "  Batch 100/224 | Loss: 0.9252 | LR: 9.79e-05\n",
            "  Batch 200/224 | Loss: 0.9360 | LR: 9.79e-05\n",
            "\n",
            "  Train Loss: 0.9359\n",
            "  Val Loss:   0.9198 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 103/1000\n",
            "  Batch 100/224 | Loss: 0.9312 | LR: 9.79e-05\n",
            "  Batch 200/224 | Loss: 0.9372 | LR: 9.78e-05\n",
            "\n",
            "  Train Loss: 0.9377\n",
            "  Val Loss:   0.9158 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 104/1000\n",
            "  Batch 100/224 | Loss: 0.9223 | LR: 9.78e-05\n",
            "  Batch 200/224 | Loss: 0.9331 | LR: 9.78e-05\n",
            "\n",
            "  Train Loss: 0.9368\n",
            "  Val Loss:   0.9162 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 105/1000\n",
            "  Batch 100/224 | Loss: 0.9349 | LR: 9.78e-05\n",
            "  Batch 200/224 | Loss: 0.9370 | LR: 9.77e-05\n",
            "\n",
            "  Train Loss: 0.9362\n",
            "  Val Loss:   0.9219 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_105.pt\n",
            "Epoch 106/1000\n",
            "  Batch 100/224 | Loss: 0.9274 | LR: 9.77e-05\n",
            "  Batch 200/224 | Loss: 0.9318 | LR: 9.77e-05\n",
            "\n",
            "  Train Loss: 0.9357\n",
            "  Val Loss:   0.9141 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_106.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 107/1000\n",
            "  Batch 100/224 | Loss: 0.9294 | LR: 9.77e-05\n",
            "  Batch 200/224 | Loss: 0.9284 | LR: 9.77e-05\n",
            "\n",
            "  Train Loss: 0.9312\n",
            "  Val Loss:   0.9282 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 108/1000\n",
            "  Batch 100/224 | Loss: 0.9303 | LR: 9.76e-05\n",
            "  Batch 200/224 | Loss: 0.9300 | LR: 9.76e-05\n",
            "\n",
            "  Train Loss: 0.9282\n",
            "  Val Loss:   0.9161 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 109/1000\n",
            "  Batch 100/224 | Loss: 0.9276 | LR: 9.76e-05\n",
            "  Batch 200/224 | Loss: 0.9353 | LR: 9.76e-05\n",
            "\n",
            "  Train Loss: 0.9340\n",
            "  Val Loss:   0.9193 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 110/1000\n",
            "  Batch 100/224 | Loss: 0.9220 | LR: 9.75e-05\n",
            "  Batch 200/224 | Loss: 0.9325 | LR: 9.75e-05\n",
            "\n",
            "  Train Loss: 0.9347\n",
            "  Val Loss:   0.9196 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_110.pt\n",
            "Epoch 111/1000\n",
            "  Batch 100/224 | Loss: 0.9314 | LR: 9.75e-05\n",
            "  Batch 200/224 | Loss: 0.9347 | LR: 9.75e-05\n",
            "\n",
            "  Train Loss: 0.9337\n",
            "  Val Loss:   0.9139 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_111.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 112/1000\n",
            "  Batch 100/224 | Loss: 0.9232 | LR: 9.74e-05\n",
            "  Batch 200/224 | Loss: 0.9351 | LR: 9.74e-05\n",
            "\n",
            "  Train Loss: 0.9352\n",
            "  Val Loss:   0.9121 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_112.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 113/1000\n",
            "  Batch 100/224 | Loss: 0.9266 | LR: 9.74e-05\n",
            "  Batch 200/224 | Loss: 0.9264 | LR: 9.74e-05\n",
            "\n",
            "  Train Loss: 0.9276\n",
            "  Val Loss:   0.9164 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 114/1000\n",
            "  Batch 100/224 | Loss: 0.9102 | LR: 9.73e-05\n",
            "  Batch 200/224 | Loss: 0.9234 | LR: 9.73e-05\n",
            "\n",
            "  Train Loss: 0.9264\n",
            "  Val Loss:   0.9145 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 115/1000\n",
            "  Batch 100/224 | Loss: 0.9231 | LR: 9.73e-05\n",
            "  Batch 200/224 | Loss: 0.9293 | LR: 9.73e-05\n",
            "\n",
            "  Train Loss: 0.9332\n",
            "  Val Loss:   0.9130 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_115.pt\n",
            "Epoch 116/1000\n",
            "  Batch 100/224 | Loss: 0.9307 | LR: 9.72e-05\n",
            "  Batch 200/224 | Loss: 0.9328 | LR: 9.72e-05\n",
            "\n",
            "  Train Loss: 0.9310\n",
            "  Val Loss:   0.9172 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 117/1000\n",
            "  Batch 100/224 | Loss: 0.9338 | LR: 9.72e-05\n",
            "  Batch 200/224 | Loss: 0.9366 | LR: 9.72e-05\n",
            "\n",
            "  Train Loss: 0.9294\n",
            "  Val Loss:   0.9099 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_117.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 118/1000\n",
            "  Batch 100/224 | Loss: 0.9259 | LR: 9.71e-05\n",
            "  Batch 200/224 | Loss: 0.9243 | LR: 9.71e-05\n",
            "\n",
            "  Train Loss: 0.9262\n",
            "  Val Loss:   0.9105 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 119/1000\n",
            "  Batch 100/224 | Loss: 0.9113 | LR: 9.71e-05\n",
            "  Batch 200/224 | Loss: 0.9191 | LR: 9.70e-05\n",
            "\n",
            "  Train Loss: 0.9252\n",
            "  Val Loss:   0.9096 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_119.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 120/1000\n",
            "  Batch 100/224 | Loss: 0.9104 | LR: 9.70e-05\n",
            "  Batch 200/224 | Loss: 0.9239 | LR: 9.70e-05\n",
            "\n",
            "  Train Loss: 0.9207\n",
            "  Val Loss:   0.9111 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_120.pt\n",
            "Epoch 121/1000\n",
            "  Batch 100/224 | Loss: 0.9110 | LR: 9.70e-05\n",
            "  Batch 200/224 | Loss: 0.9159 | LR: 9.69e-05\n",
            "\n",
            "  Train Loss: 0.9216\n",
            "  Val Loss:   0.9128 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 122/1000\n",
            "  Batch 100/224 | Loss: 0.9178 | LR: 9.69e-05\n",
            "  Batch 200/224 | Loss: 0.9228 | LR: 9.69e-05\n",
            "\n",
            "  Train Loss: 0.9241\n",
            "  Val Loss:   0.9113 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 123/1000\n",
            "  Batch 100/224 | Loss: 0.9233 | LR: 9.69e-05\n",
            "  Batch 200/224 | Loss: 0.9223 | LR: 9.68e-05\n",
            "\n",
            "  Train Loss: 0.9226\n",
            "  Val Loss:   0.9136 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 124/1000\n",
            "  Batch 100/224 | Loss: 0.9280 | LR: 9.68e-05\n",
            "  Batch 200/224 | Loss: 0.9192 | LR: 9.68e-05\n",
            "\n",
            "  Train Loss: 0.9212\n",
            "  Val Loss:   0.9110 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 125/1000\n",
            "  Batch 100/224 | Loss: 0.9179 | LR: 9.67e-05\n",
            "  Batch 200/224 | Loss: 0.9217 | LR: 9.67e-05\n",
            "\n",
            "  Train Loss: 0.9190\n",
            "  Val Loss:   0.9104 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_125.pt\n",
            "Epoch 126/1000\n",
            "  Batch 100/224 | Loss: 0.9163 | LR: 9.67e-05\n",
            "  Batch 200/224 | Loss: 0.9102 | LR: 9.67e-05\n",
            "\n",
            "  Train Loss: 0.9167\n",
            "  Val Loss:   0.9034 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_126.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 127/1000\n",
            "  Batch 100/224 | Loss: 0.9146 | LR: 9.66e-05\n",
            "  Batch 200/224 | Loss: 0.9214 | LR: 9.66e-05\n",
            "\n",
            "  Train Loss: 0.9185\n",
            "  Val Loss:   0.9047 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 128/1000\n",
            "  Batch 100/224 | Loss: 0.9164 | LR: 9.66e-05\n",
            "  Batch 200/224 | Loss: 0.9258 | LR: 9.65e-05\n",
            "\n",
            "  Train Loss: 0.9177\n",
            "  Val Loss:   0.9080 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 129/1000\n",
            "  Batch 100/224 | Loss: 0.9180 | LR: 9.65e-05\n",
            "  Batch 200/224 | Loss: 0.9136 | LR: 9.65e-05\n",
            "\n",
            "  Train Loss: 0.9182\n",
            "  Val Loss:   0.9067 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 130/1000\n",
            "  Batch 100/224 | Loss: 0.9127 | LR: 9.65e-05\n",
            "  Batch 200/224 | Loss: 0.9124 | LR: 9.64e-05\n",
            "\n",
            "  Train Loss: 0.9141\n",
            "  Val Loss:   0.9020 [BEST]\n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_130.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 131/1000\n",
            "  Batch 100/224 | Loss: 0.9157 | LR: 9.64e-05\n",
            "  Batch 200/224 | Loss: 0.9173 | LR: 9.64e-05\n",
            "\n",
            "  Train Loss: 0.9170\n",
            "  Val Loss:   0.9008 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_131.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 132/1000\n",
            "  Batch 100/224 | Loss: 0.9090 | LR: 9.63e-05\n",
            "  Batch 200/224 | Loss: 0.9180 | LR: 9.63e-05\n",
            "\n",
            "  Train Loss: 0.9166\n",
            "  Val Loss:   0.9007 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_132.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 133/1000\n",
            "  Batch 100/224 | Loss: 0.9083 | LR: 9.63e-05\n",
            "  Batch 200/224 | Loss: 0.9032 | LR: 9.63e-05\n",
            "\n",
            "  Train Loss: 0.9105\n",
            "  Val Loss:   0.8982 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_133.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 134/1000\n",
            "  Batch 100/224 | Loss: 0.9119 | LR: 9.62e-05\n",
            "  Batch 200/224 | Loss: 0.9025 | LR: 9.62e-05\n",
            "\n",
            "  Train Loss: 0.9062\n",
            "  Val Loss:   0.9036 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 135/1000\n",
            "  Batch 100/224 | Loss: 0.9265 | LR: 9.62e-05\n",
            "  Batch 200/224 | Loss: 0.9153 | LR: 9.61e-05\n",
            "\n",
            "  Train Loss: 0.9113\n",
            "  Val Loss:   0.9003 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_135.pt\n",
            "Epoch 136/1000\n",
            "  Batch 100/224 | Loss: 0.9045 | LR: 9.61e-05\n",
            "  Batch 200/224 | Loss: 0.9078 | LR: 9.61e-05\n",
            "\n",
            "  Train Loss: 0.9099\n",
            "  Val Loss:   0.9018 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 137/1000\n",
            "  Batch 100/224 | Loss: 0.8986 | LR: 9.60e-05\n",
            "  Batch 200/224 | Loss: 0.9033 | LR: 9.60e-05\n",
            "\n",
            "  Train Loss: 0.9101\n",
            "  Val Loss:   0.8963 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_137.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 138/1000\n",
            "  Batch 100/224 | Loss: 0.8995 | LR: 9.60e-05\n",
            "  Batch 200/224 | Loss: 0.9040 | LR: 9.60e-05\n",
            "\n",
            "  Train Loss: 0.9052\n",
            "  Val Loss:   0.9002 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 139/1000\n",
            "  Batch 100/224 | Loss: 0.9041 | LR: 9.59e-05\n",
            "  Batch 200/224 | Loss: 0.9105 | LR: 9.59e-05\n",
            "\n",
            "  Train Loss: 0.9092\n",
            "  Val Loss:   0.8993 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 140/1000\n",
            "  Batch 100/224 | Loss: 0.9077 | LR: 9.59e-05\n",
            "  Batch 200/224 | Loss: 0.9028 | LR: 9.58e-05\n",
            "\n",
            "  Train Loss: 0.9048\n",
            "  Val Loss:   0.8988 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_140.pt\n",
            "Epoch 141/1000\n",
            "  Batch 100/224 | Loss: 0.8996 | LR: 9.58e-05\n",
            "  Batch 200/224 | Loss: 0.9020 | LR: 9.58e-05\n",
            "\n",
            "  Train Loss: 0.9015\n",
            "  Val Loss:   0.9008 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 142/1000\n",
            "  Batch 100/224 | Loss: 0.8976 | LR: 9.57e-05\n",
            "  Batch 200/224 | Loss: 0.9036 | LR: 9.57e-05\n",
            "\n",
            "  Train Loss: 0.9037\n",
            "  Val Loss:   0.8979 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 143/1000\n",
            "  Batch 100/224 | Loss: 0.9149 | LR: 9.57e-05\n",
            "  Batch 200/224 | Loss: 0.8981 | LR: 9.56e-05\n",
            "\n",
            "  Train Loss: 0.8976\n",
            "  Val Loss:   0.8951 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_143.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 144/1000\n",
            "  Batch 100/224 | Loss: 0.9031 | LR: 9.56e-05\n",
            "  Batch 200/224 | Loss: 0.8961 | LR: 9.56e-05\n",
            "\n",
            "  Train Loss: 0.8970\n",
            "  Val Loss:   0.8959 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 145/1000\n",
            "  Batch 100/224 | Loss: 0.8985 | LR: 9.55e-05\n",
            "  Batch 200/224 | Loss: 0.8956 | LR: 9.55e-05\n",
            "\n",
            "  Train Loss: 0.8972\n",
            "  Val Loss:   0.8929 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_145.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 146/1000\n",
            "  Batch 100/224 | Loss: 0.8824 | LR: 9.55e-05\n",
            "  Batch 200/224 | Loss: 0.8955 | LR: 9.54e-05\n",
            "\n",
            "  Train Loss: 0.8963\n",
            "  Val Loss:   0.8905 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_146.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 147/1000\n",
            "  Batch 100/224 | Loss: 0.8811 | LR: 9.54e-05\n",
            "  Batch 200/224 | Loss: 0.8968 | LR: 9.54e-05\n",
            "\n",
            "  Train Loss: 0.8960\n",
            "  Val Loss:   0.8917 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 148/1000\n",
            "  Batch 100/224 | Loss: 0.8835 | LR: 9.53e-05\n",
            "  Batch 200/224 | Loss: 0.8927 | LR: 9.53e-05\n",
            "\n",
            "  Train Loss: 0.8923\n",
            "  Val Loss:   0.8903 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_148.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 149/1000\n",
            "  Batch 100/224 | Loss: 0.8922 | LR: 9.53e-05\n",
            "  Batch 200/224 | Loss: 0.8915 | LR: 9.52e-05\n",
            "\n",
            "  Train Loss: 0.8892\n",
            "  Val Loss:   0.8867 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_149.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 150/1000\n",
            "  Batch 100/224 | Loss: 0.8919 | LR: 9.52e-05\n",
            "  Batch 200/224 | Loss: 0.8939 | LR: 9.52e-05\n",
            "\n",
            "  Train Loss: 0.8918\n",
            "  Val Loss:   0.9079 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_150.pt\n",
            "Epoch 151/1000\n",
            "  Batch 100/224 | Loss: 0.8938 | LR: 9.51e-05\n",
            "  Batch 200/224 | Loss: 0.8954 | LR: 9.51e-05\n",
            "\n",
            "  Train Loss: 0.8938\n",
            "  Val Loss:   0.8895 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 152/1000\n",
            "  Batch 100/224 | Loss: 0.8956 | LR: 9.51e-05\n",
            "  Batch 200/224 | Loss: 0.8856 | LR: 9.50e-05\n",
            "\n",
            "  Train Loss: 0.8871\n",
            "  Val Loss:   0.8888 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 153/1000\n",
            "  Batch 100/224 | Loss: 0.8878 | LR: 9.50e-05\n",
            "  Batch 200/224 | Loss: 0.8815 | LR: 9.50e-05\n",
            "\n",
            "  Train Loss: 0.8883\n",
            "  Val Loss:   0.8867 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 154/1000\n",
            "  Batch 100/224 | Loss: 0.8811 | LR: 9.49e-05\n",
            "  Batch 200/224 | Loss: 0.8876 | LR: 9.49e-05\n",
            "\n",
            "  Train Loss: 0.8882\n",
            "  Val Loss:   0.8861 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_154.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 155/1000\n",
            "  Batch 100/224 | Loss: 0.8739 | LR: 9.49e-05\n",
            "  Batch 200/224 | Loss: 0.8796 | LR: 9.48e-05\n",
            "\n",
            "  Train Loss: 0.8822\n",
            "  Val Loss:   0.8850 [BEST]\n",
            "  Time:       2.8s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_155.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 156/1000\n",
            "  Batch 100/224 | Loss: 0.8913 | LR: 9.48e-05\n",
            "  Batch 200/224 | Loss: 0.8863 | LR: 9.48e-05\n",
            "\n",
            "  Train Loss: 0.8886\n",
            "  Val Loss:   0.8839 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_156.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 157/1000\n",
            "  Batch 100/224 | Loss: 0.8789 | LR: 9.47e-05\n",
            "  Batch 200/224 | Loss: 0.8851 | LR: 9.47e-05\n",
            "\n",
            "  Train Loss: 0.8816\n",
            "  Val Loss:   0.8884 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 158/1000\n",
            "  Batch 100/224 | Loss: 0.8862 | LR: 9.47e-05\n",
            "  Batch 200/224 | Loss: 0.8780 | LR: 9.46e-05\n",
            "\n",
            "  Train Loss: 0.8822\n",
            "  Val Loss:   0.8877 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 159/1000\n",
            "  Batch 100/224 | Loss: 0.8857 | LR: 9.46e-05\n",
            "  Batch 200/224 | Loss: 0.8836 | LR: 9.46e-05\n",
            "\n",
            "  Train Loss: 0.8800\n",
            "  Val Loss:   0.8814 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_159.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 160/1000\n",
            "  Batch 100/224 | Loss: 0.8793 | LR: 9.45e-05\n",
            "  Batch 200/224 | Loss: 0.8784 | LR: 9.45e-05\n",
            "\n",
            "  Train Loss: 0.8823\n",
            "  Val Loss:   0.8882 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_160.pt\n",
            "Epoch 161/1000\n",
            "  Batch 100/224 | Loss: 0.8747 | LR: 9.44e-05\n",
            "  Batch 200/224 | Loss: 0.8840 | LR: 9.44e-05\n",
            "\n",
            "  Train Loss: 0.8782\n",
            "  Val Loss:   0.8874 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 162/1000\n",
            "  Batch 100/224 | Loss: 0.8799 | LR: 9.44e-05\n",
            "  Batch 200/224 | Loss: 0.8835 | LR: 9.43e-05\n",
            "\n",
            "  Train Loss: 0.8835\n",
            "  Val Loss:   0.8852 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 163/1000\n",
            "  Batch 100/224 | Loss: 0.8808 | LR: 9.43e-05\n",
            "  Batch 200/224 | Loss: 0.8820 | LR: 9.43e-05\n",
            "\n",
            "  Train Loss: 0.8808\n",
            "  Val Loss:   0.8852 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 164/1000\n",
            "  Batch 100/224 | Loss: 0.8713 | LR: 9.42e-05\n",
            "  Batch 200/224 | Loss: 0.8780 | LR: 9.42e-05\n",
            "\n",
            "  Train Loss: 0.8796\n",
            "  Val Loss:   0.8884 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 165/1000\n",
            "  Batch 100/224 | Loss: 0.8858 | LR: 9.42e-05\n",
            "  Batch 200/224 | Loss: 0.8820 | LR: 9.41e-05\n",
            "\n",
            "  Train Loss: 0.8801\n",
            "  Val Loss:   0.8851 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_165.pt\n",
            "Epoch 166/1000\n",
            "  Batch 100/224 | Loss: 0.8536 | LR: 9.41e-05\n",
            "  Batch 200/224 | Loss: 0.8745 | LR: 9.40e-05\n",
            "\n",
            "  Train Loss: 0.8757\n",
            "  Val Loss:   0.8921 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 167/1000\n",
            "  Batch 100/224 | Loss: 0.8695 | LR: 9.40e-05\n",
            "  Batch 200/224 | Loss: 0.8742 | LR: 9.40e-05\n",
            "\n",
            "  Train Loss: 0.8779\n",
            "  Val Loss:   0.8787 [BEST]\n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_167.pt\n",
            "  ✓ Saved best model: checkpoints/best_model.pt\n",
            "Epoch 168/1000\n",
            "  Batch 100/224 | Loss: 0.8769 | LR: 9.39e-05\n",
            "  Batch 200/224 | Loss: 0.8760 | LR: 9.39e-05\n",
            "\n",
            "  Train Loss: 0.8736\n",
            "  Val Loss:   0.8906 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 169/1000\n",
            "  Batch 100/224 | Loss: 0.8728 | LR: 9.39e-05\n",
            "  Batch 200/224 | Loss: 0.8707 | LR: 9.38e-05\n",
            "\n",
            "  Train Loss: 0.8724\n",
            "  Val Loss:   0.8850 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 170/1000\n",
            "  Batch 100/224 | Loss: 0.8742 | LR: 9.38e-05\n",
            "  Batch 200/224 | Loss: 0.8686 | LR: 9.37e-05\n",
            "\n",
            "  Train Loss: 0.8696\n",
            "  Val Loss:   0.8943 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_170.pt\n",
            "Epoch 171/1000\n",
            "  Batch 100/224 | Loss: 0.8690 | LR: 9.37e-05\n",
            "  Batch 200/224 | Loss: 0.8725 | LR: 9.37e-05\n",
            "\n",
            "  Train Loss: 0.8734\n",
            "  Val Loss:   0.8820 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 172/1000\n",
            "  Batch 100/224 | Loss: 0.8649 | LR: 9.36e-05\n",
            "  Batch 200/224 | Loss: 0.8680 | LR: 9.36e-05\n",
            "\n",
            "  Train Loss: 0.8713\n",
            "  Val Loss:   0.8830 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 173/1000\n",
            "  Batch 100/224 | Loss: 0.8891 | LR: 9.35e-05\n",
            "  Batch 200/224 | Loss: 0.8738 | LR: 9.35e-05\n",
            "\n",
            "  Train Loss: 0.8706\n",
            "  Val Loss:   0.8827 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 174/1000\n",
            "  Batch 100/224 | Loss: 0.8799 | LR: 9.35e-05\n",
            "  Batch 200/224 | Loss: 0.8751 | LR: 9.34e-05\n",
            "\n",
            "  Train Loss: 0.8738\n",
            "  Val Loss:   0.8831 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 175/1000\n",
            "  Batch 100/224 | Loss: 0.8719 | LR: 9.34e-05\n",
            "  Batch 200/224 | Loss: 0.8696 | LR: 9.34e-05\n",
            "\n",
            "  Train Loss: 0.8699\n",
            "  Val Loss:   0.8791 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_175.pt\n",
            "Epoch 176/1000\n",
            "  Batch 100/224 | Loss: 0.8700 | LR: 9.33e-05\n",
            "  Batch 200/224 | Loss: 0.8692 | LR: 9.33e-05\n",
            "\n",
            "  Train Loss: 0.8711\n",
            "  Val Loss:   0.8868 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 177/1000\n",
            "  Batch 100/224 | Loss: 0.8785 | LR: 9.32e-05\n",
            "  Batch 200/224 | Loss: 0.8695 | LR: 9.32e-05\n",
            "\n",
            "  Train Loss: 0.8690\n",
            "  Val Loss:   0.8895 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 178/1000\n",
            "  Batch 100/224 | Loss: 0.8692 | LR: 9.32e-05\n",
            "  Batch 200/224 | Loss: 0.8653 | LR: 9.31e-05\n",
            "\n",
            "  Train Loss: 0.8689\n",
            "  Val Loss:   0.8798 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 179/1000\n",
            "  Batch 100/224 | Loss: 0.8715 | LR: 9.31e-05\n",
            "  Batch 200/224 | Loss: 0.8671 | LR: 9.30e-05\n",
            "\n",
            "  Train Loss: 0.8649\n",
            "  Val Loss:   0.8856 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 180/1000\n",
            "  Batch 100/224 | Loss: 0.8680 | LR: 9.30e-05\n",
            "  Batch 200/224 | Loss: 0.8647 | LR: 9.30e-05\n",
            "\n",
            "  Train Loss: 0.8659\n",
            "  Val Loss:   0.8835 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_180.pt\n",
            "Epoch 181/1000\n",
            "  Batch 100/224 | Loss: 0.8567 | LR: 9.29e-05\n",
            "  Batch 200/224 | Loss: 0.8683 | LR: 9.29e-05\n",
            "\n",
            "  Train Loss: 0.8672\n",
            "  Val Loss:   0.8821 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 182/1000\n",
            "  Batch 100/224 | Loss: 0.8558 | LR: 9.28e-05\n",
            "  Batch 200/224 | Loss: 0.8671 | LR: 9.28e-05\n",
            "\n",
            "  Train Loss: 0.8666\n",
            "  Val Loss:   0.8829 \n",
            "  Time:       2.8s\n",
            "\n",
            "Epoch 183/1000\n",
            "  Batch 100/224 | Loss: 0.8652 | LR: 9.28e-05\n",
            "  Batch 200/224 | Loss: 0.8622 | LR: 9.27e-05\n",
            "\n",
            "  Train Loss: 0.8639\n",
            "  Val Loss:   0.8796 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 184/1000\n",
            "  Batch 100/224 | Loss: 0.8628 | LR: 9.27e-05\n",
            "  Batch 200/224 | Loss: 0.8617 | LR: 9.26e-05\n",
            "\n",
            "  Train Loss: 0.8648\n",
            "  Val Loss:   0.8812 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 185/1000\n",
            "  Batch 100/224 | Loss: 0.8506 | LR: 9.26e-05\n",
            "  Batch 200/224 | Loss: 0.8626 | LR: 9.26e-05\n",
            "\n",
            "  Train Loss: 0.8628\n",
            "  Val Loss:   0.8820 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_185.pt\n",
            "Epoch 186/1000\n",
            "  Batch 100/224 | Loss: 0.8599 | LR: 9.25e-05\n",
            "  Batch 200/224 | Loss: 0.8606 | LR: 9.25e-05\n",
            "\n",
            "  Train Loss: 0.8639\n",
            "  Val Loss:   0.8816 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 187/1000\n",
            "  Batch 100/224 | Loss: 0.8472 | LR: 9.24e-05\n",
            "  Batch 200/224 | Loss: 0.8604 | LR: 9.24e-05\n",
            "\n",
            "  Train Loss: 0.8624\n",
            "  Val Loss:   0.8839 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 188/1000\n",
            "  Batch 100/224 | Loss: 0.8565 | LR: 9.23e-05\n",
            "  Batch 200/224 | Loss: 0.8610 | LR: 9.23e-05\n",
            "\n",
            "  Train Loss: 0.8576\n",
            "  Val Loss:   0.8799 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 189/1000\n",
            "  Batch 100/224 | Loss: 0.8563 | LR: 9.23e-05\n",
            "  Batch 200/224 | Loss: 0.8571 | LR: 9.22e-05\n",
            "\n",
            "  Train Loss: 0.8604\n",
            "  Val Loss:   0.8804 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 190/1000\n",
            "  Batch 100/224 | Loss: 0.8589 | LR: 9.22e-05\n",
            "  Batch 200/224 | Loss: 0.8594 | LR: 9.21e-05\n",
            "\n",
            "  Train Loss: 0.8619\n",
            "  Val Loss:   0.8825 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_190.pt\n",
            "Epoch 191/1000\n",
            "  Batch 100/224 | Loss: 0.8531 | LR: 9.21e-05\n",
            "  Batch 200/224 | Loss: 0.8586 | LR: 9.21e-05\n",
            "\n",
            "  Train Loss: 0.8588\n",
            "  Val Loss:   0.8841 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 192/1000\n",
            "  Batch 100/224 | Loss: 0.8743 | LR: 9.20e-05\n",
            "  Batch 200/224 | Loss: 0.8625 | LR: 9.20e-05\n",
            "\n",
            "  Train Loss: 0.8624\n",
            "  Val Loss:   0.8817 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 193/1000\n",
            "  Batch 100/224 | Loss: 0.8476 | LR: 9.19e-05\n",
            "  Batch 200/224 | Loss: 0.8481 | LR: 9.19e-05\n",
            "\n",
            "  Train Loss: 0.8535\n",
            "  Val Loss:   0.8789 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 194/1000\n",
            "  Batch 100/224 | Loss: 0.8615 | LR: 9.18e-05\n",
            "  Batch 200/224 | Loss: 0.8567 | LR: 9.18e-05\n",
            "\n",
            "  Train Loss: 0.8610\n",
            "  Val Loss:   0.8818 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 195/1000\n",
            "  Batch 100/224 | Loss: 0.8661 | LR: 9.17e-05\n",
            "  Batch 200/224 | Loss: 0.8626 | LR: 9.17e-05\n",
            "\n",
            "  Train Loss: 0.8578\n",
            "  Val Loss:   0.8797 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_195.pt\n",
            "Epoch 196/1000\n",
            "  Batch 100/224 | Loss: 0.8612 | LR: 9.17e-05\n",
            "  Batch 200/224 | Loss: 0.8656 | LR: 9.16e-05\n",
            "\n",
            "  Train Loss: 0.8584\n",
            "  Val Loss:   0.8821 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 197/1000\n",
            "  Batch 100/224 | Loss: 0.8515 | LR: 9.16e-05\n",
            "  Batch 200/224 | Loss: 0.8621 | LR: 9.15e-05\n",
            "\n",
            "  Train Loss: 0.8590\n",
            "  Val Loss:   0.8873 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 198/1000\n",
            "  Batch 100/224 | Loss: 0.8509 | LR: 9.15e-05\n",
            "  Batch 200/224 | Loss: 0.8546 | LR: 9.14e-05\n",
            "\n",
            "  Train Loss: 0.8567\n",
            "  Val Loss:   0.8804 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 199/1000\n",
            "  Batch 100/224 | Loss: 0.8497 | LR: 9.14e-05\n",
            "  Batch 200/224 | Loss: 0.8532 | LR: 9.14e-05\n",
            "\n",
            "  Train Loss: 0.8561\n",
            "  Val Loss:   0.8904 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 200/1000\n",
            "  Batch 100/224 | Loss: 0.8536 | LR: 9.13e-05\n",
            "  Batch 200/224 | Loss: 0.8519 | LR: 9.13e-05\n",
            "\n",
            "  Train Loss: 0.8501\n",
            "  Val Loss:   0.8867 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_200.pt\n",
            "Epoch 201/1000\n",
            "  Batch 100/224 | Loss: 0.8582 | LR: 9.12e-05\n",
            "  Batch 200/224 | Loss: 0.8519 | LR: 9.12e-05\n",
            "\n",
            "  Train Loss: 0.8528\n",
            "  Val Loss:   0.8836 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 202/1000\n",
            "  Batch 100/224 | Loss: 0.8572 | LR: 9.11e-05\n",
            "  Batch 200/224 | Loss: 0.8456 | LR: 9.11e-05\n",
            "\n",
            "  Train Loss: 0.8492\n",
            "  Val Loss:   0.8808 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 203/1000\n",
            "  Batch 100/224 | Loss: 0.8461 | LR: 9.10e-05\n",
            "  Batch 200/224 | Loss: 0.8490 | LR: 9.10e-05\n",
            "\n",
            "  Train Loss: 0.8526\n",
            "  Val Loss:   0.8833 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 204/1000\n",
            "  Batch 100/224 | Loss: 0.8561 | LR: 9.10e-05\n",
            "  Batch 200/224 | Loss: 0.8526 | LR: 9.09e-05\n",
            "\n",
            "  Train Loss: 0.8519\n",
            "  Val Loss:   0.8919 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 205/1000\n",
            "  Batch 100/224 | Loss: 0.8446 | LR: 9.09e-05\n",
            "  Batch 200/224 | Loss: 0.8496 | LR: 9.08e-05\n",
            "\n",
            "  Train Loss: 0.8496\n",
            "  Val Loss:   0.8803 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_205.pt\n",
            "Epoch 206/1000\n",
            "  Batch 100/224 | Loss: 0.8627 | LR: 9.08e-05\n",
            "  Batch 200/224 | Loss: 0.8472 | LR: 9.07e-05\n",
            "\n",
            "  Train Loss: 0.8487\n",
            "  Val Loss:   0.8836 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 207/1000\n",
            "  Batch 100/224 | Loss: 0.8404 | LR: 9.07e-05\n",
            "  Batch 200/224 | Loss: 0.8524 | LR: 9.06e-05\n",
            "\n",
            "  Train Loss: 0.8534\n",
            "  Val Loss:   0.8843 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 208/1000\n",
            "  Batch 100/224 | Loss: 0.8576 | LR: 9.06e-05\n",
            "  Batch 200/224 | Loss: 0.8510 | LR: 9.05e-05\n",
            "\n",
            "  Train Loss: 0.8505\n",
            "  Val Loss:   0.8907 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 209/1000\n",
            "  Batch 100/224 | Loss: 0.8524 | LR: 9.05e-05\n",
            "  Batch 200/224 | Loss: 0.8471 | LR: 9.05e-05\n",
            "\n",
            "  Train Loss: 0.8486\n",
            "  Val Loss:   0.8857 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 210/1000\n",
            "  Batch 100/224 | Loss: 0.8550 | LR: 9.04e-05\n",
            "  Batch 200/224 | Loss: 0.8506 | LR: 9.04e-05\n",
            "\n",
            "  Train Loss: 0.8514\n",
            "  Val Loss:   0.8822 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_210.pt\n",
            "Epoch 211/1000\n",
            "  Batch 100/224 | Loss: 0.8506 | LR: 9.03e-05\n",
            "  Batch 200/224 | Loss: 0.8569 | LR: 9.03e-05\n",
            "\n",
            "  Train Loss: 0.8529\n",
            "  Val Loss:   0.8790 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 212/1000\n",
            "  Batch 100/224 | Loss: 0.8484 | LR: 9.02e-05\n",
            "  Batch 200/224 | Loss: 0.8526 | LR: 9.02e-05\n",
            "\n",
            "  Train Loss: 0.8510\n",
            "  Val Loss:   0.8809 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 213/1000\n",
            "  Batch 100/224 | Loss: 0.8475 | LR: 9.01e-05\n",
            "  Batch 200/224 | Loss: 0.8473 | LR: 9.01e-05\n",
            "\n",
            "  Train Loss: 0.8501\n",
            "  Val Loss:   0.8826 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 214/1000\n",
            "  Batch 100/224 | Loss: 0.8580 | LR: 9.00e-05\n",
            "  Batch 200/224 | Loss: 0.8510 | LR: 9.00e-05\n",
            "\n",
            "  Train Loss: 0.8477\n",
            "  Val Loss:   0.8854 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 215/1000\n",
            "  Batch 100/224 | Loss: 0.8464 | LR: 8.99e-05\n",
            "  Batch 200/224 | Loss: 0.8490 | LR: 8.99e-05\n",
            "\n",
            "  Train Loss: 0.8470\n",
            "  Val Loss:   0.8823 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_215.pt\n",
            "Epoch 216/1000\n",
            "  Batch 100/224 | Loss: 0.8413 | LR: 8.98e-05\n",
            "  Batch 200/224 | Loss: 0.8416 | LR: 8.98e-05\n",
            "\n",
            "  Train Loss: 0.8424\n",
            "  Val Loss:   0.8838 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 217/1000\n",
            "  Batch 100/224 | Loss: 0.8400 | LR: 8.97e-05\n",
            "  Batch 200/224 | Loss: 0.8480 | LR: 8.97e-05\n",
            "\n",
            "  Train Loss: 0.8446\n",
            "  Val Loss:   0.8924 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 218/1000\n",
            "  Batch 100/224 | Loss: 0.8303 | LR: 8.97e-05\n",
            "  Batch 200/224 | Loss: 0.8350 | LR: 8.96e-05\n",
            "\n",
            "  Train Loss: 0.8398\n",
            "  Val Loss:   0.8790 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 219/1000\n",
            "  Batch 100/224 | Loss: 0.8302 | LR: 8.96e-05\n",
            "  Batch 200/224 | Loss: 0.8408 | LR: 8.95e-05\n",
            "\n",
            "  Train Loss: 0.8409\n",
            "  Val Loss:   0.8915 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 220/1000\n",
            "  Batch 100/224 | Loss: 0.8611 | LR: 8.95e-05\n",
            "  Batch 200/224 | Loss: 0.8514 | LR: 8.94e-05\n",
            "\n",
            "  Train Loss: 0.8444\n",
            "  Val Loss:   0.8842 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_220.pt\n",
            "Epoch 221/1000\n",
            "  Batch 100/224 | Loss: 0.8361 | LR: 8.94e-05\n",
            "  Batch 200/224 | Loss: 0.8410 | LR: 8.93e-05\n",
            "\n",
            "  Train Loss: 0.8435\n",
            "  Val Loss:   0.8860 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 222/1000\n",
            "  Batch 100/224 | Loss: 0.8338 | LR: 8.93e-05\n",
            "  Batch 200/224 | Loss: 0.8431 | LR: 8.92e-05\n",
            "\n",
            "  Train Loss: 0.8429\n",
            "  Val Loss:   0.8907 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 223/1000\n",
            "  Batch 100/224 | Loss: 0.8458 | LR: 8.92e-05\n",
            "  Batch 200/224 | Loss: 0.8397 | LR: 8.91e-05\n",
            "\n",
            "  Train Loss: 0.8412\n",
            "  Val Loss:   0.8844 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 224/1000\n",
            "  Batch 100/224 | Loss: 0.8279 | LR: 8.91e-05\n",
            "  Batch 200/224 | Loss: 0.8383 | LR: 8.90e-05\n",
            "\n",
            "  Train Loss: 0.8424\n",
            "  Val Loss:   0.8892 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 225/1000\n",
            "  Batch 100/224 | Loss: 0.8522 | LR: 8.90e-05\n",
            "  Batch 200/224 | Loss: 0.8436 | LR: 8.89e-05\n",
            "\n",
            "  Train Loss: 0.8439\n",
            "  Val Loss:   0.8862 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_225.pt\n",
            "Epoch 226/1000\n",
            "  Batch 100/224 | Loss: 0.8305 | LR: 8.89e-05\n",
            "  Batch 200/224 | Loss: 0.8356 | LR: 8.88e-05\n",
            "\n",
            "  Train Loss: 0.8389\n",
            "  Val Loss:   0.8948 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 227/1000\n",
            "  Batch 100/224 | Loss: 0.8362 | LR: 8.88e-05\n",
            "  Batch 200/224 | Loss: 0.8339 | LR: 8.87e-05\n",
            "\n",
            "  Train Loss: 0.8383\n",
            "  Val Loss:   0.8842 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 228/1000\n",
            "  Batch 100/224 | Loss: 0.8396 | LR: 8.87e-05\n",
            "  Batch 200/224 | Loss: 0.8425 | LR: 8.86e-05\n",
            "\n",
            "  Train Loss: 0.8424\n",
            "  Val Loss:   0.8875 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 229/1000\n",
            "  Batch 100/224 | Loss: 0.8544 | LR: 8.86e-05\n",
            "  Batch 200/224 | Loss: 0.8449 | LR: 8.85e-05\n",
            "\n",
            "  Train Loss: 0.8374\n",
            "  Val Loss:   0.8870 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 230/1000\n",
            "  Batch 100/224 | Loss: 0.8182 | LR: 8.85e-05\n",
            "  Batch 200/224 | Loss: 0.8367 | LR: 8.84e-05\n",
            "\n",
            "  Train Loss: 0.8373\n",
            "  Val Loss:   0.8893 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_230.pt\n",
            "Epoch 231/1000\n",
            "  Batch 100/224 | Loss: 0.8407 | LR: 8.84e-05\n",
            "  Batch 200/224 | Loss: 0.8420 | LR: 8.83e-05\n",
            "\n",
            "  Train Loss: 0.8416\n",
            "  Val Loss:   0.8942 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 232/1000\n",
            "  Batch 100/224 | Loss: 0.8381 | LR: 8.83e-05\n",
            "  Batch 200/224 | Loss: 0.8350 | LR: 8.82e-05\n",
            "\n",
            "  Train Loss: 0.8344\n",
            "  Val Loss:   0.8852 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 233/1000\n",
            "  Batch 100/224 | Loss: 0.8264 | LR: 8.82e-05\n",
            "  Batch 200/224 | Loss: 0.8318 | LR: 8.81e-05\n",
            "\n",
            "  Train Loss: 0.8365\n",
            "  Val Loss:   0.8897 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 234/1000\n",
            "  Batch 100/224 | Loss: 0.8351 | LR: 8.81e-05\n",
            "  Batch 200/224 | Loss: 0.8373 | LR: 8.80e-05\n",
            "\n",
            "  Train Loss: 0.8334\n",
            "  Val Loss:   0.8965 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 235/1000\n",
            "  Batch 100/224 | Loss: 0.8394 | LR: 8.80e-05\n",
            "  Batch 200/224 | Loss: 0.8383 | LR: 8.79e-05\n",
            "\n",
            "  Train Loss: 0.8359\n",
            "  Val Loss:   0.8919 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_235.pt\n",
            "Epoch 236/1000\n",
            "  Batch 100/224 | Loss: 0.8239 | LR: 8.79e-05\n",
            "  Batch 200/224 | Loss: 0.8291 | LR: 8.78e-05\n",
            "\n",
            "  Train Loss: 0.8321\n",
            "  Val Loss:   0.8937 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 237/1000\n",
            "  Batch 100/224 | Loss: 0.8392 | LR: 8.78e-05\n",
            "  Batch 200/224 | Loss: 0.8349 | LR: 8.77e-05\n",
            "\n",
            "  Train Loss: 0.8411\n",
            "  Val Loss:   0.8863 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 238/1000\n",
            "  Batch 100/224 | Loss: 0.8253 | LR: 8.77e-05\n",
            "  Batch 200/224 | Loss: 0.8319 | LR: 8.76e-05\n",
            "\n",
            "  Train Loss: 0.8342\n",
            "  Val Loss:   0.8926 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 239/1000\n",
            "  Batch 100/224 | Loss: 0.8393 | LR: 8.76e-05\n",
            "  Batch 200/224 | Loss: 0.8325 | LR: 8.75e-05\n",
            "\n",
            "  Train Loss: 0.8357\n",
            "  Val Loss:   0.8908 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 240/1000\n",
            "  Batch 100/224 | Loss: 0.8312 | LR: 8.75e-05\n",
            "  Batch 200/224 | Loss: 0.8319 | LR: 8.74e-05\n",
            "\n",
            "  Train Loss: 0.8313\n",
            "  Val Loss:   0.8931 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_240.pt\n",
            "Epoch 241/1000\n",
            "  Batch 100/224 | Loss: 0.8325 | LR: 8.73e-05\n",
            "  Batch 200/224 | Loss: 0.8301 | LR: 8.73e-05\n",
            "\n",
            "  Train Loss: 0.8320\n",
            "  Val Loss:   0.8932 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 242/1000\n",
            "  Batch 100/224 | Loss: 0.8267 | LR: 8.72e-05\n",
            "  Batch 200/224 | Loss: 0.8354 | LR: 8.72e-05\n",
            "\n",
            "  Train Loss: 0.8351\n",
            "  Val Loss:   0.8948 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 243/1000\n",
            "  Batch 100/224 | Loss: 0.8157 | LR: 8.71e-05\n",
            "  Batch 200/224 | Loss: 0.8278 | LR: 8.71e-05\n",
            "\n",
            "  Train Loss: 0.8269\n",
            "  Val Loss:   0.8866 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 244/1000\n",
            "  Batch 100/224 | Loss: 0.8342 | LR: 8.70e-05\n",
            "  Batch 200/224 | Loss: 0.8357 | LR: 8.70e-05\n",
            "\n",
            "  Train Loss: 0.8320\n",
            "  Val Loss:   0.8945 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 245/1000\n",
            "  Batch 100/224 | Loss: 0.8233 | LR: 8.69e-05\n",
            "  Batch 200/224 | Loss: 0.8262 | LR: 8.69e-05\n",
            "\n",
            "  Train Loss: 0.8330\n",
            "  Val Loss:   0.8965 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_245.pt\n",
            "Epoch 246/1000\n",
            "  Batch 100/224 | Loss: 0.8464 | LR: 8.68e-05\n",
            "  Batch 200/224 | Loss: 0.8373 | LR: 8.68e-05\n",
            "\n",
            "  Train Loss: 0.8326\n",
            "  Val Loss:   0.8942 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 247/1000\n",
            "  Batch 100/224 | Loss: 0.8124 | LR: 8.67e-05\n",
            "  Batch 200/224 | Loss: 0.8258 | LR: 8.67e-05\n",
            "\n",
            "  Train Loss: 0.8266\n",
            "  Val Loss:   0.8942 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 248/1000\n",
            "  Batch 100/224 | Loss: 0.8086 | LR: 8.66e-05\n",
            "  Batch 200/224 | Loss: 0.8270 | LR: 8.66e-05\n",
            "\n",
            "  Train Loss: 0.8302\n",
            "  Val Loss:   0.8926 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 249/1000\n",
            "  Batch 100/224 | Loss: 0.8315 | LR: 8.65e-05\n",
            "  Batch 200/224 | Loss: 0.8303 | LR: 8.65e-05\n",
            "\n",
            "  Train Loss: 0.8301\n",
            "  Val Loss:   0.9016 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 250/1000\n",
            "  Batch 100/224 | Loss: 0.8200 | LR: 8.64e-05\n",
            "  Batch 200/224 | Loss: 0.8317 | LR: 8.63e-05\n",
            "\n",
            "  Train Loss: 0.8312\n",
            "  Val Loss:   0.8992 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_250.pt\n",
            "Epoch 251/1000\n",
            "  Batch 100/224 | Loss: 0.8162 | LR: 8.63e-05\n",
            "  Batch 200/224 | Loss: 0.8275 | LR: 8.62e-05\n",
            "\n",
            "  Train Loss: 0.8310\n",
            "  Val Loss:   0.8963 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 252/1000\n",
            "  Batch 100/224 | Loss: 0.8242 | LR: 8.62e-05\n",
            "  Batch 200/224 | Loss: 0.8253 | LR: 8.61e-05\n",
            "\n",
            "  Train Loss: 0.8262\n",
            "  Val Loss:   0.9014 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 253/1000\n",
            "  Batch 100/224 | Loss: 0.8257 | LR: 8.61e-05\n",
            "  Batch 200/224 | Loss: 0.8297 | LR: 8.60e-05\n",
            "\n",
            "  Train Loss: 0.8279\n",
            "  Val Loss:   0.8981 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 254/1000\n",
            "  Batch 100/224 | Loss: 0.8362 | LR: 8.60e-05\n",
            "  Batch 200/224 | Loss: 0.8290 | LR: 8.59e-05\n",
            "\n",
            "  Train Loss: 0.8286\n",
            "  Val Loss:   0.9034 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 255/1000\n",
            "  Batch 100/224 | Loss: 0.8264 | LR: 8.58e-05\n",
            "  Batch 200/224 | Loss: 0.8284 | LR: 8.58e-05\n",
            "\n",
            "  Train Loss: 0.8244\n",
            "  Val Loss:   0.8991 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_255.pt\n",
            "Epoch 256/1000\n",
            "  Batch 100/224 | Loss: 0.8074 | LR: 8.57e-05\n",
            "  Batch 200/224 | Loss: 0.8205 | LR: 8.57e-05\n",
            "\n",
            "  Train Loss: 0.8249\n",
            "  Val Loss:   0.8984 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 257/1000\n",
            "  Batch 100/224 | Loss: 0.8396 | LR: 8.56e-05\n",
            "  Batch 200/224 | Loss: 0.8289 | LR: 8.56e-05\n",
            "\n",
            "  Train Loss: 0.8275\n",
            "  Val Loss:   0.8992 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 258/1000\n",
            "  Batch 100/224 | Loss: 0.8367 | LR: 8.55e-05\n",
            "  Batch 200/224 | Loss: 0.8227 | LR: 8.55e-05\n",
            "\n",
            "  Train Loss: 0.8227\n",
            "  Val Loss:   0.8977 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 259/1000\n",
            "  Batch 100/224 | Loss: 0.8423 | LR: 8.54e-05\n",
            "  Batch 200/224 | Loss: 0.8282 | LR: 8.54e-05\n",
            "\n",
            "  Train Loss: 0.8246\n",
            "  Val Loss:   0.9048 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 260/1000\n",
            "  Batch 100/224 | Loss: 0.8195 | LR: 8.53e-05\n",
            "  Batch 200/224 | Loss: 0.8189 | LR: 8.52e-05\n",
            "\n",
            "  Train Loss: 0.8250\n",
            "  Val Loss:   0.8989 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_260.pt\n",
            "Epoch 261/1000\n",
            "  Batch 100/224 | Loss: 0.8143 | LR: 8.52e-05\n",
            "  Batch 200/224 | Loss: 0.8280 | LR: 8.51e-05\n",
            "\n",
            "  Train Loss: 0.8256\n",
            "  Val Loss:   0.9009 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 262/1000\n",
            "  Batch 100/224 | Loss: 0.8258 | LR: 8.51e-05\n",
            "  Batch 200/224 | Loss: 0.8254 | LR: 8.50e-05\n",
            "\n",
            "  Train Loss: 0.8223\n",
            "  Val Loss:   0.9007 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 263/1000\n",
            "  Batch 100/224 | Loss: 0.8077 | LR: 8.50e-05\n",
            "  Batch 200/224 | Loss: 0.8222 | LR: 8.49e-05\n",
            "\n",
            "  Train Loss: 0.8227\n",
            "  Val Loss:   0.9055 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 264/1000\n",
            "  Batch 100/224 | Loss: 0.8227 | LR: 8.49e-05\n",
            "  Batch 200/224 | Loss: 0.8228 | LR: 8.48e-05\n",
            "\n",
            "  Train Loss: 0.8233\n",
            "  Val Loss:   0.9026 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 265/1000\n",
            "  Batch 100/224 | Loss: 0.8169 | LR: 8.47e-05\n",
            "  Batch 200/224 | Loss: 0.8238 | LR: 8.47e-05\n",
            "\n",
            "  Train Loss: 0.8247\n",
            "  Val Loss:   0.8988 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_265.pt\n",
            "Epoch 266/1000\n",
            "  Batch 100/224 | Loss: 0.8182 | LR: 8.46e-05\n",
            "  Batch 200/224 | Loss: 0.8156 | LR: 8.46e-05\n",
            "\n",
            "  Train Loss: 0.8169\n",
            "  Val Loss:   0.9017 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 267/1000\n",
            "  Batch 100/224 | Loss: 0.8262 | LR: 8.45e-05\n",
            "  Batch 200/224 | Loss: 0.8236 | LR: 8.45e-05\n",
            "\n",
            "  Train Loss: 0.8227\n",
            "  Val Loss:   0.9027 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 268/1000\n",
            "  Batch 100/224 | Loss: 0.8068 | LR: 8.44e-05\n",
            "  Batch 200/224 | Loss: 0.8112 | LR: 8.43e-05\n",
            "\n",
            "  Train Loss: 0.8136\n",
            "  Val Loss:   0.9113 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 269/1000\n",
            "  Batch 100/224 | Loss: 0.8190 | LR: 8.43e-05\n",
            "  Batch 200/224 | Loss: 0.8156 | LR: 8.42e-05\n",
            "\n",
            "  Train Loss: 0.8172\n",
            "  Val Loss:   0.9050 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 270/1000\n",
            "  Batch 100/224 | Loss: 0.8210 | LR: 8.42e-05\n",
            "  Batch 200/224 | Loss: 0.8201 | LR: 8.41e-05\n",
            "\n",
            "  Train Loss: 0.8178\n",
            "  Val Loss:   0.9058 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_270.pt\n",
            "Epoch 271/1000\n",
            "  Batch 100/224 | Loss: 0.8151 | LR: 8.41e-05\n",
            "  Batch 200/224 | Loss: 0.8195 | LR: 8.40e-05\n",
            "\n",
            "  Train Loss: 0.8179\n",
            "  Val Loss:   0.9073 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 272/1000\n",
            "  Batch 100/224 | Loss: 0.8125 | LR: 8.39e-05\n",
            "  Batch 200/224 | Loss: 0.8169 | LR: 8.39e-05\n",
            "\n",
            "  Train Loss: 0.8174\n",
            "  Val Loss:   0.9082 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 273/1000\n",
            "  Batch 100/224 | Loss: 0.8158 | LR: 8.38e-05\n",
            "  Batch 200/224 | Loss: 0.8184 | LR: 8.38e-05\n",
            "\n",
            "  Train Loss: 0.8201\n",
            "  Val Loss:   0.9084 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 274/1000\n",
            "  Batch 100/224 | Loss: 0.8208 | LR: 8.37e-05\n",
            "  Batch 200/224 | Loss: 0.8163 | LR: 8.37e-05\n",
            "\n",
            "  Train Loss: 0.8134\n",
            "  Val Loss:   0.9081 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 275/1000\n",
            "  Batch 100/224 | Loss: 0.8131 | LR: 8.36e-05\n",
            "  Batch 200/224 | Loss: 0.8147 | LR: 8.35e-05\n",
            "\n",
            "  Train Loss: 0.8156\n",
            "  Val Loss:   0.9110 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_275.pt\n",
            "Epoch 276/1000\n",
            "  Batch 100/224 | Loss: 0.8173 | LR: 8.35e-05\n",
            "  Batch 200/224 | Loss: 0.8114 | LR: 8.34e-05\n",
            "\n",
            "  Train Loss: 0.8132\n",
            "  Val Loss:   0.9115 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 277/1000\n",
            "  Batch 100/224 | Loss: 0.8219 | LR: 8.34e-05\n",
            "  Batch 200/224 | Loss: 0.8130 | LR: 8.33e-05\n",
            "\n",
            "  Train Loss: 0.8118\n",
            "  Val Loss:   0.9134 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 278/1000\n",
            "  Batch 100/224 | Loss: 0.8125 | LR: 8.32e-05\n",
            "  Batch 200/224 | Loss: 0.8127 | LR: 8.32e-05\n",
            "\n",
            "  Train Loss: 0.8140\n",
            "  Val Loss:   0.9053 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 279/1000\n",
            "  Batch 100/224 | Loss: 0.7993 | LR: 8.31e-05\n",
            "  Batch 200/224 | Loss: 0.8052 | LR: 8.31e-05\n",
            "\n",
            "  Train Loss: 0.8121\n",
            "  Val Loss:   0.9080 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 280/1000\n",
            "  Batch 100/224 | Loss: 0.7939 | LR: 8.30e-05\n",
            "  Batch 200/224 | Loss: 0.8095 | LR: 8.30e-05\n",
            "\n",
            "  Train Loss: 0.8121\n",
            "  Val Loss:   0.9132 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_280.pt\n",
            "Epoch 281/1000\n",
            "  Batch 100/224 | Loss: 0.7944 | LR: 8.29e-05\n",
            "  Batch 200/224 | Loss: 0.8058 | LR: 8.28e-05\n",
            "\n",
            "  Train Loss: 0.8102\n",
            "  Val Loss:   0.9100 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 282/1000\n",
            "  Batch 100/224 | Loss: 0.8147 | LR: 8.28e-05\n",
            "  Batch 200/224 | Loss: 0.8101 | LR: 8.27e-05\n",
            "\n",
            "  Train Loss: 0.8084\n",
            "  Val Loss:   0.9110 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 283/1000\n",
            "  Batch 100/224 | Loss: 0.8050 | LR: 8.26e-05\n",
            "  Batch 200/224 | Loss: 0.8127 | LR: 8.26e-05\n",
            "\n",
            "  Train Loss: 0.8119\n",
            "  Val Loss:   0.9156 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 284/1000\n",
            "  Batch 100/224 | Loss: 0.8043 | LR: 8.25e-05\n",
            "  Batch 200/224 | Loss: 0.8126 | LR: 8.25e-05\n",
            "\n",
            "  Train Loss: 0.8125\n",
            "  Val Loss:   0.9119 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 285/1000\n",
            "  Batch 100/224 | Loss: 0.8161 | LR: 8.24e-05\n",
            "  Batch 200/224 | Loss: 0.8083 | LR: 8.24e-05\n",
            "\n",
            "  Train Loss: 0.8093\n",
            "  Val Loss:   0.9124 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_285.pt\n",
            "Epoch 286/1000\n",
            "  Batch 100/224 | Loss: 0.8051 | LR: 8.23e-05\n",
            "  Batch 200/224 | Loss: 0.8093 | LR: 8.22e-05\n",
            "\n",
            "  Train Loss: 0.8090\n",
            "  Val Loss:   0.9143 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 287/1000\n",
            "  Batch 100/224 | Loss: 0.8035 | LR: 8.22e-05\n",
            "  Batch 200/224 | Loss: 0.8099 | LR: 8.21e-05\n",
            "\n",
            "  Train Loss: 0.8083\n",
            "  Val Loss:   0.9130 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 288/1000\n",
            "  Batch 100/224 | Loss: 0.8157 | LR: 8.21e-05\n",
            "  Batch 200/224 | Loss: 0.8106 | LR: 8.20e-05\n",
            "\n",
            "  Train Loss: 0.8092\n",
            "  Val Loss:   0.9105 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 289/1000\n",
            "  Batch 100/224 | Loss: 0.8123 | LR: 8.19e-05\n",
            "  Batch 200/224 | Loss: 0.8121 | LR: 8.19e-05\n",
            "\n",
            "  Train Loss: 0.8134\n",
            "  Val Loss:   0.9137 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 290/1000\n",
            "  Batch 100/224 | Loss: 0.7988 | LR: 8.18e-05\n",
            "  Batch 200/224 | Loss: 0.8049 | LR: 8.18e-05\n",
            "\n",
            "  Train Loss: 0.8091\n",
            "  Val Loss:   0.9155 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_290.pt\n",
            "Epoch 291/1000\n",
            "  Batch 100/224 | Loss: 0.8071 | LR: 8.17e-05\n",
            "  Batch 200/224 | Loss: 0.8068 | LR: 8.16e-05\n",
            "\n",
            "  Train Loss: 0.8072\n",
            "  Val Loss:   0.9174 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 292/1000\n",
            "  Batch 100/224 | Loss: 0.7967 | LR: 8.16e-05\n",
            "  Batch 200/224 | Loss: 0.8045 | LR: 8.15e-05\n",
            "\n",
            "  Train Loss: 0.8065\n",
            "  Val Loss:   0.9107 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 293/1000\n",
            "  Batch 100/224 | Loss: 0.8169 | LR: 8.14e-05\n",
            "  Batch 200/224 | Loss: 0.8090 | LR: 8.14e-05\n",
            "\n",
            "  Train Loss: 0.8078\n",
            "  Val Loss:   0.9177 \n",
            "  Time:       2.8s\n",
            "\n",
            "Epoch 294/1000\n",
            "  Batch 100/224 | Loss: 0.8034 | LR: 8.13e-05\n",
            "  Batch 200/224 | Loss: 0.8025 | LR: 8.13e-05\n",
            "\n",
            "  Train Loss: 0.8036\n",
            "  Val Loss:   0.9164 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 295/1000\n",
            "  Batch 100/224 | Loss: 0.7945 | LR: 8.12e-05\n",
            "  Batch 200/224 | Loss: 0.8001 | LR: 8.11e-05\n",
            "\n",
            "  Train Loss: 0.8042\n",
            "  Val Loss:   0.9140 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_295.pt\n",
            "Epoch 296/1000\n",
            "  Batch 100/224 | Loss: 0.7927 | LR: 8.11e-05\n",
            "  Batch 200/224 | Loss: 0.8040 | LR: 8.10e-05\n",
            "\n",
            "  Train Loss: 0.8068\n",
            "  Val Loss:   0.9145 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 297/1000\n",
            "  Batch 100/224 | Loss: 0.8057 | LR: 8.10e-05\n",
            "  Batch 200/224 | Loss: 0.8070 | LR: 8.09e-05\n",
            "\n",
            "  Train Loss: 0.8076\n",
            "  Val Loss:   0.9182 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 298/1000\n",
            "  Batch 100/224 | Loss: 0.7856 | LR: 8.08e-05\n",
            "  Batch 200/224 | Loss: 0.8020 | LR: 8.08e-05\n",
            "\n",
            "  Train Loss: 0.8000\n",
            "  Val Loss:   0.9191 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 299/1000\n",
            "  Batch 100/224 | Loss: 0.7920 | LR: 8.07e-05\n",
            "  Batch 200/224 | Loss: 0.7991 | LR: 8.06e-05\n",
            "\n",
            "  Train Loss: 0.8031\n",
            "  Val Loss:   0.9186 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 300/1000\n",
            "  Batch 100/224 | Loss: 0.8064 | LR: 8.06e-05\n",
            "  Batch 200/224 | Loss: 0.7993 | LR: 8.05e-05\n",
            "\n",
            "  Train Loss: 0.8018\n",
            "  Val Loss:   0.9223 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_300.pt\n",
            "Epoch 301/1000\n",
            "  Batch 100/224 | Loss: 0.8145 | LR: 8.05e-05\n",
            "  Batch 200/224 | Loss: 0.8069 | LR: 8.04e-05\n",
            "\n",
            "  Train Loss: 0.8031\n",
            "  Val Loss:   0.9190 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 302/1000\n",
            "  Batch 100/224 | Loss: 0.8044 | LR: 8.03e-05\n",
            "  Batch 200/224 | Loss: 0.8032 | LR: 8.03e-05\n",
            "\n",
            "  Train Loss: 0.8005\n",
            "  Val Loss:   0.9237 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 303/1000\n",
            "  Batch 100/224 | Loss: 0.8064 | LR: 8.02e-05\n",
            "  Batch 200/224 | Loss: 0.8055 | LR: 8.02e-05\n",
            "\n",
            "  Train Loss: 0.8029\n",
            "  Val Loss:   0.9187 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 304/1000\n",
            "  Batch 100/224 | Loss: 0.7835 | LR: 8.01e-05\n",
            "  Batch 200/224 | Loss: 0.7947 | LR: 8.00e-05\n",
            "\n",
            "  Train Loss: 0.7942\n",
            "  Val Loss:   0.9237 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 305/1000\n",
            "  Batch 100/224 | Loss: 0.8049 | LR: 8.00e-05\n",
            "  Batch 200/224 | Loss: 0.8003 | LR: 7.99e-05\n",
            "\n",
            "  Train Loss: 0.7991\n",
            "  Val Loss:   0.9248 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_305.pt\n",
            "Epoch 306/1000\n",
            "  Batch 100/224 | Loss: 0.7880 | LR: 7.98e-05\n",
            "  Batch 200/224 | Loss: 0.7979 | LR: 7.98e-05\n",
            "\n",
            "  Train Loss: 0.7999\n",
            "  Val Loss:   0.9237 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 307/1000\n",
            "  Batch 100/224 | Loss: 0.7886 | LR: 7.97e-05\n",
            "  Batch 200/224 | Loss: 0.7966 | LR: 7.96e-05\n",
            "\n",
            "  Train Loss: 0.7982\n",
            "  Val Loss:   0.9276 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 308/1000\n",
            "  Batch 100/224 | Loss: 0.7775 | LR: 7.96e-05\n",
            "  Batch 200/224 | Loss: 0.7988 | LR: 7.95e-05\n",
            "\n",
            "  Train Loss: 0.8006\n",
            "  Val Loss:   0.9262 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 309/1000\n",
            "  Batch 100/224 | Loss: 0.7925 | LR: 7.95e-05\n",
            "  Batch 200/224 | Loss: 0.7952 | LR: 7.94e-05\n",
            "\n",
            "  Train Loss: 0.7993\n",
            "  Val Loss:   0.9324 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 310/1000\n",
            "  Batch 100/224 | Loss: 0.8065 | LR: 7.93e-05\n",
            "  Batch 200/224 | Loss: 0.8021 | LR: 7.93e-05\n",
            "\n",
            "  Train Loss: 0.8030\n",
            "  Val Loss:   0.9228 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_310.pt\n",
            "Epoch 311/1000\n",
            "  Batch 100/224 | Loss: 0.7972 | LR: 7.92e-05\n",
            "  Batch 200/224 | Loss: 0.7974 | LR: 7.91e-05\n",
            "\n",
            "  Train Loss: 0.7944\n",
            "  Val Loss:   0.9288 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 312/1000\n",
            "  Batch 100/224 | Loss: 0.7916 | LR: 7.91e-05\n",
            "  Batch 200/224 | Loss: 0.7932 | LR: 7.90e-05\n",
            "\n",
            "  Train Loss: 0.7912\n",
            "  Val Loss:   0.9320 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 313/1000\n",
            "  Batch 100/224 | Loss: 0.8114 | LR: 7.89e-05\n",
            "  Batch 200/224 | Loss: 0.7994 | LR: 7.89e-05\n",
            "\n",
            "  Train Loss: 0.8006\n",
            "  Val Loss:   0.9147 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 314/1000\n",
            "  Batch 100/224 | Loss: 0.7882 | LR: 7.88e-05\n",
            "  Batch 200/224 | Loss: 0.7900 | LR: 7.88e-05\n",
            "\n",
            "  Train Loss: 0.7916\n",
            "  Val Loss:   0.9243 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 315/1000\n",
            "  Batch 100/224 | Loss: 0.7904 | LR: 7.87e-05\n",
            "  Batch 200/224 | Loss: 0.8027 | LR: 7.86e-05\n",
            "\n",
            "  Train Loss: 0.8015\n",
            "  Val Loss:   0.9233 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_315.pt\n",
            "Epoch 316/1000\n",
            "  Batch 100/224 | Loss: 0.7920 | LR: 7.86e-05\n",
            "  Batch 200/224 | Loss: 0.7916 | LR: 7.85e-05\n",
            "\n",
            "  Train Loss: 0.7931\n",
            "  Val Loss:   0.9266 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 317/1000\n",
            "  Batch 100/224 | Loss: 0.7885 | LR: 7.84e-05\n",
            "  Batch 200/224 | Loss: 0.7954 | LR: 7.84e-05\n",
            "\n",
            "  Train Loss: 0.7970\n",
            "  Val Loss:   0.9245 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 318/1000\n",
            "  Batch 100/224 | Loss: 0.7817 | LR: 7.83e-05\n",
            "  Batch 200/224 | Loss: 0.7914 | LR: 7.82e-05\n",
            "\n",
            "  Train Loss: 0.7960\n",
            "  Val Loss:   0.9263 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 319/1000\n",
            "  Batch 100/224 | Loss: 0.7759 | LR: 7.82e-05\n",
            "  Batch 200/224 | Loss: 0.7856 | LR: 7.81e-05\n",
            "\n",
            "  Train Loss: 0.7910\n",
            "  Val Loss:   0.9235 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 320/1000\n",
            "  Batch 100/224 | Loss: 0.8012 | LR: 7.80e-05\n",
            "  Batch 200/224 | Loss: 0.7969 | LR: 7.80e-05\n",
            "\n",
            "  Train Loss: 0.8008\n",
            "  Val Loss:   0.9251 \n",
            "  Time:       3.1s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_320.pt\n",
            "Epoch 321/1000\n",
            "  Batch 100/224 | Loss: 0.7695 | LR: 7.79e-05\n",
            "  Batch 200/224 | Loss: 0.7836 | LR: 7.78e-05\n",
            "\n",
            "  Train Loss: 0.7869\n",
            "  Val Loss:   0.9365 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 322/1000\n",
            "  Batch 100/224 | Loss: 0.7847 | LR: 7.78e-05\n",
            "  Batch 200/224 | Loss: 0.7885 | LR: 7.77e-05\n",
            "\n",
            "  Train Loss: 0.7912\n",
            "  Val Loss:   0.9299 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 323/1000\n",
            "  Batch 100/224 | Loss: 0.7863 | LR: 7.76e-05\n",
            "  Batch 200/224 | Loss: 0.7876 | LR: 7.76e-05\n",
            "\n",
            "  Train Loss: 0.7905\n",
            "  Val Loss:   0.9340 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 324/1000\n",
            "  Batch 100/224 | Loss: 0.7817 | LR: 7.75e-05\n",
            "  Batch 200/224 | Loss: 0.7913 | LR: 7.75e-05\n",
            "\n",
            "  Train Loss: 0.7903\n",
            "  Val Loss:   0.9292 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 325/1000\n",
            "  Batch 100/224 | Loss: 0.7677 | LR: 7.74e-05\n",
            "  Batch 200/224 | Loss: 0.7861 | LR: 7.73e-05\n",
            "\n",
            "  Train Loss: 0.7868\n",
            "  Val Loss:   0.9351 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_325.pt\n",
            "Epoch 326/1000\n",
            "  Batch 100/224 | Loss: 0.7910 | LR: 7.73e-05\n",
            "  Batch 200/224 | Loss: 0.7954 | LR: 7.72e-05\n",
            "\n",
            "  Train Loss: 0.7916\n",
            "  Val Loss:   0.9391 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 327/1000\n",
            "  Batch 100/224 | Loss: 0.7880 | LR: 7.71e-05\n",
            "  Batch 200/224 | Loss: 0.7920 | LR: 7.71e-05\n",
            "\n",
            "  Train Loss: 0.7875\n",
            "  Val Loss:   0.9342 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 328/1000\n",
            "  Batch 100/224 | Loss: 0.7805 | LR: 7.70e-05\n",
            "  Batch 200/224 | Loss: 0.7870 | LR: 7.69e-05\n",
            "\n",
            "  Train Loss: 0.7865\n",
            "  Val Loss:   0.9297 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 329/1000\n",
            "  Batch 100/224 | Loss: 0.7827 | LR: 7.69e-05\n",
            "  Batch 200/224 | Loss: 0.7828 | LR: 7.68e-05\n",
            "\n",
            "  Train Loss: 0.7834\n",
            "  Val Loss:   0.9303 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 330/1000\n",
            "  Batch 100/224 | Loss: 0.7975 | LR: 7.67e-05\n",
            "  Batch 200/224 | Loss: 0.7901 | LR: 7.67e-05\n",
            "\n",
            "  Train Loss: 0.7902\n",
            "  Val Loss:   0.9322 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_330.pt\n",
            "Epoch 331/1000\n",
            "  Batch 100/224 | Loss: 0.7783 | LR: 7.66e-05\n",
            "  Batch 200/224 | Loss: 0.7884 | LR: 7.65e-05\n",
            "\n",
            "  Train Loss: 0.7865\n",
            "  Val Loss:   0.9283 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 332/1000\n",
            "  Batch 100/224 | Loss: 0.7729 | LR: 7.65e-05\n",
            "  Batch 200/224 | Loss: 0.7846 | LR: 7.64e-05\n",
            "\n",
            "  Train Loss: 0.7836\n",
            "  Val Loss:   0.9326 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 333/1000\n",
            "  Batch 100/224 | Loss: 0.7799 | LR: 7.63e-05\n",
            "  Batch 200/224 | Loss: 0.7822 | LR: 7.63e-05\n",
            "\n",
            "  Train Loss: 0.7828\n",
            "  Val Loss:   0.9337 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 334/1000\n",
            "  Batch 100/224 | Loss: 0.7948 | LR: 7.62e-05\n",
            "  Batch 200/224 | Loss: 0.7888 | LR: 7.61e-05\n",
            "\n",
            "  Train Loss: 0.7899\n",
            "  Val Loss:   0.9325 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 335/1000\n",
            "  Batch 100/224 | Loss: 0.7740 | LR: 7.61e-05\n",
            "  Batch 200/224 | Loss: 0.7840 | LR: 7.60e-05\n",
            "\n",
            "  Train Loss: 0.7836\n",
            "  Val Loss:   0.9337 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_335.pt\n",
            "Epoch 336/1000\n",
            "  Batch 100/224 | Loss: 0.7772 | LR: 7.59e-05\n",
            "  Batch 200/224 | Loss: 0.7872 | LR: 7.59e-05\n",
            "\n",
            "  Train Loss: 0.7854\n",
            "  Val Loss:   0.9350 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 337/1000\n",
            "  Batch 100/224 | Loss: 0.7846 | LR: 7.58e-05\n",
            "  Batch 200/224 | Loss: 0.7791 | LR: 7.57e-05\n",
            "\n",
            "  Train Loss: 0.7841\n",
            "  Val Loss:   0.9388 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 338/1000\n",
            "  Batch 100/224 | Loss: 0.7603 | LR: 7.57e-05\n",
            "  Batch 200/224 | Loss: 0.7782 | LR: 7.56e-05\n",
            "\n",
            "  Train Loss: 0.7820\n",
            "  Val Loss:   0.9400 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 339/1000\n",
            "  Batch 100/224 | Loss: 0.7898 | LR: 7.55e-05\n",
            "  Batch 200/224 | Loss: 0.7838 | LR: 7.55e-05\n",
            "\n",
            "  Train Loss: 0.7844\n",
            "  Val Loss:   0.9432 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 340/1000\n",
            "  Batch 100/224 | Loss: 0.7809 | LR: 7.54e-05\n",
            "  Batch 200/224 | Loss: 0.7798 | LR: 7.53e-05\n",
            "\n",
            "  Train Loss: 0.7768\n",
            "  Val Loss:   0.9466 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_340.pt\n",
            "Epoch 341/1000\n",
            "  Batch 100/224 | Loss: 0.7664 | LR: 7.52e-05\n",
            "  Batch 200/224 | Loss: 0.7830 | LR: 7.52e-05\n",
            "\n",
            "  Train Loss: 0.7827\n",
            "  Val Loss:   0.9372 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 342/1000\n",
            "  Batch 100/224 | Loss: 0.7870 | LR: 7.51e-05\n",
            "  Batch 200/224 | Loss: 0.7803 | LR: 7.51e-05\n",
            "\n",
            "  Train Loss: 0.7792\n",
            "  Val Loss:   0.9507 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 343/1000\n",
            "  Batch 100/224 | Loss: 0.7741 | LR: 7.50e-05\n",
            "  Batch 200/224 | Loss: 0.7831 | LR: 7.49e-05\n",
            "\n",
            "  Train Loss: 0.7814\n",
            "  Val Loss:   0.9410 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 344/1000\n",
            "  Batch 100/224 | Loss: 0.7672 | LR: 7.48e-05\n",
            "  Batch 200/224 | Loss: 0.7777 | LR: 7.48e-05\n",
            "\n",
            "  Train Loss: 0.7769\n",
            "  Val Loss:   0.9445 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 345/1000\n",
            "  Batch 100/224 | Loss: 0.7861 | LR: 7.47e-05\n",
            "  Batch 200/224 | Loss: 0.7786 | LR: 7.46e-05\n",
            "\n",
            "  Train Loss: 0.7804\n",
            "  Val Loss:   0.9497 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_345.pt\n",
            "Epoch 346/1000\n",
            "  Batch 100/224 | Loss: 0.7585 | LR: 7.46e-05\n",
            "  Batch 200/224 | Loss: 0.7707 | LR: 7.45e-05\n",
            "\n",
            "  Train Loss: 0.7751\n",
            "  Val Loss:   0.9458 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 347/1000\n",
            "  Batch 100/224 | Loss: 0.7828 | LR: 7.44e-05\n",
            "  Batch 200/224 | Loss: 0.7807 | LR: 7.44e-05\n",
            "\n",
            "  Train Loss: 0.7802\n",
            "  Val Loss:   0.9489 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 348/1000\n",
            "  Batch 100/224 | Loss: 0.7753 | LR: 7.43e-05\n",
            "  Batch 200/224 | Loss: 0.7831 | LR: 7.42e-05\n",
            "\n",
            "  Train Loss: 0.7787\n",
            "  Val Loss:   0.9469 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 349/1000\n",
            "  Batch 100/224 | Loss: 0.7818 | LR: 7.42e-05\n",
            "  Batch 200/224 | Loss: 0.7807 | LR: 7.41e-05\n",
            "\n",
            "  Train Loss: 0.7811\n",
            "  Val Loss:   0.9462 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 350/1000\n",
            "  Batch 100/224 | Loss: 0.7866 | LR: 7.40e-05\n",
            "  Batch 200/224 | Loss: 0.7742 | LR: 7.40e-05\n",
            "\n",
            "  Train Loss: 0.7754\n",
            "  Val Loss:   0.9499 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_350.pt\n",
            "Epoch 351/1000\n",
            "  Batch 100/224 | Loss: 0.7782 | LR: 7.39e-05\n",
            "  Batch 200/224 | Loss: 0.7768 | LR: 7.38e-05\n",
            "\n",
            "  Train Loss: 0.7787\n",
            "  Val Loss:   0.9467 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 352/1000\n",
            "  Batch 100/224 | Loss: 0.7564 | LR: 7.37e-05\n",
            "  Batch 200/224 | Loss: 0.7674 | LR: 7.37e-05\n",
            "\n",
            "  Train Loss: 0.7723\n",
            "  Val Loss:   0.9464 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 353/1000\n",
            "  Batch 100/224 | Loss: 0.7644 | LR: 7.36e-05\n",
            "  Batch 200/224 | Loss: 0.7723 | LR: 7.35e-05\n",
            "\n",
            "  Train Loss: 0.7729\n",
            "  Val Loss:   0.9499 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 354/1000\n",
            "  Batch 100/224 | Loss: 0.7835 | LR: 7.35e-05\n",
            "  Batch 200/224 | Loss: 0.7736 | LR: 7.34e-05\n",
            "\n",
            "  Train Loss: 0.7782\n",
            "  Val Loss:   0.9501 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 355/1000\n",
            "  Batch 100/224 | Loss: 0.7616 | LR: 7.33e-05\n",
            "  Batch 200/224 | Loss: 0.7709 | LR: 7.33e-05\n",
            "\n",
            "  Train Loss: 0.7715\n",
            "  Val Loss:   0.9544 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_355.pt\n",
            "Epoch 356/1000\n",
            "  Batch 100/224 | Loss: 0.7763 | LR: 7.32e-05\n",
            "  Batch 200/224 | Loss: 0.7733 | LR: 7.31e-05\n",
            "\n",
            "  Train Loss: 0.7745\n",
            "  Val Loss:   0.9493 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 357/1000\n",
            "  Batch 100/224 | Loss: 0.7842 | LR: 7.30e-05\n",
            "  Batch 200/224 | Loss: 0.7756 | LR: 7.30e-05\n",
            "\n",
            "  Train Loss: 0.7723\n",
            "  Val Loss:   0.9507 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 358/1000\n",
            "  Batch 100/224 | Loss: 0.7581 | LR: 7.29e-05\n",
            "  Batch 200/224 | Loss: 0.7766 | LR: 7.28e-05\n",
            "\n",
            "  Train Loss: 0.7740\n",
            "  Val Loss:   0.9523 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 359/1000\n",
            "  Batch 100/224 | Loss: 0.7614 | LR: 7.28e-05\n",
            "  Batch 200/224 | Loss: 0.7723 | LR: 7.27e-05\n",
            "\n",
            "  Train Loss: 0.7707\n",
            "  Val Loss:   0.9491 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 360/1000\n",
            "  Batch 100/224 | Loss: 0.7580 | LR: 7.26e-05\n",
            "  Batch 200/224 | Loss: 0.7702 | LR: 7.26e-05\n",
            "\n",
            "  Train Loss: 0.7710\n",
            "  Val Loss:   0.9534 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_360.pt\n",
            "Epoch 361/1000\n",
            "  Batch 100/224 | Loss: 0.7584 | LR: 7.25e-05\n",
            "  Batch 200/224 | Loss: 0.7689 | LR: 7.24e-05\n",
            "\n",
            "  Train Loss: 0.7731\n",
            "  Val Loss:   0.9480 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 362/1000\n",
            "  Batch 100/224 | Loss: 0.7528 | LR: 7.23e-05\n",
            "  Batch 200/224 | Loss: 0.7650 | LR: 7.23e-05\n",
            "\n",
            "  Train Loss: 0.7666\n",
            "  Val Loss:   0.9575 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 363/1000\n",
            "  Batch 100/224 | Loss: 0.7562 | LR: 7.22e-05\n",
            "  Batch 200/224 | Loss: 0.7662 | LR: 7.21e-05\n",
            "\n",
            "  Train Loss: 0.7665\n",
            "  Val Loss:   0.9541 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 364/1000\n",
            "  Batch 100/224 | Loss: 0.7601 | LR: 7.21e-05\n",
            "  Batch 200/224 | Loss: 0.7698 | LR: 7.20e-05\n",
            "\n",
            "  Train Loss: 0.7692\n",
            "  Val Loss:   0.9563 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 365/1000\n",
            "  Batch 100/224 | Loss: 0.7726 | LR: 7.19e-05\n",
            "  Batch 200/224 | Loss: 0.7740 | LR: 7.19e-05\n",
            "\n",
            "  Train Loss: 0.7771\n",
            "  Val Loss:   0.9584 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_365.pt\n",
            "Epoch 366/1000\n",
            "  Batch 100/224 | Loss: 0.7496 | LR: 7.18e-05\n",
            "  Batch 200/224 | Loss: 0.7676 | LR: 7.17e-05\n",
            "\n",
            "  Train Loss: 0.7689\n",
            "  Val Loss:   0.9608 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 367/1000\n",
            "  Batch 100/224 | Loss: 0.7729 | LR: 7.16e-05\n",
            "  Batch 200/224 | Loss: 0.7718 | LR: 7.16e-05\n",
            "\n",
            "  Train Loss: 0.7673\n",
            "  Val Loss:   0.9685 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 368/1000\n",
            "  Batch 100/224 | Loss: 0.7634 | LR: 7.15e-05\n",
            "  Batch 200/224 | Loss: 0.7625 | LR: 7.14e-05\n",
            "\n",
            "  Train Loss: 0.7702\n",
            "  Val Loss:   0.9573 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 369/1000\n",
            "  Batch 100/224 | Loss: 0.7554 | LR: 7.14e-05\n",
            "  Batch 200/224 | Loss: 0.7708 | LR: 7.13e-05\n",
            "\n",
            "  Train Loss: 0.7698\n",
            "  Val Loss:   0.9570 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 370/1000\n",
            "  Batch 100/224 | Loss: 0.7493 | LR: 7.12e-05\n",
            "  Batch 200/224 | Loss: 0.7622 | LR: 7.12e-05\n",
            "\n",
            "  Train Loss: 0.7658\n",
            "  Val Loss:   0.9539 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_370.pt\n",
            "Epoch 371/1000\n",
            "  Batch 100/224 | Loss: 0.7674 | LR: 7.11e-05\n",
            "  Batch 200/224 | Loss: 0.7604 | LR: 7.10e-05\n",
            "\n",
            "  Train Loss: 0.7600\n",
            "  Val Loss:   0.9574 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 372/1000\n",
            "  Batch 100/224 | Loss: 0.7589 | LR: 7.09e-05\n",
            "  Batch 200/224 | Loss: 0.7682 | LR: 7.09e-05\n",
            "\n",
            "  Train Loss: 0.7705\n",
            "  Val Loss:   0.9591 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 373/1000\n",
            "  Batch 100/224 | Loss: 0.7607 | LR: 7.08e-05\n",
            "  Batch 200/224 | Loss: 0.7660 | LR: 7.07e-05\n",
            "\n",
            "  Train Loss: 0.7653\n",
            "  Val Loss:   0.9638 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 374/1000\n",
            "  Batch 100/224 | Loss: 0.7648 | LR: 7.06e-05\n",
            "  Batch 200/224 | Loss: 0.7687 | LR: 7.06e-05\n",
            "\n",
            "  Train Loss: 0.7671\n",
            "  Val Loss:   0.9586 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 375/1000\n",
            "  Batch 100/224 | Loss: 0.7597 | LR: 7.05e-05\n",
            "  Batch 200/224 | Loss: 0.7626 | LR: 7.04e-05\n",
            "\n",
            "  Train Loss: 0.7609\n",
            "  Val Loss:   0.9657 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_375.pt\n",
            "Epoch 376/1000\n",
            "  Batch 100/224 | Loss: 0.7612 | LR: 7.04e-05\n",
            "  Batch 200/224 | Loss: 0.7678 | LR: 7.03e-05\n",
            "\n",
            "  Train Loss: 0.7646\n",
            "  Val Loss:   0.9631 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 377/1000\n",
            "  Batch 100/224 | Loss: 0.7555 | LR: 7.02e-05\n",
            "  Batch 200/224 | Loss: 0.7631 | LR: 7.02e-05\n",
            "\n",
            "  Train Loss: 0.7611\n",
            "  Val Loss:   0.9696 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 378/1000\n",
            "  Batch 100/224 | Loss: 0.7486 | LR: 7.01e-05\n",
            "  Batch 200/224 | Loss: 0.7580 | LR: 7.00e-05\n",
            "\n",
            "  Train Loss: 0.7561\n",
            "  Val Loss:   0.9629 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 379/1000\n",
            "  Batch 100/224 | Loss: 0.7616 | LR: 6.99e-05\n",
            "  Batch 200/224 | Loss: 0.7546 | LR: 6.99e-05\n",
            "\n",
            "  Train Loss: 0.7606\n",
            "  Val Loss:   0.9633 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 380/1000\n",
            "  Batch 100/224 | Loss: 0.7545 | LR: 6.98e-05\n",
            "  Batch 200/224 | Loss: 0.7649 | LR: 6.97e-05\n",
            "\n",
            "  Train Loss: 0.7640\n",
            "  Val Loss:   0.9645 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_380.pt\n",
            "Epoch 381/1000\n",
            "  Batch 100/224 | Loss: 0.7606 | LR: 6.96e-05\n",
            "  Batch 200/224 | Loss: 0.7666 | LR: 6.96e-05\n",
            "\n",
            "  Train Loss: 0.7637\n",
            "  Val Loss:   0.9579 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 382/1000\n",
            "  Batch 100/224 | Loss: 0.7366 | LR: 6.95e-05\n",
            "  Batch 200/224 | Loss: 0.7601 | LR: 6.94e-05\n",
            "\n",
            "  Train Loss: 0.7608\n",
            "  Val Loss:   0.9668 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 383/1000\n",
            "  Batch 100/224 | Loss: 0.7258 | LR: 6.93e-05\n",
            "  Batch 200/224 | Loss: 0.7593 | LR: 6.93e-05\n",
            "\n",
            "  Train Loss: 0.7615\n",
            "  Val Loss:   0.9693 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 384/1000\n",
            "  Batch 100/224 | Loss: 0.7592 | LR: 6.92e-05\n",
            "  Batch 200/224 | Loss: 0.7614 | LR: 6.91e-05\n",
            "\n",
            "  Train Loss: 0.7649\n",
            "  Val Loss:   0.9697 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 385/1000\n",
            "  Batch 100/224 | Loss: 0.7673 | LR: 6.91e-05\n",
            "  Batch 200/224 | Loss: 0.7652 | LR: 6.90e-05\n",
            "\n",
            "  Train Loss: 0.7673\n",
            "  Val Loss:   0.9681 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_385.pt\n",
            "Epoch 386/1000\n",
            "  Batch 100/224 | Loss: 0.7777 | LR: 6.89e-05\n",
            "  Batch 200/224 | Loss: 0.7624 | LR: 6.88e-05\n",
            "\n",
            "  Train Loss: 0.7612\n",
            "  Val Loss:   0.9651 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 387/1000\n",
            "  Batch 100/224 | Loss: 0.7416 | LR: 6.88e-05\n",
            "  Batch 200/224 | Loss: 0.7560 | LR: 6.87e-05\n",
            "\n",
            "  Train Loss: 0.7548\n",
            "  Val Loss:   0.9705 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 388/1000\n",
            "  Batch 100/224 | Loss: 0.7672 | LR: 6.86e-05\n",
            "  Batch 200/224 | Loss: 0.7602 | LR: 6.86e-05\n",
            "\n",
            "  Train Loss: 0.7609\n",
            "  Val Loss:   0.9755 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 389/1000\n",
            "  Batch 100/224 | Loss: 0.7554 | LR: 6.85e-05\n",
            "  Batch 200/224 | Loss: 0.7527 | LR: 6.84e-05\n",
            "\n",
            "  Train Loss: 0.7571\n",
            "  Val Loss:   0.9707 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 390/1000\n",
            "  Batch 100/224 | Loss: 0.7495 | LR: 6.83e-05\n",
            "  Batch 200/224 | Loss: 0.7570 | LR: 6.83e-05\n",
            "\n",
            "  Train Loss: 0.7559\n",
            "  Val Loss:   0.9709 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_390.pt\n",
            "Epoch 391/1000\n",
            "  Batch 100/224 | Loss: 0.7573 | LR: 6.82e-05\n",
            "  Batch 200/224 | Loss: 0.7580 | LR: 6.81e-05\n",
            "\n",
            "  Train Loss: 0.7606\n",
            "  Val Loss:   0.9738 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 392/1000\n",
            "  Batch 100/224 | Loss: 0.7336 | LR: 6.80e-05\n",
            "  Batch 200/224 | Loss: 0.7513 | LR: 6.80e-05\n",
            "\n",
            "  Train Loss: 0.7540\n",
            "  Val Loss:   0.9762 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 393/1000\n",
            "  Batch 100/224 | Loss: 0.7513 | LR: 6.79e-05\n",
            "  Batch 200/224 | Loss: 0.7541 | LR: 6.78e-05\n",
            "\n",
            "  Train Loss: 0.7520\n",
            "  Val Loss:   0.9803 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 394/1000\n",
            "  Batch 100/224 | Loss: 0.7442 | LR: 6.77e-05\n",
            "  Batch 200/224 | Loss: 0.7563 | LR: 6.77e-05\n",
            "\n",
            "  Train Loss: 0.7554\n",
            "  Val Loss:   0.9832 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 395/1000\n",
            "  Batch 100/224 | Loss: 0.7579 | LR: 6.76e-05\n",
            "  Batch 200/224 | Loss: 0.7555 | LR: 6.75e-05\n",
            "\n",
            "  Train Loss: 0.7559\n",
            "  Val Loss:   0.9655 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_395.pt\n",
            "Epoch 396/1000\n",
            "  Batch 100/224 | Loss: 0.7384 | LR: 6.75e-05\n",
            "  Batch 200/224 | Loss: 0.7502 | LR: 6.74e-05\n",
            "\n",
            "  Train Loss: 0.7544\n",
            "  Val Loss:   0.9726 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 397/1000\n",
            "  Batch 100/224 | Loss: 0.7440 | LR: 6.73e-05\n",
            "  Batch 200/224 | Loss: 0.7510 | LR: 6.72e-05\n",
            "\n",
            "  Train Loss: 0.7518\n",
            "  Val Loss:   0.9719 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 398/1000\n",
            "  Batch 100/224 | Loss: 0.7410 | LR: 6.72e-05\n",
            "  Batch 200/224 | Loss: 0.7462 | LR: 6.71e-05\n",
            "\n",
            "  Train Loss: 0.7471\n",
            "  Val Loss:   0.9774 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 399/1000\n",
            "  Batch 100/224 | Loss: 0.7434 | LR: 6.70e-05\n",
            "  Batch 200/224 | Loss: 0.7531 | LR: 6.69e-05\n",
            "\n",
            "  Train Loss: 0.7484\n",
            "  Val Loss:   0.9805 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 400/1000\n",
            "  Batch 100/224 | Loss: 0.7484 | LR: 6.69e-05\n",
            "  Batch 200/224 | Loss: 0.7534 | LR: 6.68e-05\n",
            "\n",
            "  Train Loss: 0.7524\n",
            "  Val Loss:   0.9728 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_400.pt\n",
            "Epoch 401/1000\n",
            "  Batch 100/224 | Loss: 0.7497 | LR: 6.67e-05\n",
            "  Batch 200/224 | Loss: 0.7444 | LR: 6.66e-05\n",
            "\n",
            "  Train Loss: 0.7495\n",
            "  Val Loss:   0.9823 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 402/1000\n",
            "  Batch 100/224 | Loss: 0.7346 | LR: 6.66e-05\n",
            "  Batch 200/224 | Loss: 0.7498 | LR: 6.65e-05\n",
            "\n",
            "  Train Loss: 0.7499\n",
            "  Val Loss:   0.9826 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 403/1000\n",
            "  Batch 100/224 | Loss: 0.7381 | LR: 6.64e-05\n",
            "  Batch 200/224 | Loss: 0.7462 | LR: 6.63e-05\n",
            "\n",
            "  Train Loss: 0.7483\n",
            "  Val Loss:   0.9782 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 404/1000\n",
            "  Batch 100/224 | Loss: 0.7484 | LR: 6.63e-05\n",
            "  Batch 200/224 | Loss: 0.7556 | LR: 6.62e-05\n",
            "\n",
            "  Train Loss: 0.7539\n",
            "  Val Loss:   0.9816 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 405/1000\n",
            "  Batch 100/224 | Loss: 0.7616 | LR: 6.61e-05\n",
            "  Batch 200/224 | Loss: 0.7595 | LR: 6.61e-05\n",
            "\n",
            "  Train Loss: 0.7540\n",
            "  Val Loss:   0.9862 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_405.pt\n",
            "Epoch 406/1000\n",
            "  Batch 100/224 | Loss: 0.7436 | LR: 6.60e-05\n",
            "  Batch 200/224 | Loss: 0.7450 | LR: 6.59e-05\n",
            "\n",
            "  Train Loss: 0.7433\n",
            "  Val Loss:   0.9913 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 407/1000\n",
            "  Batch 100/224 | Loss: 0.7250 | LR: 6.58e-05\n",
            "  Batch 200/224 | Loss: 0.7427 | LR: 6.58e-05\n",
            "\n",
            "  Train Loss: 0.7430\n",
            "  Val Loss:   0.9962 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 408/1000\n",
            "  Batch 100/224 | Loss: 0.7549 | LR: 6.57e-05\n",
            "  Batch 200/224 | Loss: 0.7537 | LR: 6.56e-05\n",
            "\n",
            "  Train Loss: 0.7511\n",
            "  Val Loss:   0.9858 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 409/1000\n",
            "  Batch 100/224 | Loss: 0.7487 | LR: 6.55e-05\n",
            "  Batch 200/224 | Loss: 0.7392 | LR: 6.55e-05\n",
            "\n",
            "  Train Loss: 0.7392\n",
            "  Val Loss:   0.9850 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 410/1000\n",
            "  Batch 100/224 | Loss: 0.7315 | LR: 6.54e-05\n",
            "  Batch 200/224 | Loss: 0.7441 | LR: 6.53e-05\n",
            "\n",
            "  Train Loss: 0.7439\n",
            "  Val Loss:   0.9913 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_410.pt\n",
            "Epoch 411/1000\n",
            "  Batch 100/224 | Loss: 0.7476 | LR: 6.52e-05\n",
            "  Batch 200/224 | Loss: 0.7333 | LR: 6.52e-05\n",
            "\n",
            "  Train Loss: 0.7432\n",
            "  Val Loss:   0.9958 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 412/1000\n",
            "  Batch 100/224 | Loss: 0.7359 | LR: 6.51e-05\n",
            "  Batch 200/224 | Loss: 0.7431 | LR: 6.50e-05\n",
            "\n",
            "  Train Loss: 0.7378\n",
            "  Val Loss:   0.9919 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 413/1000\n",
            "  Batch 100/224 | Loss: 0.7370 | LR: 6.49e-05\n",
            "  Batch 200/224 | Loss: 0.7444 | LR: 6.49e-05\n",
            "\n",
            "  Train Loss: 0.7445\n",
            "  Val Loss:   0.9914 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 414/1000\n",
            "  Batch 100/224 | Loss: 0.7336 | LR: 6.48e-05\n",
            "  Batch 200/224 | Loss: 0.7386 | LR: 6.47e-05\n",
            "\n",
            "  Train Loss: 0.7398\n",
            "  Val Loss:   0.9977 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 415/1000\n",
            "  Batch 100/224 | Loss: 0.7421 | LR: 6.46e-05\n",
            "  Batch 200/224 | Loss: 0.7461 | LR: 6.46e-05\n",
            "\n",
            "  Train Loss: 0.7449\n",
            "  Val Loss:   0.9848 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_415.pt\n",
            "Epoch 416/1000\n",
            "  Batch 100/224 | Loss: 0.7290 | LR: 6.45e-05\n",
            "  Batch 200/224 | Loss: 0.7370 | LR: 6.44e-05\n",
            "\n",
            "  Train Loss: 0.7404\n",
            "  Val Loss:   0.9865 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 417/1000\n",
            "  Batch 100/224 | Loss: 0.7379 | LR: 6.43e-05\n",
            "  Batch 200/224 | Loss: 0.7411 | LR: 6.43e-05\n",
            "\n",
            "  Train Loss: 0.7374\n",
            "  Val Loss:   0.9945 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 418/1000\n",
            "  Batch 100/224 | Loss: 0.7604 | LR: 6.42e-05\n",
            "  Batch 200/224 | Loss: 0.7458 | LR: 6.41e-05\n",
            "\n",
            "  Train Loss: 0.7466\n",
            "  Val Loss:   1.0042 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 419/1000\n",
            "  Batch 100/224 | Loss: 0.7442 | LR: 6.40e-05\n",
            "  Batch 200/224 | Loss: 0.7451 | LR: 6.40e-05\n",
            "\n",
            "  Train Loss: 0.7416\n",
            "  Val Loss:   0.9965 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 420/1000\n",
            "  Batch 100/224 | Loss: 0.7310 | LR: 6.39e-05\n",
            "  Batch 200/224 | Loss: 0.7384 | LR: 6.38e-05\n",
            "\n",
            "  Train Loss: 0.7401\n",
            "  Val Loss:   0.9905 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_420.pt\n",
            "Epoch 421/1000\n",
            "  Batch 100/224 | Loss: 0.7423 | LR: 6.37e-05\n",
            "  Batch 200/224 | Loss: 0.7405 | LR: 6.37e-05\n",
            "\n",
            "  Train Loss: 0.7419\n",
            "  Val Loss:   0.9990 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 422/1000\n",
            "  Batch 100/224 | Loss: 0.7430 | LR: 6.36e-05\n",
            "  Batch 200/224 | Loss: 0.7355 | LR: 6.35e-05\n",
            "\n",
            "  Train Loss: 0.7367\n",
            "  Val Loss:   0.9974 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 423/1000\n",
            "  Batch 100/224 | Loss: 0.7391 | LR: 6.34e-05\n",
            "  Batch 200/224 | Loss: 0.7335 | LR: 6.34e-05\n",
            "\n",
            "  Train Loss: 0.7366\n",
            "  Val Loss:   0.9966 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 424/1000\n",
            "  Batch 100/224 | Loss: 0.7370 | LR: 6.33e-05\n",
            "  Batch 200/224 | Loss: 0.7341 | LR: 6.32e-05\n",
            "\n",
            "  Train Loss: 0.7340\n",
            "  Val Loss:   0.9978 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 425/1000\n",
            "  Batch 100/224 | Loss: 0.7295 | LR: 6.31e-05\n",
            "  Batch 200/224 | Loss: 0.7300 | LR: 6.30e-05\n",
            "\n",
            "  Train Loss: 0.7337\n",
            "  Val Loss:   0.9996 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_425.pt\n",
            "Epoch 426/1000\n",
            "  Batch 100/224 | Loss: 0.7367 | LR: 6.30e-05\n",
            "  Batch 200/224 | Loss: 0.7339 | LR: 6.29e-05\n",
            "\n",
            "  Train Loss: 0.7392\n",
            "  Val Loss:   0.9947 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 427/1000\n",
            "  Batch 100/224 | Loss: 0.7250 | LR: 6.28e-05\n",
            "  Batch 200/224 | Loss: 0.7371 | LR: 6.27e-05\n",
            "\n",
            "  Train Loss: 0.7365\n",
            "  Val Loss:   0.9998 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 428/1000\n",
            "  Batch 100/224 | Loss: 0.7407 | LR: 6.27e-05\n",
            "  Batch 200/224 | Loss: 0.7349 | LR: 6.26e-05\n",
            "\n",
            "  Train Loss: 0.7338\n",
            "  Val Loss:   0.9968 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 429/1000\n",
            "  Batch 100/224 | Loss: 0.7243 | LR: 6.25e-05\n",
            "  Batch 200/224 | Loss: 0.7396 | LR: 6.24e-05\n",
            "\n",
            "  Train Loss: 0.7396\n",
            "  Val Loss:   1.0022 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 430/1000\n",
            "  Batch 100/224 | Loss: 0.7271 | LR: 6.24e-05\n",
            "  Batch 200/224 | Loss: 0.7352 | LR: 6.23e-05\n",
            "\n",
            "  Train Loss: 0.7391\n",
            "  Val Loss:   0.9924 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_430.pt\n",
            "Epoch 431/1000\n",
            "  Batch 100/224 | Loss: 0.7262 | LR: 6.22e-05\n",
            "  Batch 200/224 | Loss: 0.7345 | LR: 6.21e-05\n",
            "\n",
            "  Train Loss: 0.7358\n",
            "  Val Loss:   1.0061 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 432/1000\n",
            "  Batch 100/224 | Loss: 0.7285 | LR: 6.21e-05\n",
            "  Batch 200/224 | Loss: 0.7364 | LR: 6.20e-05\n",
            "\n",
            "  Train Loss: 0.7385\n",
            "  Val Loss:   0.9973 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 433/1000\n",
            "  Batch 100/224 | Loss: 0.7320 | LR: 6.19e-05\n",
            "  Batch 200/224 | Loss: 0.7333 | LR: 6.18e-05\n",
            "\n",
            "  Train Loss: 0.7331\n",
            "  Val Loss:   0.9985 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 434/1000\n",
            "  Batch 100/224 | Loss: 0.7241 | LR: 6.17e-05\n",
            "  Batch 200/224 | Loss: 0.7342 | LR: 6.17e-05\n",
            "\n",
            "  Train Loss: 0.7311\n",
            "  Val Loss:   0.9971 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 435/1000\n",
            "  Batch 100/224 | Loss: 0.7369 | LR: 6.16e-05\n",
            "  Batch 200/224 | Loss: 0.7286 | LR: 6.15e-05\n",
            "\n",
            "  Train Loss: 0.7274\n",
            "  Val Loss:   1.0088 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_435.pt\n",
            "Epoch 436/1000\n",
            "  Batch 100/224 | Loss: 0.7391 | LR: 6.14e-05\n",
            "  Batch 200/224 | Loss: 0.7371 | LR: 6.14e-05\n",
            "\n",
            "  Train Loss: 0.7365\n",
            "  Val Loss:   1.0026 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 437/1000\n",
            "  Batch 100/224 | Loss: 0.7375 | LR: 6.13e-05\n",
            "  Batch 200/224 | Loss: 0.7303 | LR: 6.12e-05\n",
            "\n",
            "  Train Loss: 0.7380\n",
            "  Val Loss:   1.0066 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 438/1000\n",
            "  Batch 100/224 | Loss: 0.7298 | LR: 6.11e-05\n",
            "  Batch 200/224 | Loss: 0.7289 | LR: 6.11e-05\n",
            "\n",
            "  Train Loss: 0.7323\n",
            "  Val Loss:   1.0015 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 439/1000\n",
            "  Batch 100/224 | Loss: 0.7250 | LR: 6.10e-05\n",
            "  Batch 200/224 | Loss: 0.7378 | LR: 6.09e-05\n",
            "\n",
            "  Train Loss: 0.7348\n",
            "  Val Loss:   1.0147 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 440/1000\n",
            "  Batch 100/224 | Loss: 0.7343 | LR: 6.08e-05\n",
            "  Batch 200/224 | Loss: 0.7269 | LR: 6.08e-05\n",
            "\n",
            "  Train Loss: 0.7271\n",
            "  Val Loss:   1.0129 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_440.pt\n",
            "Epoch 441/1000\n",
            "  Batch 100/224 | Loss: 0.7418 | LR: 6.07e-05\n",
            "  Batch 200/224 | Loss: 0.7303 | LR: 6.06e-05\n",
            "\n",
            "  Train Loss: 0.7281\n",
            "  Val Loss:   1.0155 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 442/1000\n",
            "  Batch 100/224 | Loss: 0.7364 | LR: 6.05e-05\n",
            "  Batch 200/224 | Loss: 0.7304 | LR: 6.05e-05\n",
            "\n",
            "  Train Loss: 0.7274\n",
            "  Val Loss:   1.0180 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 443/1000\n",
            "  Batch 100/224 | Loss: 0.7240 | LR: 6.04e-05\n",
            "  Batch 200/224 | Loss: 0.7279 | LR: 6.03e-05\n",
            "\n",
            "  Train Loss: 0.7290\n",
            "  Val Loss:   1.0134 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 444/1000\n",
            "  Batch 100/224 | Loss: 0.7217 | LR: 6.02e-05\n",
            "  Batch 200/224 | Loss: 0.7234 | LR: 6.01e-05\n",
            "\n",
            "  Train Loss: 0.7294\n",
            "  Val Loss:   1.0166 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 445/1000\n",
            "  Batch 100/224 | Loss: 0.7264 | LR: 6.01e-05\n",
            "  Batch 200/224 | Loss: 0.7258 | LR: 6.00e-05\n",
            "\n",
            "  Train Loss: 0.7277\n",
            "  Val Loss:   1.0240 \n",
            "  Time:       2.9s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_445.pt\n",
            "Epoch 446/1000\n",
            "  Batch 100/224 | Loss: 0.7219 | LR: 5.99e-05\n",
            "  Batch 200/224 | Loss: 0.7280 | LR: 5.98e-05\n",
            "\n",
            "  Train Loss: 0.7300\n",
            "  Val Loss:   1.0200 \n",
            "  Time:       3.1s\n",
            "\n",
            "Epoch 447/1000\n",
            "  Batch 100/224 | Loss: 0.7279 | LR: 5.98e-05\n",
            "  Batch 200/224 | Loss: 0.7342 | LR: 5.97e-05\n",
            "\n",
            "  Train Loss: 0.7316\n",
            "  Val Loss:   1.0168 \n",
            "  Time:       2.9s\n",
            "\n",
            "Epoch 448/1000\n",
            "  Batch 100/224 | Loss: 0.7132 | LR: 5.96e-05\n",
            "  Batch 200/224 | Loss: 0.7251 | LR: 5.95e-05\n",
            "\n",
            "  Train Loss: 0.7281\n",
            "  Val Loss:   1.0166 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 449/1000\n",
            "  Batch 100/224 | Loss: 0.7151 | LR: 5.94e-05\n",
            "  Batch 200/224 | Loss: 0.7290 | LR: 5.94e-05\n",
            "\n",
            "  Train Loss: 0.7288\n",
            "  Val Loss:   1.0206 \n",
            "  Time:       3.0s\n",
            "\n",
            "Epoch 450/1000\n",
            "  Batch 100/224 | Loss: 0.7042 | LR: 5.93e-05\n",
            "  Batch 200/224 | Loss: 0.7256 | LR: 5.92e-05\n",
            "\n",
            "  Train Loss: 0.7255\n",
            "  Val Loss:   1.0201 \n",
            "  Time:       3.0s\n",
            "\n",
            "  ✓ Saved checkpoint: checkpoints/checkpoint_epoch_450.pt\n",
            "Epoch 451/1000\n",
            "  Batch 100/224 | Loss: 0.7314 | LR: 5.91e-05\n"
          ]
        }
      ]
    }
  ]
}