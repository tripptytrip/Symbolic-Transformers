# Symbolic Transformers

**Tiny Transformers for First-Order Logic ‚Äî Edge-Deployable, Interpretable, Data-Efficient**

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tripptytrip/Symbolic-Transformers/blob/main/train_symbolic_transformer.ipynb)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

---

## Who is this for?


- **Researchers** testing how far tiny transformers can go on formal reasoning tasks
- **TinyML/edge developers** who need reasoning models in MBs, not GBs
- **Educators** building interactive logic tutors or puzzle tools
- **Neuro-symbolic builders** experimenting with custom symbolic alphabets

## What this is NOT (yet)

- ‚ùå Not a full theorem prover
- ‚ùå Not a verified proof pipeline (no external prover integration yet)
- ‚ùå Not trained on natural language ‚Äî this is formal-symbol sequence learning
- ‚ùå Not claiming to replace grammars for pure syntax checking

## Why not just write a grammar?

A grammar can tell you what strings are *syntactically valid* with 100% accuracy. This project asks a different question: 

**what can a tiny model learn from data beyond validity?**

- Frequency priors (which constructions are common vs rare)
- Soft semantic constraints (which predicates tend to co-occur)
- Generalisation under distribution shift (unseen variable indices, longer formulas)
- Foundation for inference tasks where grammars can't help (Phase 4+)

---

## üéØ Overview

**Symbolic Transformers** achieve competitive performance on logical reasoning tasks using only **566K parameters** and **662 discrete symbols** ‚Äî 200√ó more parameter-efficient than traditional language models.

### Key Innovation

Instead of high-dimensional continuous embeddings (768-4096 dims √ó 50K tokens), we use:

- **Compositional category tokens**: `VAR 12 ‚Üí [VAR, 1, 2]` ‚Äî unlimited variables with finite vocabulary
- **Compact vocabulary**: 662 symbols (625 numerals + 37 FOL operators)
- **Symbolic grounding**: Each numeral has a corresponding 25√ó25 grid pattern via [base-625](https://github.com/tripptytrip/base-625)

**Result:** A **2.2 MB model** that runs on edge devices and learns FOL syntax rules with near-perfect accuracy.

### How base-625 fits in

The companion repo [**base-625**](https://github.com/tripptytrip/base-625) provides a 25√ó25 glyph/grid system for visual symbol representations.

**Current status:** The transformer trains on **discrete token IDs** (learned embeddings). The base-625 grids are used for:
- Rendering and inspection (human-readable symbol visualisation)
- Debugging and interpretability
- Future experiments with CNN/bit-vector embeddings and visual augmentation

This is a deliberate architecture choice ‚Äî token embeddings are simpler and sufficient for the current task, while the visual grounding provides interpretability and a path to vision-based extensions.

---

## üöÄ Quick Start

### Option 1: Train in Google Colab (Recommended)

No local setup required ‚Äî GPU training in your browser:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tripptytrip/Symbolic-Transformers/blob/main/train_symbolic_transformer.ipynb)

### Option 2: Local Installation

```bash
git clone https://github.com/tripptytrip/Symbolic-Transformers.git
cd Symbolic-Transformers
pip install torch numpy tqdm rich
python quickstart.py
```

### Option 3: Train Directly

```bash
python data/dataset_generator.py                    # Generate data
python training/train.py --model-size tiny --num-epochs 100  # Train
python evaluate_model.py                            # Evaluate
```

---

## üìä Results

### Model Performance

| Model | Parameters | Size | Val Loss | Top-1 Acc | Top-5 Acc |
|-------|-----------|------|----------|-----------|-----------|
| **Tiny** | 566K | 2.2 MB | 0.88 | ~44% | ~75% |
| **Small** | 3.5M | 14 MB | 0.75* | ~52%* | ~82%* |
| **Base** | 19.6M | 78 MB | 0.65* | ~58%* | ~86%* |

*Projected based on scaling experiments. Base model training in progress (epoch 1: val loss 0.96).

### Baselines & Context

To understand what 44% accuracy means:

| Method | Top-1 Accuracy | Notes |
|--------|---------------|-------|
| Random | 0.15% | 1/662 tokens |
| Unigram frequency | ~8% | Always predict most common token |
| Bigram model | ~18% | Conditional on previous token |
| Grammar-constrained random | ~25-30% | Random among valid next tokens |
| **Symbolic Transformer (tiny)** | **~44%** | Learns soft preferences beyond grammar |

The model's value is in learning *which* valid continuations are likely, not just *what* is valid.

### What the Model Learns

The model achieves **99.6% confidence** on hard syntax rules:

```
Input: FORALL
Prediction: VAR (99.6%)  ‚Üê Learned that ‚àÄ must bind a variable

Input: FORALL VAR
Prediction: [numeral] (uniform ~2% each)  ‚Üê Appropriate uncertainty over indices

Input: FORALL VAR 1
Prediction: PRED (75%), FORALL (10%), EXISTS (5%)  ‚Üê Soft preferences among valid options
```

### Key Finding: Data Quality > Model Size

Our experiments revealed that **more data beats more epochs**:

- Tiny model (566K params) + 14K samples ‚Üí overfits after ~170 epochs
- Best checkpoint: epoch 167 (val loss 0.879)
- Continued training to epoch 700 made val loss *worse* (1.18)
- 10√ó more data (144K samples) ‚Üí base model hits val loss 0.96 in epoch 1

**Lesson:** In the low-parameter regime, data efficiency matters more than architecture scaling.

---

## üèóÔ∏è Architecture

### Compositional Token Design

The key insight: instead of infinite embeddings for every variable/predicate, use compositional sequences:

```
Traditional:  VAR_0, VAR_1, VAR_2, ... VAR_999  (1000 embeddings)
Ours:         VAR + [0-624]                      (626 embeddings, unlimited range)

VAR 0:   [VAR, 0]
VAR 42:  [VAR, 4, 2]
VAR 999: [VAR, 9, 9, 9]
PRED 5:  [PRED, 5]
```

This is reusable for any symbolic domain with indexed entities.

### Model Configurations

| Config | d_model | Layers | Heads | FF dim | Parameters |
|--------|---------|--------|-------|--------|------------|
| tiny   | 128     | 2      | 4     | 512    | 566K |
| small  | 256     | 4      | 4     | 1024   | 3.5M |
| base   | 512     | 6      | 8     | 2048   | 19.6M |

### Vocabulary (662 symbols)

- **Numerals (0-624):** Base-625 compositional encoding
- **Quantifiers:** FORALL (‚àÄ), EXISTS (‚àÉ), EXISTS_UNIQUE (‚àÉ!)
- **Connectives:** NOT (¬¨), AND (‚àß), OR (‚à®), IMPLIES (‚Üí), IFF (‚Üî)
- **Relations:** EQUALS (=), NOT_EQUALS (‚â†), LESS_THAN (<), etc.
- **Structural:** LPAREN, RPAREN, COMMA, COLON, DOT
- **Categories:** VAR, CONST, PRED, FUNC, SORT

---

## üìö Training Guide

### Recommended Configurations

| Goal | Model | Formulas | Epochs | Time |
|------|-------|----------|--------|------|
| Quick test | tiny | 500 | 30 | ~5 min |
| Standard | tiny | 1,000 | 100-150 | ~30 min |
| Best results | small | 5,000+ | 150-200 | 2+ hours |
| Research scale | base | 10,000+ | 100-200 | 6+ hours |

### ‚ö†Ô∏è Avoiding Overfitting

**Warning signs:**
- Train loss keeps dropping, val loss plateaus
- Val loss starts **increasing**
- Train/val gap exceeds 0.2

**Rule of thumb:**

| Data Size | Max Epochs (tiny) | Max Epochs (small) |
|-----------|-------------------|-------------------|
| 1K formulas | 100-150 | 80-120 |
| 5K formulas | 150-200 | 120-150 |
| 10K formulas | 200-300 | 150-200 |

### Interpreting Loss

- **Val Loss > 2.0**: Learning basic patterns
- **Val Loss 1.0-2.0**: Learning syntax rules
- **Val Loss < 1.0**: Good FOL structure understanding
- **Val Loss < 0.85**: Excellent (approaching capacity)

---

## üéÆ Interactive UI

Explore the model in real-time:

```bash
./run_ui.sh
```

```
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Current Formula ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ ‚àÄ x‚ÇÅ                                  ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Predictions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ 1. VAR    99.6%  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   ‚îÇ
‚îÇ 2. NUM_1   0.1%  -                    ‚îÇ
‚îÇ 3. NUM_3   0.1%  -                    ‚îÇ
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
```

Features:
- Type tokens or select from top-5 predictions
- Auto-complete formulas with temperature control
- Human-readable rendering (‚àÄx‚ÇÅ P‚ÇÖ(x‚ÇÅ))
- Confidence visualisation

---

## üöÄ Deployment

### Model Sizes

| Model | FP32 | INT8 (quantized) |
|-------|------|------------------|
| Tiny  | 2.2 MB | ~0.6 MB |
| Small | 14 MB | ~3.5 MB |
| Base  | 78 MB | ~20 MB |

### Edge Targets

The tiny model (2.2 MB / 0.6 MB quantized) runs on:
- ‚úÖ Raspberry Pi Zero
- ‚úÖ ESP32 (with INT8 quantization)
- ‚úÖ Web browser (ONNX.js / WebAssembly)
- ‚úÖ Mobile devices (iOS/Android)
- ‚úÖ Any system with Python

### Export to ONNX

```python
torch.onnx.export(
    model, sample_input, "symbolic_transformer.onnx",
    input_names=['input'], output_names=['output'],
    dynamic_axes={'input': {0: 'batch', 1: 'seq_len'}}
)
```

---

## üìÅ Project Structure

```
Symbolic-Transformers/
‚îú‚îÄ‚îÄ train_symbolic_transformer.ipynb  # üöÄ Colab notebook
‚îú‚îÄ‚îÄ quickstart.py                     # Interactive setup
‚îú‚îÄ‚îÄ evaluate_model.py                 # Evaluation script
‚îú‚îÄ‚îÄ run_ui.sh                         # Launch UI
‚îú‚îÄ‚îÄ unified_vocabulary.json           # 662-token vocabulary
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ dataset_generator.py          # FOL formula generation
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ transformer.py                # Model architecture
‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îî‚îÄ‚îÄ train.py                      # Training loop
‚îú‚îÄ‚îÄ evaluation/
‚îÇ   ‚îî‚îÄ‚îÄ evaluate.py                   # Metrics
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ vocabulary.py                 # Encoding/decoding
‚îú‚îÄ‚îÄ ui/                               # Interactive terminal UI
‚îÇ
‚îú‚îÄ‚îÄ datasets/                         # Generated data
‚îî‚îÄ‚îÄ checkpoints/                      # Saved models
```

---

## üìà Roadmap

- [x] **Phase 1:** Next-symbol prediction ‚úì
- [x] **Phase 2:** Interactive UI ‚úì
- [x] **Phase 3:** Colab training notebook ‚úì
- [ ] **Phase 4:** Inference task (premise ‚Üí conclusion)
- [ ] **Phase 5:** Multi-step proof generation
- [ ] **Phase 6:** Symbol invention (learn new operators)

**Phase 4** is where this project moves beyond what grammars can do ‚Äî learning to predict valid conclusions from premises.

---

## üîó Related Projects

- [**base-625**](https://github.com/tripptytrip/base-625) ‚Äî The 25√ó25 glyph/grid symbol system used for visual grounding
- [**Quantum Rat**](https://github.com/tripptytrip/quantum-rat) ‚Äî Computational neuroethology platform (related research)

---

## üìù Citation

```bibtex
@software{symbolic_transformers_2025,
  author = {Tom Weaver},
  title = {Symbolic Transformers: Tiny Models for First-Order Logic 
           with Compositional Token Design},
  year = {2025},
  url = {https://github.com/tripptytrip/Symbolic-Transformers},
  note = {566K parameter model achieving 44\% next-token accuracy on FOL}
}
```

---

## ü§ù Contributing

Contributions welcome! High-impact areas:

- **Baselines:** Implement n-gram, LSTM, grammar-constrained baselines
- **Inference tasks:** Premise ‚Üí conclusion dataset and training
- **Benchmarks:** Generalisation tests (longer formulas, held-out symbols)
- **Domains:** Arithmetic, set theory, type theory vocabularies
- **Deployment:** ONNX export, quantization, browser demo

---

## üìú License

MIT License ‚Äî see [LICENSE](LICENSE)

---

## üìû Contact

- **Issues:** [GitHub Issues](https://github.com/tripptytrip/Symbolic-Transformers/issues)
- **X/Twitter:** [@_trippitytrip_](https://x.com/_trippitytrip)

---

**Built for efficient, interpretable, edge-deployable AI reasoning** üß†
